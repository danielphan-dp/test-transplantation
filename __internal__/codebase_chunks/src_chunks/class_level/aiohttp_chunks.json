{
  "chunks": [
    {
      "chunk_id": 0,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "import os\nimport pathlib\nimport sys\n\nfrom setuptools import Extension, setup"
    },
    {
      "chunk_id": 1,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "if sys.version_info < (3, 9):\n    raise RuntimeError(\"aiohttp 4.x requires Python 3.9+\")"
    },
    {
      "chunk_id": 2,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "NO_EXTENSIONS: bool = bool(os.environ.get(\"AIOHTTP_NO_EXTENSIONS\"))\nHERE = pathlib.Path(__file__).parent\nIS_GIT_REPO = (HERE / \".git\").exists()"
    },
    {
      "chunk_id": 3,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "if sys.implementation.name != \"cpython\":\n    NO_EXTENSIONS = True"
    },
    {
      "chunk_id": 4,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "if IS_GIT_REPO and not (HERE / \"vendor/llhttp/README.md\").exists():\n    print(\"Install submodules when building from git clone\", file=sys.stderr)\n    print(\"Hint:\", file=sys.stderr)\n    print(\"  git submodule update --init\", file=sys.stderr)\n    sys.exit(2)"
    },
    {
      "chunk_id": 5,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "extensions = [\n    Extension(\"aiohttp._websocket.mask\", [\"aiohttp/_websocket/mask.c\"]),\n    Extension(\n        \"aiohttp._http_parser\",\n        [\n            \"aiohttp/_http_parser.c\",\n            \"aiohttp/_find_header.c\",\n            \"vendor/llhttp/build/c/llhttp.c\",\n            \"vendor/llhttp/src/native/api.c\",\n            \"vendor/llhttp/src/native/http.c\",\n        ],\n        define_macros=[(\"LLHTTP_STRICT_MODE\", 0)],\n        include_dirs=[\"vendor/llhttp/build\"],\n    ),\n    Extension(\"aiohttp._http_writer\", [\"aiohttp/_http_writer.c\"]),\n    Extension(\"aiohttp._websocket.reader_c\", [\"aiohttp/_websocket/reader_c.c\"]),\n]"
    },
    {
      "chunk_id": 6,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "build_type = \"Pure\" if NO_EXTENSIONS else \"Accelerated\"\nsetup_kwargs = {} if NO_EXTENSIONS else {\"ext_modules\": extensions}"
    },
    {
      "chunk_id": 7,
      "source": "__internal__/data_repo/aiohttp/setup.py",
      "content": "print(\"*********************\", file=sys.stderr)\nprint(\"* {build_type} build *\".format_map(locals()), file=sys.stderr)\nprint(\"*********************\", file=sys.stderr)\nsetup(**setup_kwargs)"
    },
    {
      "chunk_id": 8,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "__version__ = \"4.0.0a2.dev0\""
    },
    {
      "chunk_id": 9,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "from typing import TYPE_CHECKING, Tuple\n\nfrom . import hdrs\nfrom .client import (\n    BaseConnector,\n    ClientConnectionError,\n    ClientConnectionResetError,\n    ClientConnectorCertificateError,\n    ClientConnectorDNSError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientError,\n    ClientHttpProxyError,\n    ClientOSError,\n    ClientPayloadError,\n    ClientProxyConnectionError,\n    ClientRequest,\n    ClientResponse,\n    ClientResponseError,\n    ClientSession,\n    ClientSSLError,\n    ClientTimeout,\n    ClientWebSocketResponse,\n    ClientWSTimeout,\n    ConnectionTimeoutError,\n    ContentTypeError,\n    Fingerprint,\n    InvalidURL,\n    InvalidUrlClientError,\n    InvalidUrlRedirectClientError,\n    NamedPipeConnector,\n    NonHttpUrlClientError,\n    NonHttpUrlRedirectClientError,\n    RedirectClientError,\n    RequestInfo,\n    ServerConnectionError,\n    ServerDisconnectedError,\n    ServerFingerprintMismatch,\n    ServerTimeoutError,\n    SocketTimeoutError,\n    TCPConnector,\n    TooManyRedirects,\n    UnixConnector,\n    WSMessageTypeError,\n    WSServerHandshakeError,\n    request,\n)\nfrom .cookiejar import CookieJar, DummyCookieJar\nfrom .formdata import FormData\nfrom .helpers import BasicAuth, ChainMapProxy, ETag\nfrom .http import (\n    HttpVersion,\n    HttpVersion10,\n    HttpVersion11,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .multipart import (\n    BadContentDispositionHeader,\n    BadContentDispositionParam,\n    BodyPartReader,\n    MultipartReader,\n    MultipartWriter,\n    content_disposition_filename,\n    parse_content_disposition,\n)\nfrom .payload import (\n    PAYLOAD_REGISTRY,\n    AsyncIterablePayload,\n    BufferedReaderPayload,\n    BytesIOPayload,\n    BytesPayload,\n    IOBasePayload,\n    JsonPayload,\n    Payload,\n    StringIOPayload,\n    StringPayload,\n    TextIOPayload,\n    get_payload,\n    payload_type,\n)\nfrom .resolver import AsyncResolver, DefaultResolver, ThreadedResolver\nfrom .streams import EMPTY_PAYLOAD, DataQueue, EofStream, StreamReader\nfrom .tracing import (\n    TraceConfig,\n    TraceConnectionCreateEndParams,\n    TraceConnectionCreateStartParams,\n    TraceConnectionQueuedEndParams,\n    TraceConnectionQueuedStartParams,\n    TraceConnectionReuseconnParams,\n    TraceDnsCacheHitParams,\n    TraceDnsCacheMissParams,\n    TraceDnsResolveHostEndParams,\n    TraceDnsResolveHostStartParams,\n    TraceRequestChunkSentParams,\n    TraceRequestEndParams,\n    TraceRequestExceptionParams,\n    TraceRequestHeadersSentParams,\n    TraceRequestRedirectParams,\n    TraceRequestStartParams,\n    TraceResponseChunkReceivedParams,\n)"
    },
    {
      "chunk_id": 10,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "if TYPE_CHECKING:\n    # At runtime these are lazy-loaded at the bottom of the file.\n    from .worker import GunicornUVLoopWebWorker, GunicornWebWorker"
    },
    {
      "chunk_id": 11,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "__all__: Tuple[str, ...] = (\n    \"hdrs\",\n    # client\n    \"BaseConnector\",\n    \"ClientConnectionError\",\n    \"ClientConnectionResetError\",\n    \"ClientConnectorCertificateError\",\n    \"ClientConnectorDNSError\",\n    \"ClientConnectorError\",\n    \"ClientConnectorSSLError\",\n    \"ClientError\",\n    \"ClientHttpProxyError\",\n    \"ClientOSError\",\n    \"ClientPayloadError\",\n    \"ClientProxyConnectionError\",\n    \"ClientResponse\",\n    \"ClientRequest\",\n    \"ClientResponseError\",\n    \"ClientSSLError\",\n    \"ClientSession\",\n    \"ClientTimeout\",\n    \"ClientWebSocketResponse\",\n    \"ClientWSTimeout\",\n    \"ConnectionTimeoutError\",\n    \"ContentTypeError\",\n    \"Fingerprint\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"RedirectClientError\",\n    \"RequestInfo\",\n    \"ServerConnectionError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ServerTimeoutError\",\n    \"SocketTimeoutError\",\n    \"TCPConnector\",\n    \"TooManyRedirects\",\n    \"UnixConnector\",\n    \"NamedPipeConnector\",\n    \"WSServerHandshakeError\",\n    \"request\",\n    # cookiejar\n    \"CookieJar\",\n    \"DummyCookieJar\",\n    # formdata\n    \"FormData\",\n    # helpers\n    \"BasicAuth\",\n    \"ChainMapProxy\",\n    \"ETag\",\n    # http\n    \"HttpVersion\",\n    \"HttpVersion10\",\n    \"HttpVersion11\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    # multipart\n    \"BadContentDispositionHeader\",\n    \"BadContentDispositionParam\",\n    \"BodyPartReader\",\n    \"MultipartReader\",\n    \"MultipartWriter\",\n    \"content_disposition_filename\",\n    \"parse_content_disposition\",\n    # payload\n    \"AsyncIterablePayload\",\n    \"BufferedReaderPayload\",\n    \"BytesIOPayload\",\n    \"BytesPayload\",\n    \"IOBasePayload\",\n    \"JsonPayload\",\n    \"PAYLOAD_REGISTRY\",\n    \"Payload\",\n    \"StringIOPayload\",\n    \"StringPayload\",\n    \"TextIOPayload\",\n    \"get_payload\",\n    \"payload_type\",\n    # resolver\n    \"AsyncResolver\",\n    \"DefaultResolver\",\n    \"ThreadedResolver\",\n    # streams\n    \"DataQueue\",\n    \"EMPTY_PAYLOAD\",\n    \"EofStream\",\n    \"StreamReader\",\n    # tracing\n    \"TraceConfig\",\n    \"TraceConnectionCreateEndParams\",\n    \"TraceConnectionCreateStartParams\",\n    \"TraceConnectionQueuedEndParams\",\n    \"TraceConnectionQueuedStartParams\",\n    \"TraceConnectionReuseconnParams\",\n    \"TraceDnsCacheHitParams\",\n    \"TraceDnsCacheMissParams\",\n    \"TraceDnsResolveHostEndParams\",\n    \"TraceDnsResolveHostStartParams\",\n    \"TraceRequestChunkSentParams\",\n    \"TraceRequestEndParams\",\n    \"TraceRequestExceptionParams\",\n    \"TraceRequestHeadersSentParams\",\n    \"TraceRequestRedirectParams\",\n    \"TraceRequestStartParams\",\n    \"TraceResponseChunkReceivedParams\",\n    # workers (imported lazily with __getattr__)\n    \"GunicornUVLoopWebWorker\",\n    \"GunicornWebWorker\",\n    \"WSMessageTypeError\",\n)"
    },
    {
      "chunk_id": 12,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "def __dir__() -> Tuple[str, ...]:\n    return __all__ + (\"__doc__\",)"
    },
    {
      "chunk_id": 13,
      "source": "__internal__/data_repo/aiohttp/aiohttp/__init__.py",
      "content": "def __getattr__(name: str) -> object:\n    global GunicornUVLoopWebWorker, GunicornWebWorker\n\n    # Importing gunicorn takes a long time (>100ms), so only import if actually needed.\n    if name in (\"GunicornUVLoopWebWorker\", \"GunicornWebWorker\"):\n        try:\n            from .worker import GunicornUVLoopWebWorker as guv, GunicornWebWorker as gw\n        except ImportError:\n            return None\n\n        GunicornUVLoopWebWorker = guv  # type: ignore[misc]\n        GunicornWebWorker = gw  # type: ignore[misc]\n        return guv if name == \"GunicornUVLoopWebWorker\" else gw\n\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")"
    },
    {
      "chunk_id": 14,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "\"\"\"Low-level http related exceptions.\"\"\"\n\nfrom textwrap import indent\nfrom typing import Optional, Union\n\nfrom .typedefs import _CIMultiDict\n\n__all__ = (\"HttpProcessingError\",)"
    },
    {
      "chunk_id": 15,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class HttpProcessingError(Exception):\n    \"\"\"HTTP error.\n\n    Shortcut for raising HTTP errors with custom code, message and headers.\n\n    code: HTTP Error code.\n    message: (optional) Error message.\n    headers: (optional) Headers to be sent in response, a list of pairs\n    \"\"\"\n\n    code = 0\n    message = \"\"\n    headers = None\n\n    def __init__(\n        self,\n        *,\n        code: Optional[int] = None,\n        message: str = \"\",\n        headers: Optional[_CIMultiDict] = None,\n    ) -> None:\n        if code is not None:\n            self.code = code\n        self.headers = headers\n        self.message = message\n\n    def __str__(self) -> str:\n        msg = indent(self.message, \"  \")\n        return f\"{self.code}, message:\\n{msg}\"\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self.code}, message={self.message!r}>\""
    },
    {
      "chunk_id": 16,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class BadHttpMessage(HttpProcessingError):\n    code = 400\n    message = \"Bad Request\"\n\n    def __init__(self, message: str, *, headers: Optional[_CIMultiDict] = None) -> None:\n        super().__init__(message=message, headers=headers)\n        self.args = (message,)"
    },
    {
      "chunk_id": 17,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class HttpBadRequest(BadHttpMessage):\n    code = 400\n    message = \"Bad Request\""
    },
    {
      "chunk_id": 18,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class PayloadEncodingError(BadHttpMessage):\n    \"\"\"Base class for payload errors\"\"\""
    },
    {
      "chunk_id": 19,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class ContentEncodingError(PayloadEncodingError):\n    \"\"\"Content encoding error.\"\"\""
    },
    {
      "chunk_id": 20,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class TransferEncodingError(PayloadEncodingError):\n    \"\"\"transfer encoding error.\"\"\""
    },
    {
      "chunk_id": 21,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class ContentLengthError(PayloadEncodingError):\n    \"\"\"Not enough data for satisfy content length header.\"\"\""
    },
    {
      "chunk_id": 22,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class LineTooLong(BadHttpMessage):\n    def __init__(\n        self, line: str, limit: str = \"Unknown\", actual_size: str = \"Unknown\"\n    ) -> None:\n        super().__init__(\n            f\"Got more than {limit} bytes ({actual_size}) when reading {line}.\"\n        )\n        self.args = (line, limit, actual_size)"
    },
    {
      "chunk_id": 23,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class InvalidHeader(BadHttpMessage):\n    def __init__(self, hdr: Union[bytes, str]) -> None:\n        hdr_s = hdr.decode(errors=\"backslashreplace\") if isinstance(hdr, bytes) else hdr\n        super().__init__(f\"Invalid HTTP header: {hdr!r}\")\n        self.hdr = hdr_s\n        self.args = (hdr,)"
    },
    {
      "chunk_id": 24,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class BadStatusLine(BadHttpMessage):\n    def __init__(self, line: str = \"\", error: Optional[str] = None) -> None:\n        super().__init__(error or f\"Bad status line {line!r}\")\n        self.args = (line,)\n        self.line = line"
    },
    {
      "chunk_id": 25,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class BadHttpMethod(BadStatusLine):\n    \"\"\"Invalid HTTP method in status line.\"\"\"\n\n    def __init__(self, line: str = \"\", error: Optional[str] = None) -> None:\n        super().__init__(line, error or f\"Bad HTTP method in status line {line!r}\")"
    },
    {
      "chunk_id": 26,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_exceptions.py",
      "content": "class InvalidURLError(BadHttpMessage):\n    pass"
    },
    {
      "chunk_id": 27,
      "source": "__internal__/data_repo/aiohttp/aiohttp/resolver.py",
      "content": "import asyncio\nimport socket\nfrom typing import Any, List, Tuple, Type, Union\n\nfrom .abc import AbstractResolver, ResolveResult\n\n__all__ = (\"ThreadedResolver\", \"AsyncResolver\", \"DefaultResolver\")\n\n\ntry:\n    import aiodns\n\n    aiodns_default = hasattr(aiodns.DNSResolver, \"getaddrinfo\")\nexcept ImportError:  # pragma: no cover\n    aiodns = None  # type: ignore[assignment]\n    aiodns_default = False\n\n\n_NUMERIC_SOCKET_FLAGS = socket.AI_NUMERICHOST | socket.AI_NUMERICSERV\n_NAME_SOCKET_FLAGS = socket.NI_NUMERICHOST | socket.NI_NUMERICSERV"
    },
    {
      "chunk_id": 28,
      "source": "__internal__/data_repo/aiohttp/aiohttp/resolver.py",
      "content": "class ThreadedResolver(AbstractResolver):\n    \"\"\"Threaded resolver.\n\n    Uses an Executor for synchronous getaddrinfo() calls.\n    concurrent.futures.ThreadPoolExecutor is used by default.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._loop = asyncio.get_running_loop()\n\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        infos = await self._loop.getaddrinfo(\n            host,\n            port,\n            type=socket.SOCK_STREAM,\n            family=family,\n            flags=socket.AI_ADDRCONFIG,\n        )\n\n        hosts: List[ResolveResult] = []\n        for family, _, proto, _, address in infos:\n            if family == socket.AF_INET6:\n                if len(address) < 3:\n                    # IPv6 is not supported by Python build,\n                    # or IPv6 is not enabled in the host\n                    continue\n                if address[3]:\n                    # This is essential for link-local IPv6 addresses.\n                    # LL IPv6 is a VERY rare case. Strictly speaking, we should use\n                    # getnameinfo() unconditionally, but performance makes sense.\n                    resolved_host, _port = await self._loop.getnameinfo(\n                        address, _NAME_SOCKET_FLAGS\n                    )\n                    port = int(_port)\n                else:\n                    resolved_host, port = address[:2]\n            else:  # IPv4\n                assert family == socket.AF_INET\n                resolved_host, port = address  # type: ignore[misc]\n            hosts.append(\n                ResolveResult(\n                    hostname=host,\n                    host=resolved_host,\n                    port=port,\n                    family=family,\n                    proto=proto,\n                    flags=_NUMERIC_SOCKET_FLAGS,\n                )\n            )\n\n        return hosts\n\n    async def close(self) -> None:\n        pass"
    },
    {
      "chunk_id": 29,
      "source": "__internal__/data_repo/aiohttp/aiohttp/resolver.py",
      "content": "class AsyncResolver(AbstractResolver):\n    \"\"\"Use the `aiodns` package to make asynchronous DNS lookups\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        if aiodns is None:\n            raise RuntimeError(\"Resolver requires aiodns library\")\n\n        self._resolver = aiodns.DNSResolver(*args, **kwargs)\n\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        try:\n            resp = await self._resolver.getaddrinfo(\n                host,\n                port=port,\n                type=socket.SOCK_STREAM,\n                family=family,\n                flags=socket.AI_ADDRCONFIG,\n            )\n        except aiodns.error.DNSError as exc:\n            msg = exc.args[1] if len(exc.args) >= 1 else \"DNS lookup failed\"\n            raise OSError(None, msg) from exc\n        hosts: List[ResolveResult] = []\n        for node in resp.nodes:\n            address: Union[Tuple[bytes, int], Tuple[bytes, int, int, int]] = node.addr\n            family = node.family\n            if family == socket.AF_INET6:\n                if len(address) > 3 and address[3]:\n                    # This is essential for link-local IPv6 addresses.\n                    # LL IPv6 is a VERY rare case. Strictly speaking, we should use\n                    # getnameinfo() unconditionally, but performance makes sense.\n                    result = await self._resolver.getnameinfo(\n                        (address[0].decode(\"ascii\"), *address[1:]),\n                        _NAME_SOCKET_FLAGS,\n                    )\n                    resolved_host = result.node\n                else:\n                    resolved_host = address[0].decode(\"ascii\")\n                    port = address[1]\n            else:  # IPv4\n                assert family == socket.AF_INET\n                resolved_host = address[0].decode(\"ascii\")\n                port = address[1]\n            hosts.append(\n                ResolveResult(\n                    hostname=host,\n                    host=resolved_host,\n                    port=port,\n                    family=family,\n                    proto=0,\n                    flags=_NUMERIC_SOCKET_FLAGS,\n                )\n            )\n\n        if not hosts:\n            raise OSError(None, \"DNS lookup failed\")\n\n        return hosts\n\n    async def close(self) -> None:\n        self._resolver.cancel()"
    },
    {
      "chunk_id": 30,
      "source": "__internal__/data_repo/aiohttp/aiohttp/resolver.py",
      "content": "_DefaultType = Type[Union[AsyncResolver, ThreadedResolver]]\nDefaultResolver: _DefaultType = AsyncResolver if aiodns_default else ThreadedResolver"
    },
    {
      "chunk_id": 31,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_ws.py",
      "content": "```python"
    },
    {
      "chunk_id": 32,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_ws.py",
      "content": "import asyncio\nimport base64\nimport binascii\nimport hashlib\nimport json\nimport sys\nfrom typing import Any, Final, Iterable, Optional, Tuple, Union\n\nfrom multidict import CIMultiDict\n\nfrom . import hdrs\nfrom ._websocket.reader import WebSocketDataQueue\nfrom ._websocket.writer import DEFAULT_LIMIT\nfrom .abc import AbstractStreamWriter\nfrom .client_exceptions import WSMessageTypeError\nfrom .helpers import (\n    calculate_timeout_when,\n    frozen_dataclass_decorator,\n    set_exception,\n    set_result,\n)\nfrom .http import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WS_KEY,\n    WebSocketError,\n    WebSocketReader,\n    WebSocketWriter,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n    ws_ext_gen,\n    ws_ext_parse,\n)\nfrom .http_websocket import _INTERNAL_RECEIVE_TYPES, WSMessageError\nfrom .log import ws_logger\nfrom .streams import EofStream\nfrom .typedefs import JSONDecoder, JSONEncoder\nfrom .web_exceptions import HTTPBadRequest, HTTPException\nfrom .web_request import BaseRequest\nfrom .web_response import StreamResponse\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\n__all__ = (\n    \"WebSocketResponse\",\n    \"WebSocketReady\",\n    \"WSMsgType\",\n)\n\nTHRESHOLD_CONNLOST_ACCESS: Final[int] = 5"
    },
    {
      "chunk_id": 33,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_ws.py",
      "content": "@frozen_dataclass_decorator\nclass WebSocketReady:\n    ok: bool\n    protocol: Optional[str]\n\n    def __bool__(self) -> bool:\n        return self.ok"
    },
    {
      "chunk_id": 34,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_ws.py",
      "content": "class WebSocketResponse(StreamResponse):\n\n    _length_check: bool = False\n    _ws_protocol: Optional[str] = None\n    _writer: Optional[WebSocketWriter] = None\n    _reader: Optional[WebSocketDataQueue] = None\n    _closed: bool = False\n    _closing: bool = False\n    _conn_lost: int = 0\n    _close_code: Optional[int] = None\n    _loop: Optional[asyncio.AbstractEventLoop] = None\n    _waiting: bool = False\n    _close_wait: Optional[asyncio.Future[None]] = None\n    _exception: Optional[BaseException] = None\n    _heartbeat_when: float = 0.0\n    _heartbeat_cb: Optional[asyncio.TimerHandle] = None\n    _pong_response_cb: Optional[asyncio.TimerHandle] = None\n    _ping_task: Optional[asyncio.Task[None]] = None\n\n    def __init__(\n        self,\n        *,\n        timeout: float = 10.0,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        protocols: Iterable[str] = (),\n        compress: bool = True,\n        max_msg_size: int = 4 * 1024 * 1024,\n        writer_limit: int = DEFAULT_LIMIT,\n    ) -> None:\n        super().__init__(status=101)\n        self._protocols = protocols\n        self._timeout = timeout\n        self._receive_timeout = receive_timeout\n        self._autoclose = autoclose\n        self._autoping = autoping\n        self._heartbeat = heartbeat\n        if heartbeat is not None:\n            self._pong_heartbeat = heartbeat / 2.0\n        self._compress: Union[bool, int] = compress\n        self._max_msg_size = max_msg_size\n        self._writer_limit = writer_limit\n\n    def _cancel_heartbeat(self) -> None:\n        self._cancel_pong_response_cb()\n        if self._heartbeat_cb is not None:\n            self._heartbeat_cb.cancel()\n            self._heartbeat_cb = None\n        if self._ping_task is not None:\n            self._ping_task.cancel()\n            self._ping_task = None\n\n    def _cancel_pong_response_cb(self) -> None:\n        if self._pong_response_cb is not None:\n            self._pong_response_cb.cancel()\n            self._pong_response_cb = None\n\n    def _reset_heartbeat(self) -> None:\n        if self._heartbeat is None:\n            return\n        self._cancel_pong_response_cb()\n        req = self._req\n        timeout_ceil_threshold = (\n            req._protocol._timeout_ceil_threshold if req is not None else 5\n        )\n        loop = self._loop\n        assert loop is not None\n        now = loop.time()\n        when = calculate_timeout_when(now, self._heartbeat, timeout_ceil_threshold)\n        self._heartbeat_when = when\n        if self._heartbeat_cb is None:\n            self._heartbeat_cb = loop.call_at(when, self._send_heartbeat)\n\n    def _send_heartbeat(self) -> None:\n        self._heartbeat_cb = None\n        loop = self._loop\n        assert loop is not None and self._writer is not None\n        now = loop.time()\n        if now < self._heartbeat_when:\n            self._heartbeat_cb = loop.call_at(\n                self._heartbeat_when, self._send_heartbeat\n            )\n            return\n\n        req = self._req\n        timeout_ceil_threshold = (\n            req._protocol._timeout_ceil_threshold if req is not None else 5\n        )\n        when = calculate_timeout_when(now, self._pong_heartbeat, timeout_ceil_threshold)\n        self._cancel_pong_response_cb()\n        self._pong_response_cb = loop.call_at(when, self._pong_not_received)\n\n        coro = self._writer.send_frame(b\"\", WSMsgType.PING)\n        if sys.version_info >= (3, 12):\n            ping_task = asyncio.Task(coro, loop=loop, eager_start=True)\n        else:\n            ping_task = loop.create_task(coro)\n\n        if not ping_task.done():\n            self._ping_task = ping_task\n            ping_task.add_done_callback(self._ping_task_done)\n        else:\n            self._ping_task_done(ping_task)\n\n    def _ping_task_done(self, task: \"asyncio.Task[None]\") -> None:\n        \"\"\"Callback for when the ping task completes.\"\"\"\n        if not task.cancelled() and (exc := task.exception()):\n            self._handle_ping_pong_exception(exc)\n        self._ping_task = None\n\n    def _pong_not_received(self) -> None:\n        if self._req is not None and self._req.transport is not None:\n            self._handle_ping_pong_exception(asyncio.TimeoutError())\n\n    def _handle_ping_pong_exception(self, exc: BaseException) -> None:\n        \"\"\"Handle exceptions raised during ping/pong processing.\"\"\"\n        if self._closed:\n            return\n        self._set_closed()\n        self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n        self._exception = exc\n        if self._waiting and not self._closing and self._reader is not None:\n            self._reader.feed_data(WSMessageError(data=exc, extra=None))\n\n    def _set_closed(self) -> None:\n        \"\"\"Set the connection to closed.\n\n        Cancel any heartbeat timers and set the closed flag.\n        \"\"\"\n        self._closed = True\n        self._cancel_heartbeat()\n\n    async def prepare(self, request: BaseRequest) -> AbstractStreamWriter:\n        if self._payload_writer is not None:\n            return self._payload_writer\n\n        protocol, writer = self._pre_start(request)\n        payload_writer = await super().prepare(request)\n        assert payload_writer is not None\n        self._post_start(request, protocol, writer)\n        await payload_writer.drain()\n        return payload_writer\n\n    def _handshake(\n        self, request: BaseRequest\n    ) -> Tuple[\"CIMultiDict[str]\", Optional[str], int, bool]:\n        headers = request.headers\n        if \"websocket\" != headers.get(hdrs.UPGRADE, \"\").lower().strip():\n            raise HTTPBadRequest(\n                text=(\n                    \"No WebSocket UPGRADE hdr: {}\\n Can \"\n                    '\"Upgrade\" only to \"WebSocket\".'\n                ).format(headers.get(hdrs.UPGRADE))\n            )\n\n        if \"upgrade\" not in headers.get(hdrs.CONNECTION, \"\").lower():\n            raise HTTPBadRequest(\n                text=\"No CONNECTION upgrade hdr: {}\".format(\n                    headers.get(hdrs.CONNECTION)\n                )\n            )\n\n        protocol: Optional[str] = None\n        if hdrs.SEC_WEBSOCKET_PROTOCOL in headers:\n            req_protocols = [\n                str(proto.strip())\n                for proto in headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n            ]\n\n            for proto in req_protocols:\n                if proto in self._protocols:\n                    protocol = proto\n                    break\n            else:\n                ws_logger.warning(\n                    \"Client protocols %r don\u2019t overlap server-known ones %r\",\n                    req_protocols,\n                    self._protocols,\n                )\n\n        version = headers.get(hdrs.SEC_WEBSOCKET_VERSION, \"\")\n        if version not in (\"13\", \"8\", \"7\"):\n            raise HTTPBadRequest(text=f\"Unsupported version: {version}\")\n\n        key = headers.get(hdrs.SEC_WEBSOCKET_KEY)\n        try:\n            if not key or len(base64.b64decode(key)) != 16:\n                raise HTTPBadRequest(text=f\"Handshake error: {key!r}\")\n        except binascii.Error:\n            raise HTTPBadRequest(text=f\"Handshake error: {key!r}\") from None\n\n        accept_val = base64.b64encode(\n            hashlib.sha1(key.encode() + WS_KEY).digest()\n        ).decode()\n        response_headers = CIMultiDict(\n            {\n                hdrs.UPGRADE: \"websocket\",\n                hdrs.CONNECTION: \"upgrade\",\n                hdrs.SEC_WEBSOCKET_ACCEPT: accept_val,\n            }\n        )\n\n        notakeover = False\n        compress = 0\n        if self._compress:\n            extensions = headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\n            compress, notakeover = ws_ext_parse(extensions, isserver=True)\n            if compress:\n                enabledext = ws_ext_gen(\n                    compress=compress, isserver=True, server_notakeover=notakeover\n                )\n                response_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = enabledext\n\n        if protocol:\n            response_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = protocol\n        return (\n            response_headers,\n            protocol,\n            compress,\n            notakeover,\n        )\n\n    def _pre_start(self, request: BaseRequest) -> Tuple[Optional[str], WebSocketWriter]:\n        self._loop = request._loop\n\n        headers, protocol, compress, notakeover = self._handshake(request)\n\n        self.set_status(101)\n        self.headers.update(headers)\n        self.force_close()\n        self._compress = compress\n        transport = request._protocol.transport\n        assert transport is not None\n        writer = WebSocketWriter(\n            request._protocol,\n            transport,\n            compress=compress,\n            notakeover=notakeover,\n            limit=self._writer_limit,\n        )\n\n        return protocol, writer\n\n    def _post_start(\n        self, request: BaseRequest, protocol: Optional[str], writer: WebSocketWriter\n    ) -> None:\n        self._ws_protocol = protocol\n        self._writer = writer\n\n        self._reset_heartbeat()\n\n        loop = self._loop\n        assert loop is not None\n        self._reader = WebSocketDataQueue(request._protocol, 2**16, loop=loop)\n        request.protocol.set_parser(\n            WebSocketReader(\n                self._reader, self._max_msg_size, compress=bool(self._compress)\n            )\n        )\n        request.protocol.keep_alive(False)\n\n    def can_prepare(self, request: BaseRequest) -> WebSocketReady:\n        if self._writer is not None:\n            raise RuntimeError(\"Already started\")\n        try:\n            _, protocol, _, _ = self._handshake(request)\n        except HTTPException:\n            return WebSocketReady(False, None)\n        else:\n            return WebSocketReady(True, protocol)\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def close_code(self) -> Optional[int]:\n        return self._close_code\n\n    @property\n    def ws_protocol(self) -> Optional[str]:\n        return self._ws_protocol\n\n    @property\n    def compress(self) -> Union[int, bool]:\n        return self._compress\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"Get optional transport information.\n\n        If no value associated with ``name`` is found, ``default`` is returned.\n        \"\"\"\n        writer = self._writer\n        if writer is None:\n            return default\n        return writer.transport.get_extra_info(name, default)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    async def ping(self, message: bytes = b\"\") -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        await self._writer.send_frame(message, WSMsgType.PING)\n\n    async def pong(self, message: bytes = b\"\") -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        await self._writer.send_frame(message, WSMsgType.PONG)\n\n    async def send_frame(\n        self, message: bytes, opcode: WSMsgType, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket.\"\"\"\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        await self._writer.send_frame(message, opcode, compress)\n\n    async def send_str(self, data: str, compress: Optional[int] = None) -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        if not isinstance(data, str):\n            raise TypeError(\"data argument must be str (%r)\" % type(data))\n        await self._writer.send_frame(\n            data.encode(\"utf-8\"), WSMsgType.TEXT, compress=compress\n        )\n\n    async def send_bytes(self, data: bytes, compress: Optional[int] = None) -> None:\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data argument must be byte-ish (%r)\" % type(data))\n        await self._writer.send_frame(data, WSMsgType.BINARY, compress=compress)\n\n    async def send_json(\n        self,\n        data: Any,\n        compress: Optional[int] = None,\n        *,\n        dumps: JSONEncoder = json.dumps,\n    ) -> None:\n        await self.send_str(dumps(data), compress=compress)\n\n    async def write_eof(self) -> None:  # type: ignore[override]\n        if self._eof_sent:\n            return\n        if self._payload_writer is None:\n            raise RuntimeError(\"Response has not been started\")\n\n        await self.close()\n        self._eof_sent = True\n\n    async def close(\n        self, *, code: int = WSCloseCode.OK, message: bytes = b\"\", drain: bool = True\n    ) -> bool:\n        \"\"\"Close websocket connection.\"\"\"\n        if self._writer is None:\n            raise RuntimeError(\"Call .prepare() first\")\n\n        if self._closed:\n            return False\n        self._set_closed()\n\n        try:\n            await self._writer.close(code, message)\n            writer = self._payload_writer\n            assert writer is not None\n            if drain:\n                await writer.drain()\n        except (asyncio.CancelledError, asyncio.TimeoutError):\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            raise\n        except Exception as exc:\n            self._exception = exc\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            return True\n\n        reader = self._reader\n        assert reader is not None\n        if self._waiting:\n            assert self._loop is not None\n            assert self._close_wait is None\n            self._close_wait = self._loop.create_future()\n            reader.feed_data(WS_CLOSING_MESSAGE)\n            await self._close_wait\n\n        if self._closing:\n            self._close_transport()\n            return True\n\n        try:\n            async with async_timeout.timeout(self._timeout):\n                while True:\n                    msg = await reader.read()\n                    if msg.type is WSMsgType.CLOSE:\n                        self._set_code_close_transport(msg.data)\n                        return True\n        except asyncio.CancelledError:\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            raise\n        except Exception as exc:\n            self._exception = exc\n            self._set_code_close_transport(WSCloseCode.ABNORMAL_CLOSURE)\n            return True\n\n    def _set_closing(self, code: int) -> None:\n        \"\"\"Set the close code and mark the connection as closing.\"\"\"\n        self._closing = True\n        self._close_code = code\n        self._cancel_heartbeat()\n\n    def _set_code_close_transport(self, code: int) -> None:\n        \"\"\"Set the close code and close the transport.\"\"\"\n        self._close_code = code\n        self._close_transport()\n\n    def _close_transport(self) -> None:\n        \"\"\"Close the transport.\"\"\"\n        if self._req is not None and self._req.transport is not None:\n            self._req.transport.close()\n\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\n        if self._reader is None:\n            raise RuntimeError(\"Call .prepare() first\")\n\n        receive_timeout = timeout or self._receive_timeout\n        while True:\n            if self._waiting:\n                raise RuntimeError(\"Concurrent call to receive() is not allowed\")\n\n            if self._closed:\n                self._conn_lost += 1\n                if self._conn_lost >= THRESHOLD_CONNLOST_ACCESS:\n                    raise RuntimeError(\"WebSocket connection is closed.\")\n                return WS_CLOSED_MESSAGE\n            elif self._closing:\n                return WS_CLOSING_MESSAGE\n\n            try:\n                self._waiting = True\n                try:\n                    if receive_timeout:\n                        async with async_timeout.timeout(receive_timeout):\n                            msg = await self._reader.read()\n                    else:\n                        msg = await self._reader.read()\n                    self._reset_heartbeat()\n                finally:\n                    self._waiting = False\n                    if self._close_wait:\n                        set_result(self._close_wait, None)\n            except asyncio.TimeoutError:\n                raise\n            except EofStream:\n                self._close_code = WSCloseCode.OK\n                await self.close()\n                return WS_CLOSED_MESSAGE\n            except WebSocketError as exc:\n                self._close_code = exc.code\n                await self.close(code=exc.code)\n                return WSMessageError(data=exc)\n            except Exception as exc:\n                self._exception = exc\n                self._set_closing(WSCloseCode.ABNORMAL_CLOSURE)\n                await self.close()\n                return WSMessageError(data=exc)\n\n            if msg.type not in _INTERNAL_RECEIVE_TYPES:\n                return msg\n\n            if msg.type is WSMsgType.CLOSE:\n                self._set_closing(msg.data)\n                if not self._closed and self._autoclose:\n                    await self.close(drain=False)\n            elif msg.type is WSMsgType.CLOSING:\n                self._set_closing(WSCloseCode.OK)\n            elif msg.type is WSMsgType.PING and self._autoping:\n                await self.pong(msg.data)\n                continue\n            elif msg.type is WSMsgType.PONG and self._autoping:\n                continue\n\n            return msg\n\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.TEXT:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.TEXT\"\n            )\n        return msg.data\n\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.BINARY:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.BINARY\"\n            )\n        return msg.data\n\n    async def receive_json(\n        self, *, loads: JSONDecoder = json.loads, timeout: Optional[float] = None\n    ) -> Any:\n        data = await self.receive_str(timeout=timeout)\n        return loads(data)\n\n    async def write(self, data: bytes) -> None:\n        raise RuntimeError(\"Cannot call .write() for websocket\")\n\n    def __aiter__(self) -> \"WebSocketResponse\":\n        return self\n\n    async def __anext__(self) -> WSMessage:\n        msg = await self.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            raise StopAsyncIteration\n        return msg\n\n    def _cancel(self, exc: BaseException) -> None:\n        self._closing = True\n        self._cancel_heartbeat()\n        if self._reader is not None:\n            set_exception(self._reader, exc)\n```"
    },
    {
      "chunk_id": 35,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http.py",
      "content": "import sys"
    },
    {
      "chunk_id": 36,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http.py",
      "content": "from . import __version__\nfrom .http_exceptions import HttpProcessingError\nfrom .http_parser import (\n    HeadersParser,\n    HttpParser,\n    HttpRequestParser,\n    HttpResponseParser,\n    RawRequestMessage,\n    RawResponseMessage,\n)\nfrom .http_websocket import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WS_KEY,\n    WebSocketError,\n    WebSocketReader,\n    WebSocketWriter,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n    ws_ext_gen,\n    ws_ext_parse,\n)\nfrom .http_writer import HttpVersion, HttpVersion10, HttpVersion11, StreamWriter"
    },
    {
      "chunk_id": 37,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http.py",
      "content": "__all__ = (\n    \"HttpProcessingError\",\n    \"SERVER_SOFTWARE\",\n    # .http_writer\n    \"StreamWriter\",\n    \"HttpVersion\",\n    \"HttpVersion10\",\n    \"HttpVersion11\",\n    # .http_parser\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n    # .http_websocket\n    \"WS_CLOSED_MESSAGE\",\n    \"WS_CLOSING_MESSAGE\",\n    \"WS_KEY\",\n    \"WebSocketReader\",\n    \"WebSocketWriter\",\n    \"ws_ext_gen\",\n    \"ws_ext_parse\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n)"
    },
    {
      "chunk_id": 38,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http.py",
      "content": "SERVER_SOFTWARE: str = \"Python/{0[0]}.{0[1]} aiohttp/{1}\".format(\n    sys.version_info, __version__\n)"
    },
    {
      "chunk_id": 39,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_server.py",
      "content": "\"\"\"Low level HTTP server.\"\"\""
    },
    {
      "chunk_id": 40,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_server.py",
      "content": "import asyncio\nimport warnings\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    TypeVar,\n    overload,\n)\n\nfrom .abc import AbstractStreamWriter\nfrom .http_parser import RawRequestMessage\nfrom .streams import StreamReader\nfrom .web_protocol import RequestHandler\nfrom .web_request import BaseRequest\nfrom .web_response import StreamResponse\n\n__all__ = (\"Server\",)"
    },
    {
      "chunk_id": 41,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_server.py",
      "content": "_Request = TypeVar(\"_Request\", bound=BaseRequest)\n_RequestFactory = Callable[\n    [\n        RawRequestMessage,\n        StreamReader,\n        \"RequestHandler[_Request]\",\n        AbstractStreamWriter,\n        \"asyncio.Task[None]\",\n    ],\n    _Request,\n]"
    },
    {
      "chunk_id": 42,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_server.py",
      "content": "class Server(Generic[_Request]):\n    request_factory: _RequestFactory[_Request]\n\n    @overload\n    def __init__(\n        self: \"Server[BaseRequest]\",\n        handler: Callable[[_Request], Awaitable[StreamResponse]],\n        *,\n        debug: Optional[bool] = None,\n        handler_cancellation: bool = False,\n        **kwargs: Any,\n    ) -> None: ...\n    @overload\n    def __init__(\n        self,\n        handler: Callable[[_Request], Awaitable[StreamResponse]],\n        *,\n        request_factory: Optional[_RequestFactory[_Request]],\n        debug: Optional[bool] = None,\n        handler_cancellation: bool = False,\n        **kwargs: Any,\n    ) -> None: ...\n    def __init__(\n        self,\n        handler: Callable[[_Request], Awaitable[StreamResponse]],\n        *,\n        request_factory: Optional[_RequestFactory[_Request]] = None,\n        debug: Optional[bool] = None,\n        handler_cancellation: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        if debug is not None:\n            warnings.warn(\n                \"debug argument is no-op since 4.0 and scheduled for removal in 5.0\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self._loop = asyncio.get_running_loop()\n        self._connections: Dict[RequestHandler[_Request], asyncio.Transport] = {}\n        self._kwargs = kwargs\n        # requests_count is the number of requests being processed by the server\n        # for the lifetime of the server.\n        self.requests_count = 0\n        self.request_handler = handler\n        self.request_factory = request_factory or self._make_request  # type: ignore[assignment]\n        self.handler_cancellation = handler_cancellation\n\n    @property\n    def connections(self) -> List[RequestHandler[_Request]]:\n        return list(self._connections.keys())\n\n    def connection_made(\n        self, handler: RequestHandler[_Request], transport: asyncio.Transport\n    ) -> None:\n        self._connections[handler] = transport\n\n    def connection_lost(\n        self, handler: RequestHandler[_Request], exc: Optional[BaseException] = None\n    ) -> None:\n        if handler in self._connections:\n            if handler._task_handler:\n                handler._task_handler.add_done_callback(\n                    lambda f: self._connections.pop(handler, None)\n                )\n            else:\n                del self._connections[handler]\n\n    def _make_request(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: RequestHandler[BaseRequest],\n        writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n    ) -> BaseRequest:\n        return BaseRequest(message, payload, protocol, writer, task, self._loop)\n\n    def pre_shutdown(self) -> None:\n        for conn in self._connections:\n            conn.close()\n\n    async def shutdown(self, timeout: Optional[float] = None) -> None:\n        coros = (conn.shutdown(timeout) for conn in self._connections)\n        await asyncio.gather(*coros)\n        self._connections.clear()\n\n    def __call__(self) -> RequestHandler[_Request]:\n        try:\n            return RequestHandler(self, loop=self._loop, **self._kwargs)\n        except TypeError:\n            # Failsafe creation: remove all custom handler_args\n            kwargs = {\n                k: v\n                for k, v in self._kwargs.items()\n                if k in [\"debug\", \"access_log_class\"]\n            }\n            return RequestHandler(self, loop=self._loop, **kwargs)"
    },
    {
      "chunk_id": 43,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_response.py",
      "content": "import asyncio\nimport collections.abc\nimport datetime\nimport enum\nimport json\nimport math\nimport time\nimport warnings\nimport zlib\nfrom concurrent.futures import Executor\nfrom http import HTTPStatus\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    MutableMapping,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom multidict import CIMultiDict, istr\n\nfrom . import hdrs, payload\nfrom .abc import AbstractStreamWriter\nfrom .compression_utils import ZLibCompressor\nfrom .helpers import (\n    ETAG_ANY,\n    QUOTED_ETAG_RE,\n    CookieMixin,\n    ETag,\n    HeadersMixin,\n    must_be_empty_body,\n    parse_http_date,\n    populate_with_cookies,\n    rfc822_formatted_time,\n    sentinel,\n    should_remove_content_length,\n    validate_etag_value,\n)\nfrom .http import SERVER_SOFTWARE, HttpVersion10, HttpVersion11\nfrom .payload import Payload\nfrom .typedefs import JSONEncoder, LooseHeaders\n\nREASON_PHRASES = {http_status.value: http_status.phrase for http_status in HTTPStatus}\nLARGE_BODY_SIZE = 1024**2\n\n__all__ = (\"ContentCoding\", \"StreamResponse\", \"Response\", \"json_response\")"
    },
    {
      "chunk_id": 44,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_response.py",
      "content": "class ContentCoding(enum.Enum):\n    deflate = \"deflate\"\n    gzip = \"gzip\"\n    identity = \"identity\"\n\n\nCONTENT_CODINGS = {coding.value: coding for coding in ContentCoding}"
    },
    {
      "chunk_id": 45,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_response.py",
      "content": "class StreamResponse(BaseClass, HeadersMixin, CookieMixin):\n\n    _body: Union[None, bytes, bytearray, Payload]\n    _length_check = True\n    _body = None\n    _keep_alive: Optional[bool] = None\n    _chunked: bool = False\n    _compression: bool = False\n    _compression_strategy: int = zlib.Z_DEFAULT_STRATEGY\n    _compression_force: Optional[ContentCoding] = None\n    _req: Optional[\"BaseRequest\"] = None\n    _payload_writer: Optional[AbstractStreamWriter] = None\n    _eof_sent: bool = False\n    _must_be_empty_body: Optional[bool] = None\n    _body_length = 0\n\n    def __init__(\n        self,\n        *,\n        status: int = 200,\n        reason: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n        _real_headers: Optional[CIMultiDict[str]] = None,\n    ) -> None:\n        self._state: Dict[str, Any] = {}\n\n        if _real_headers is not None:\n            self._headers = _real_headers\n        elif headers is not None:\n            self._headers: CIMultiDict[str] = CIMultiDict(headers)\n        else:\n            self._headers = CIMultiDict()\n\n        self._set_status(status, reason)\n\n    @property\n    def prepared(self) -> bool:\n        return self._eof_sent or self._payload_writer is not None\n\n    @property\n    def task(self) -> \"Optional[asyncio.Task[None]]\":\n        if self._req:\n            return self._req.task\n        else:\n            return None\n\n    @property\n    def status(self) -> int:\n        return self._status\n\n    @property\n    def chunked(self) -> bool:\n        return self._chunked\n\n    @property\n    def compression(self) -> bool:\n        return self._compression\n\n    @property\n    def reason(self) -> str:\n        return self._reason\n\n    def set_status(\n        self,\n        status: int,\n        reason: Optional[str] = None,\n    ) -> None:\n        assert (\n            not self.prepared\n        ), \"Cannot change the response status code after the headers have been sent\"\n        self._set_status(status, reason)\n\n    def _set_status(self, status: int, reason: Optional[str]) -> None:\n        self._status = status\n        if reason is None:\n            reason = REASON_PHRASES.get(self._status, \"\")\n        elif \"\\n\" in reason:\n            raise ValueError(\"Reason cannot contain \\\\n\")\n        self._reason = reason\n\n    @property\n    def keep_alive(self) -> Optional[bool]:\n        return self._keep_alive\n\n    def force_close(self) -> None:\n        self._keep_alive = False\n\n    @property\n    def body_length(self) -> int:\n        return self._body_length\n\n    def enable_chunked_encoding(self) -> None:\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\n                \"You can't enable chunked encoding when a content length is set\"\n            )\n        self._chunked = True\n\n    def enable_compression(\n        self,\n        force: Optional[ContentCoding] = None,\n        strategy: int = zlib.Z_DEFAULT_STRATEGY,\n    ) -> None:\n        self._compression = True\n        self._compression_force = force\n        self._compression_strategy = strategy\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        return self._headers\n\n    @property\n    def content_length(self) -> Optional[int]:\n        return super().content_length\n\n    @content_length.setter\n    def content_length(self, value: Optional[int]) -> None:\n        if value is not None:\n            value = int(value)\n            if self._chunked:\n                raise RuntimeError(\n                    \"You can't set content length when chunked encoding is enable\"\n                )\n            self._headers[hdrs.CONTENT_LENGTH] = str(value)\n        else:\n            self._headers.pop(hdrs.CONTENT_LENGTH, None)\n\n    @property\n    def content_type(self) -> str:\n        return super().content_type\n\n    @content_type.setter\n    def content_type(self, value: str) -> None:\n        self.content_type  # read header values if needed\n        self._content_type = str(value)\n        self._generate_content_type_header()\n\n    @property\n    def charset(self) -> Optional[str]:\n        return super().charset\n\n    @charset.setter\n    def charset(self, value: Optional[str]) -> None:\n        ctype = self.content_type  # read header values if needed\n        if ctype == \"application/octet-stream\":\n            raise RuntimeError(\n                \"Setting charset for application/octet-stream \"\n                \"doesn't make sense, setup content_type first\"\n            )\n        assert self._content_dict is not None\n        if value is None:\n            self._content_dict.pop(\"charset\", None)\n        else:\n            self._content_dict[\"charset\"] = str(value).lower()\n        self._generate_content_type_header()\n\n    @property\n    def last_modified(self) -> Optional[datetime.datetime]:\n        return parse_http_date(self._headers.get(hdrs.LAST_MODIFIED))\n\n    @last_modified.setter\n    def last_modified(\n        self, value: Optional[Union[int, float, datetime.datetime, str]]\n    ) -> None:\n        if value is None:\n            self._headers.pop(hdrs.LAST_MODIFIED, None)\n        elif isinstance(value, (int, float)):\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\n                \"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(math.ceil(value))\n            )\n        elif isinstance(value, datetime.datetime):\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\n                \"%a, %d %b %Y %H:%M:%S GMT\", value.utctimetuple()\n            )\n        elif isinstance(value, str):\n            self._headers[hdrs.LAST_MODIFIED] = value\n        else:\n            msg = f\"Unsupported type for last_modified: {type(value).__name__}\"  # type: ignore[unreachable]\n            raise TypeError(msg)\n\n    @property\n    def etag(self) -> Optional[ETag]:\n        quoted_value = self._headers.get(hdrs.ETAG)\n        if not quoted_value:\n            return None\n        elif quoted_value == ETAG_ANY:\n            return ETag(value=ETAG_ANY)\n        match = QUOTED_ETAG_RE.fullmatch(quoted_value)\n        if not match:\n            return None\n        is_weak, value = match.group(1, 2)\n        return ETag(\n            is_weak=bool(is_weak),\n            value=value,\n        )\n\n    @etag.setter\n    def etag(self, value: Optional[Union[ETag, str]]) -> None:\n        if value is None:\n            self._headers.pop(hdrs.ETAG, None)\n        elif (isinstance(value, str) and value == ETAG_ANY) or (\n            isinstance(value, ETag) and value.value == ETAG_ANY\n        ):\n            self._headers[hdrs.ETAG] = ETAG_ANY\n        elif isinstance(value, str):\n            validate_etag_value(value)\n            self._headers[hdrs.ETAG] = f'\"{value}\"'\n        elif isinstance(value, ETag) and isinstance(value.value, str):  # type: ignore[redundant-expr]\n            validate_etag_value(value.value)\n            hdr_value = f'W/\"{value.value}\"' if value.is_weak else f'\"{value.value}\"'\n            self._headers[hdrs.ETAG] = hdr_value\n        else:\n            raise ValueError(\n                f\"Unsupported etag type: {type(value)}. \"\n                f\"etag must be str, ETag or None\"\n            )\n\n    def _generate_content_type_header(\n        self, CONTENT_TYPE: istr = hdrs.CONTENT_TYPE\n    ) -> None:\n        assert self._content_dict is not None\n        assert self._content_type is not None\n        params = \"; \".join(f\"{k}={v}\" for k, v in self._content_dict.items())\n        if params:\n            ctype = self._content_type + \"; \" + params\n        else:\n            ctype = self._content_type\n        self._headers[CONTENT_TYPE] = ctype\n\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\n        if coding is ContentCoding.identity:\n            return\n        assert self._payload_writer is not None\n        self._headers[hdrs.CONTENT_ENCODING] = coding.value\n        self._payload_writer.enable_compression(\n            coding.value, self._compression_strategy\n        )\n        self._headers.popall(hdrs.CONTENT_LENGTH, None)\n\n    async def _start_compression(self, request: \"BaseRequest\") -> None:\n        if self._compression_force:\n            await self._do_start_compression(self._compression_force)\n            return\n        accept_encoding = request.headers.get(hdrs.ACCEPT_ENCODING, \"\").lower()\n        for value, coding in CONTENT_CODINGS.items():\n            if value in accept_encoding:\n                await self._do_start_compression(coding)\n                return\n\n    async def prepare(self, request: \"BaseRequest\") -> Optional[AbstractStreamWriter]:\n        if self._eof_sent:\n            return None\n        if self._payload_writer is not None:\n            return self._payload_writer\n        self._must_be_empty_body = must_be_empty_body(request.method, self.status)\n        return await self._start(request)\n\n    async def _start(self, request: \"BaseRequest\") -> AbstractStreamWriter:\n        self._req = request\n        writer = self._payload_writer = request._payload_writer\n\n        await self._prepare_headers()\n        await request._prepare_hook(self)\n        await self._write_headers()\n\n        return writer\n\n    async def _prepare_headers(self) -> None:\n        request = self._req\n        assert request is not None\n        writer = self._payload_writer\n        assert writer is not None\n        keep_alive = self._keep_alive\n        if keep_alive is None:\n            keep_alive = request.keep_alive\n        self._keep_alive = keep_alive\n\n        version = request.version\n\n        headers = self._headers\n        if self._cookies:\n            populate_with_cookies(headers, self._cookies)\n\n        if self._compression:\n            await self._start_compression(request)\n\n        if self._chunked:\n            if version != HttpVersion11:\n                raise RuntimeError(\n                    \"Using chunked encoding is forbidden \"\n                    \"for HTTP/{0.major}.{0.minor}\".format(request.version)\n                )\n            if not self._must_be_empty_body:\n                writer.enable_chunking()\n                headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n        elif self._length_check:  \n            writer.length = self.content_length\n            if writer.length is None:\n                if version >= HttpVersion11:\n                    if not self._must_be_empty_body:\n                        writer.enable_chunking()\n                        headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n                elif not self._must_be_empty_body:\n                    keep_alive = False\n\n        if self._must_be_empty_body:\n            if hdrs.CONTENT_LENGTH in headers and should_remove_content_length(\n                request.method, self.status\n            ):\n                del headers[hdrs.CONTENT_LENGTH]\n            if hdrs.TRANSFER_ENCODING in headers:\n                del headers[hdrs.TRANSFER_ENCODING]\n        elif (writer.length if self._length_check else self.content_length) != 0:\n            headers.setdefault(hdrs.CONTENT_TYPE, \"application/octet-stream\")\n        headers.setdefault(hdrs.DATE, rfc822_formatted_time())\n        headers.setdefault(hdrs.SERVER, SERVER_SOFTWARE)\n\n        if hdrs.CONNECTION not in headers:\n            if keep_alive:\n                if version == HttpVersion10:\n                    headers[hdrs.CONNECTION] = \"keep-alive\"\n            elif version == HttpVersion11:\n                headers[hdrs.CONNECTION] = \"close\"\n\n    async def _write_headers(self) -> None:\n        request = self._req\n        assert request is not None\n        writer = self._payload_writer\n        assert writer is not None\n        version = request.version\n        status_line = f\"HTTP/{version[0]}.{version[1]} {self._status} {self._reason}\"\n        await writer.write_headers(status_line, self._headers)\n\n    async def write(self, data: Union[bytes, bytearray, memoryview]) -> None:\n        assert isinstance(\n            data, (bytes, bytearray, memoryview)\n        ), \"data argument must be byte-ish (%r)\" % type(data)\n\n        if self._eof_sent:\n            raise RuntimeError(\"Cannot call write() after write_eof()\")\n        if self._payload_writer is None:\n            raise RuntimeError(\"Cannot call write() before prepare()\")\n\n        await self._payload_writer.write(data)\n\n    async def drain(self) -> None:\n        assert not self._eof_sent, \"EOF has already been sent\"\n        assert self._payload_writer is not None, \"Response has not been started\"\n        warnings.warn(\n            \"drain method is deprecated, use await resp.write()\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        await self._payload_writer.drain()\n\n    async def write_eof(self, data: bytes = b\"\") -> None:\n        assert isinstance(\n            data, (bytes, bytearray, memoryview)\n        ), \"data argument must be byte-ish (%r)\" % type(data)\n\n        if self._eof_sent:\n            return\n\n        assert self._payload_writer is not None, \"Response has not been started\"\n\n        await self._payload_writer.write_eof(data)\n        self._eof_sent = True\n        self._req = None\n        self._body_length = self._payload_writer.output_size\n        self._payload_writer = None\n\n    def __repr__(self) -> str:\n        if self._eof_sent:\n            info = \"eof\"\n        elif self.prepared:\n            assert self._req is not None\n            info = f\"{self._req.method} {self._req.path} \"\n        else:\n            info = \"not prepared\"\n        return f\"<{self.__class__.__name__} {self.reason} {info}>\"\n\n    def __getitem__(self, key: str) -> Any:\n        return self._state[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self._state[key] = value\n\n    def __delitem__(self, key: str) -> None:\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._state)\n\n    def __hash__(self) -> int:\n        return hash(id(self))\n\n    def __eq__(self, other: object) -> bool:\n        return self is other\n\n    def __bool__(self) -> bool:\n        return True"
    },
    {
      "chunk_id": 46,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_response.py",
      "content": "class Response(StreamResponse):\n\n    _compressed_body: Optional[bytes] = None\n\n    def __init__(\n        self,\n        *,\n        body: Any = None,\n        status: int = 200,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n        content_type: Optional[str] = None,\n        charset: Optional[str] = None,\n        zlib_executor_size: Optional[int] = None,\n        zlib_executor: Optional[Executor] = None,\n    ) -> None:\n        if body is not None and text is not None:\n            raise ValueError(\"body and text are not allowed together\")\n\n        if headers is None:\n            real_headers: CIMultiDict[str] = CIMultiDict()\n        elif not isinstance(headers, CIMultiDict):\n            real_headers = CIMultiDict(headers)\n        else:\n            real_headers = headers\n\n        if content_type is not None and \"charset\" in content_type:\n            raise ValueError(\"charset must not be in content_type argument\")\n\n        if text is not None:\n            if hdrs.CONTENT_TYPE in real_headers:\n                if content_type or charset:\n                    raise ValueError(\n                        \"passing both Content-Type header and \"\n                        \"content_type or charset params \"\n                        \"is forbidden\"\n                    )\n            else:\n                if not isinstance(text, str):\n                    raise TypeError(\"text argument must be str (%r)\" % type(text))\n                if content_type is None:\n                    content_type = \"text/plain\"\n                if charset is None:\n                    charset = \"utf-8\"\n                real_headers[hdrs.CONTENT_TYPE] = content_type + \"; charset=\" + charset\n                body = text.encode(charset)\n                text = None\n        elif hdrs.CONTENT_TYPE in real_headers:\n            if content_type is not None or charset is not None:\n                raise ValueError(\n                    \"passing both Content-Type header and \"\n                    \"content_type or charset params \"\n                    \"is forbidden\"\n                )\n        elif content_type is not None:\n            if charset is not None:\n                content_type += \"; charset=\" + charset\n            real_headers[hdrs.CONTENT_TYPE] = content_type\n\n        super().__init__(status=status, reason=reason, _real_headers=real_headers)\n\n        if text is not None:\n            self.text = text\n        else:\n            self.body = body\n\n        self._zlib_executor_size = zlib_executor_size\n        self._zlib_executor = zlib_executor\n\n    @property\n    def body(self) -> Optional[Union[bytes, Payload]]:\n        return self._body\n\n    @body.setter\n    def body(self, body: Any) -> None:\n        if body is None:\n            self._body = None\n        elif isinstance(body, (bytes, bytearray)):\n            self._body = body\n        else:\n            try:\n                self._body = body = payload.PAYLOAD_REGISTRY.get(body)\n            except payload.LookupError:\n                raise ValueError(\"Unsupported body type %r\" % type(body))\n\n            headers = self._headers\n\n            if hdrs.CONTENT_TYPE not in headers:\n                headers[hdrs.CONTENT_TYPE] = body.content_type\n\n            if body.headers:\n                for key, value in body.headers.items():\n                    if key not in headers:\n                        headers[key] = value\n\n        self._compressed_body = None\n\n    @property\n    def text(self) -> Optional[str]:\n        if self._body is None:\n            return None\n        return self._body.decode(self.charset or \"utf-8\")\n\n    @text.setter\n    def text(self, text: str) -> None:\n        assert isinstance(text, str), \"text argument must be str (%r)\" % type(text)\n\n        if self.content_type == \"application/octet-stream\":\n            self.content_type = \"text/plain\"\n        if self.charset is None:\n            self.charset = \"utf-8\"\n\n        self._body = text.encode(self.charset)\n        self._compressed_body = None\n\n    @property\n    def content_length(self) -> Optional[int]:\n        if self._chunked:\n            return None\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            return int(self._headers[hdrs.CONTENT_LENGTH])\n\n        if self._compressed_body is not None:\n            return len(self._compressed_body)\n        elif isinstance(self._body, Payload):\n            return None\n        elif self._body is not None:\n            return len(self._body)\n        else:\n            return 0\n\n    @content_length.setter\n    def content_length(self, value: Optional[int]) -> None:\n        raise RuntimeError(\"Content length is set automatically\")\n\n    async def write_eof(self, data: bytes = b\"\") -> None:\n        if self._eof_sent:\n            return\n        if self._compressed_body is None:\n            body: Optional[Union[bytes, Payload]] = self._body\n        else:\n            body = self._compressed_body\n        assert not data, f\"data arg is not supported, got {data!r}\"\n        assert self._req is not None\n        assert self._payload_writer is not None\n        if body is None or self._must_be_empty_body:\n            await super().write_eof()\n        elif isinstance(self._body, Payload):\n            await self._body.write(self._payload_writer)\n            await super().write_eof()\n        else:\n            await super().write_eof(cast(bytes, body))\n\n    async def _start(self, request: \"BaseRequest\") -> AbstractStreamWriter:\n        if hdrs.CONTENT_LENGTH in self._headers:\n            if should_remove_content_length(request.method, self.status):\n                del self._headers[hdrs.CONTENT_LENGTH]\n        elif not self._chunked:\n            if isinstance(self._body, Payload):\n                if self._body.size is not None:\n                    self._headers[hdrs.CONTENT_LENGTH] = str(self._body.size)\n            else:\n                body_len = len(self._body) if self._body else \"0\"\n                if body_len != \"0\" or (\n                    self.status != 304 and request.method not in hdrs.METH_HEAD_ALL\n                ):\n                    self._headers[hdrs.CONTENT_LENGTH] = str(body_len)\n\n        return await super()._start(request)\n\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\n        if self._chunked or isinstance(self._body, Payload):\n            return await super()._do_start_compression(coding)\n        if coding is ContentCoding.identity:\n            return\n        compressor = ZLibCompressor(\n            encoding=coding.value,\n            max_sync_chunk_size=self._zlib_executor_size,\n            executor=self._zlib_executor,\n        )\n        assert self._body is not None\n        if self._zlib_executor_size is None and len(self._body) > LARGE_BODY_SIZE:\n            warnings.warn(\n                \"Synchronous compression of large response bodies \"\n                f\"({len(self._body)} bytes) might block the async event loop. \"\n                \"Consider providing a custom value to zlib_executor_size/\"\n                \"zlib_executor response properties or disabling compression on it.\"\n            )\n        self._compressed_body = (\n            await compressor.compress(self._body) + compressor.flush()\n        )\n        self._headers[hdrs.CONTENT_ENCODING] = coding.value\n        self._headers[hdrs.CONTENT_LENGTH] = str(len(self._compressed_body))"
    },
    {
      "chunk_id": 47,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_response.py",
      "content": "def json_response(\n    data: Any = sentinel,\n    *,\n    text: Optional[str] = None,\n    body: Optional[bytes] = None,\n    status: int = 200,\n    reason: Optional[str] = None,\n    headers: Optional[LooseHeaders] = None,\n    content_type: str = \"application/json\",\n    dumps: JSONEncoder = json.dumps,\n) -> Response:\n    if data is not sentinel:\n        if text or body:\n            raise ValueError(\"only one of data, text, or body should be specified\")\n        else:\n            text = dumps(data)\n    return Response(\n        text=text,\n        body=body,\n        status=status,\n        reason=reason,\n        headers=headers,\n        content_type=content_type,\n    )"
    },
    {
      "chunk_id": 48,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "```python"
    },
    {
      "chunk_id": 49,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "import abc\nimport dataclasses\nimport os  # noqa\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    Union,\n    overload,\n)\n\nfrom . import hdrs\nfrom .abc import AbstractView\nfrom .typedefs import Handler, PathLike\n\nif TYPE_CHECKING:\n    from .web_request import Request\n    from .web_response import StreamResponse\n    from .web_urldispatcher import AbstractRoute, UrlDispatcher\nelse:\n    Request = StreamResponse = UrlDispatcher = AbstractRoute = None\n\n\n__all__ = (\n    \"AbstractRouteDef\",\n    \"RouteDef\",\n    \"StaticDef\",\n    \"RouteTableDef\",\n    \"head\",\n    \"options\",\n    \"get\",\n    \"post\",\n    \"patch\",\n    \"put\",\n    \"delete\",\n    \"route\",\n    \"view\",\n    \"static\",\n)"
    },
    {
      "chunk_id": 50,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "class AbstractRouteDef(abc.ABC):\n    @abc.abstractmethod\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        pass  # pragma: no cover"
    },
    {
      "chunk_id": 51,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "_HandlerType = Union[Type[AbstractView], Handler]"
    },
    {
      "chunk_id": 52,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "@dataclasses.dataclass(frozen=True, repr=False)\nclass RouteDef(AbstractRouteDef):\n    method: str\n    path: str\n    handler: _HandlerType\n    kwargs: Dict[str, Any]\n\n    def __repr__(self) -> str:\n        info = []\n        for name, value in sorted(self.kwargs.items()):\n            info.append(f\", {name}={value!r}\")\n        return \"<RouteDef {method} {path} -> {handler.__name__!r}{info}>\".format(\n            method=self.method, path=self.path, handler=self.handler, info=\"\".join(info)\n        )\n\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        if self.method in hdrs.METH_ALL:\n            reg = getattr(router, \"add_\" + self.method.lower())\n            return [reg(self.path, self.handler, **self.kwargs)]\n        else:\n            return [\n                router.add_route(self.method, self.path, self.handler, **self.kwargs)\n            ]"
    },
    {
      "chunk_id": 53,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "@dataclasses.dataclass(frozen=True, repr=False)\nclass StaticDef(AbstractRouteDef):\n    prefix: str\n    path: PathLike\n    kwargs: Dict[str, Any]\n\n    def __repr__(self) -> str:\n        info = []\n        for name, value in sorted(self.kwargs.items()):\n            info.append(f\", {name}={value!r}\")\n        return \"<StaticDef {prefix} -> {path}{info}>\".format(\n            prefix=self.prefix, path=self.path, info=\"\".join(info)\n        )\n\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\n        resource = router.add_static(self.prefix, self.path, **self.kwargs)\n        routes = resource.get_info().get(\"routes\", {})\n        return list(routes.values())"
    },
    {
      "chunk_id": 54,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def route(method: str, path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return RouteDef(method, path, handler, kwargs)"
    },
    {
      "chunk_id": 55,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def head(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_HEAD, path, handler, **kwargs)"
    },
    {
      "chunk_id": 56,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def options(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_OPTIONS, path, handler, **kwargs)"
    },
    {
      "chunk_id": 57,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def get(\n    path: str,\n    handler: _HandlerType,\n    *,\n    name: Optional[str] = None,\n    allow_head: bool = True,\n    **kwargs: Any,\n) -> RouteDef:\n    return route(\n        hdrs.METH_GET, path, handler, name=name, allow_head=allow_head, **kwargs\n    )"
    },
    {
      "chunk_id": 58,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def post(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_POST, path, handler, **kwargs)"
    },
    {
      "chunk_id": 59,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def put(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_PUT, path, handler, **kwargs)"
    },
    {
      "chunk_id": 60,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def patch(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_PATCH, path, handler, **kwargs)"
    },
    {
      "chunk_id": 61,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def delete(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_DELETE, path, handler, **kwargs)"
    },
    {
      "chunk_id": 62,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def view(path: str, handler: Type[AbstractView], **kwargs: Any) -> RouteDef:\n    return route(hdrs.METH_ANY, path, handler, **kwargs)"
    },
    {
      "chunk_id": 63,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "def static(prefix: str, path: PathLike, **kwargs: Any) -> StaticDef:\n    return StaticDef(prefix, path, kwargs)"
    },
    {
      "chunk_id": 64,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "_Deco = Callable[[_HandlerType], _HandlerType]"
    },
    {
      "chunk_id": 65,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_routedef.py",
      "content": "class RouteTableDef(Sequence[AbstractRouteDef]):\n    \"\"\"Route definition table\"\"\"\n\n    def __init__(self) -> None:\n        self._items: List[AbstractRouteDef] = []\n\n    def __repr__(self) -> str:\n        return f\"<RouteTableDef count={len(self._items)}>\"\n\n    @overload\n    def __getitem__(self, index: int) -> AbstractRouteDef: ...\n\n    @overload\n    def __getitem__(self, index: slice) -> List[AbstractRouteDef]: ...\n\n    def __getitem__(self, index):  # type: ignore[no-untyped-def]\n        return self._items[index]\n\n    def __iter__(self) -> Iterator[AbstractRouteDef]:\n        return iter(self._items)\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __contains__(self, item: object) -> bool:\n        return item in self._items\n\n    def route(self, method: str, path: str, **kwargs: Any) -> _Deco:\n        def inner(handler: _HandlerType) -> _HandlerType:\n            self._items.append(RouteDef(method, path, handler, kwargs))\n            return handler\n\n        return inner\n\n    def head(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_HEAD, path, **kwargs)\n\n    def get(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_GET, path, **kwargs)\n\n    def post(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_POST, path, **kwargs)\n\n    def put(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_PUT, path, **kwargs)\n\n    def patch(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_PATCH, path, **kwargs)\n\n    def delete(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_DELETE, path, **kwargs)\n\n    def options(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_OPTIONS, path, **kwargs)\n\n    def view(self, path: str, **kwargs: Any) -> _Deco:\n        return self.route(hdrs.METH_ANY, path, **kwargs)\n\n    def static(self, prefix: str, path: PathLike, **kwargs: Any) -> None:\n        self._items.append(StaticDef(prefix, path, kwargs))\n```"
    },
    {
      "chunk_id": 66,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_request.py",
      "content": "```python"
    },
    {
      "chunk_id": 67,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_request.py",
      "content": "import asyncio\nimport datetime\nimport io\nimport re\nimport socket\nimport string\nimport sys\nimport tempfile\nimport types\nfrom http.cookies import SimpleCookie\nfrom types import MappingProxyType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Final,\n    Iterator,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Pattern,\n    Tuple,\n    Union,\n    cast,\n)\nfrom urllib.parse import parse_qsl\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import (\n    _SENTINEL,\n    ETAG_ANY,\n    LIST_QUOTED_ETAG_RE,\n    ChainMapProxy,\n    ETag,\n    HeadersMixin,\n    frozen_dataclass_decorator,\n    is_expected_content_type,\n    parse_http_date,\n    reify,\n    sentinel,\n    set_exception,\n)\nfrom .http_parser import RawRequestMessage\nfrom .http_writer import HttpVersion\nfrom .multipart import BodyPartReader, MultipartReader\nfrom .streams import EmptyStreamReader, StreamReader\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    JSONDecoder,\n    LooseHeaders,\n    RawHeaders,\n    StrOrURL,\n)\nfrom .web_exceptions import (\n    HTTPBadRequest,\n    HTTPRequestEntityTooLarge,\n    HTTPUnsupportedMediaType,\n)\nfrom .web_response import StreamResponse\n\nif sys.version_info >= (3, 11):\n    from typing import Self\nelse:\n    Self = Any\n\n__all__ = (\"BaseRequest\", \"FileField\", \"Request\")"
    },
    {
      "chunk_id": 68,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_request.py",
      "content": "@frozen_dataclass_decorator\nclass FileField:\n    name: str\n    filename: str\n    file: io.BufferedReader\n    content_type: str\n    headers: CIMultiDictProxy[str]"
    },
    {
      "chunk_id": 69,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_request.py",
      "content": "class BaseRequest(MutableMapping[str, Any], HeadersMixin):\n    POST_METHODS = {\n        hdrs.METH_PATCH,\n        hdrs.METH_POST,\n        hdrs.METH_PUT,\n        hdrs.METH_TRACE,\n        hdrs.METH_DELETE,\n    }\n\n    _post: Optional[MultiDictProxy[Union[str, bytes, FileField]]] = None\n    _read_bytes: Optional[bytes] = None\n\n    def __init__(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: \"RequestHandler[Self]\",\n        payload_writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n        loop: asyncio.AbstractEventLoop,\n        *,\n        client_max_size: int = 1024**2,\n        state: Optional[Dict[str, Any]] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        remote: Optional[str] = None,\n    ) -> None:\n        self._message = message\n        self._protocol = protocol\n        self._payload_writer = payload_writer\n\n        self._payload = payload\n        self._headers: CIMultiDictProxy[str] = message.headers\n        self._method = message.method\n        self._version = message.version\n        self._cache: Dict[str, Any] = {}\n        url = message.url\n        if url.absolute:\n            if scheme is not None:\n                url = url.with_scheme(scheme)\n            if host is not None:\n                url = url.with_host(host)\n            self._cache[\"url\"] = url\n            self._cache[\"host\"] = url.host\n            self._cache[\"scheme\"] = url.scheme\n            self._rel_url = url.relative()\n        else:\n            self._rel_url = url\n            if scheme is not None:\n                self._cache[\"scheme\"] = scheme\n            if host is not None:\n                self._cache[\"host\"] = host\n\n        self._state = {} if state is None else state\n        self._task = task\n        self._client_max_size = client_max_size\n        self._loop = loop\n\n        transport = protocol.transport\n        assert transport is not None\n        self._transport_sslcontext = transport.get_extra_info(\"sslcontext\")\n        self._transport_peername = transport.get_extra_info(\"peername\")\n\n        if remote is not None:\n            self._cache[\"remote\"] = remote\n\n    def clone(\n        self,\n        *,\n        method: Union[str, _SENTINEL] = sentinel,\n        rel_url: Union[StrOrURL, _SENTINEL] = sentinel,\n        headers: Union[LooseHeaders, _SENTINEL] = sentinel,\n        scheme: Union[str, _SENTINEL] = sentinel,\n        host: Union[str, _SENTINEL] = sentinel,\n        remote: Union[str, _SENTINEL] = sentinel,\n        client_max_size: Union[int, _SENTINEL] = sentinel,\n    ) -> \"BaseRequest\":\n        if self._read_bytes:\n            raise RuntimeError(\"Cannot clone request after reading its content\")\n\n        dct: Dict[str, Any] = {}\n        if method is not sentinel:\n            dct[\"method\"] = method\n        if rel_url is not sentinel:\n            new_url: URL = URL(rel_url)\n            dct[\"url\"] = new_url\n            dct[\"path\"] = str(new_url)\n        if headers is not sentinel:\n            new_headers = CIMultiDictProxy(CIMultiDict(headers))\n            dct[\"headers\"] = new_headers\n            dct[\"raw_headers\"] = tuple(\n                (k.encode(\"utf-8\"), v.encode(\"utf-8\")) for k, v in new_headers.items()\n            )\n\n        message = self._message._replace(**dct)\n\n        kwargs: Dict[str, str] = {}\n        if scheme is not sentinel:\n            kwargs[\"scheme\"] = scheme\n        if host is not sentinel:\n            kwargs[\"host\"] = host\n        if remote is not sentinel:\n            kwargs[\"remote\"] = remote\n        if client_max_size is sentinel:\n            client_max_size = self._client_max_size\n\n        return self.__class__(\n            message,\n            self._payload,\n            self._protocol,  # type: ignore[arg-type]\n            self._payload_writer,\n            self._task,\n            self._loop,\n            client_max_size=client_max_size,\n            state=self._state.copy(),\n            **kwargs,\n        )\n\n    @property\n    def task(self) -> \"asyncio.Task[None]\":\n        return self._task\n\n    @property\n    def protocol(self) -> \"RequestHandler[Self]\":\n        return self._protocol\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        return self._protocol.transport\n\n    @property\n    def writer(self) -> AbstractStreamWriter:\n        return self._payload_writer\n\n    @property\n    def client_max_size(self) -> int:\n        return self._client_max_size\n\n    @reify\n    def rel_url(self) -> URL:\n        return self._rel_url\n\n    # MutableMapping API\n\n    def __getitem__(self, key: str) -> Any:\n        return self._state[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self._state[key] = value\n\n    def __delitem__(self, key: str) -> None:\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._state)\n\n    ########\n\n    @reify\n    def secure(self) -> bool:\n        return self.scheme == \"https\"\n\n    @reify\n    def forwarded(self) -> Tuple[Mapping[str, str], ...]:\n        elems = []\n        for field_value in self._message.headers.getall(hdrs.FORWARDED, ()):\n            length = len(field_value)\n            pos = 0\n            need_separator = False\n            elem: Dict[str, str] = {}\n            elems.append(types.MappingProxyType(elem))\n            while 0 <= pos < length:\n                match = _FORWARDED_PAIR_RE.match(field_value, pos)\n                if match is not None:\n                    if need_separator:\n                        pos = field_value.find(\",\", pos)\n                    else:\n                        name, value, port = match.groups()\n                        if value[0] == '\"':\n                            value = _QUOTED_PAIR_REPLACE_RE.sub(r\"\\1\", value[1:-1])\n                        if port:\n                            value += port\n                        elem[name.lower()] = value\n                        pos += len(match.group(0))\n                        need_separator = True\n                elif field_value[pos] == \",\":\n                    need_separator = False\n                    elem = {}\n                    elems.append(types.MappingProxyType(elem))\n                    pos += 1\n                elif field_value[pos] == \";\":\n                    need_separator = False\n                    pos += 1\n                elif field_value[pos] in \" \\t\":\n                    pos += 1\n                else:\n                    pos = field_value.find(\",\", pos)\n        return tuple(elems)\n\n    @reify\n    def scheme(self) -> str:\n        if self._transport_sslcontext:\n            return \"https\"\n        else:\n            return \"http\"\n\n    @reify\n    def method(self) -> str:\n        return self._method\n\n    @reify\n    def version(self) -> HttpVersion:\n        return self._version\n\n    @reify\n    def host(self) -> str:\n        host = self._message.headers.get(hdrs.HOST)\n        if host is not None:\n            return host\n        return socket.getfqdn()\n\n    @reify\n    def remote(self) -> Optional[str]:\n        if self._transport_peername is None:\n            return None\n        if isinstance(self._transport_peername, (list, tuple)):\n            return str(self._transport_peername[0])\n        return str(self._transport_peername)\n\n    @reify\n    def url(self) -> URL:\n        return URL.build(scheme=self.scheme, authority=self.host).join(self._rel_url)\n\n    @reify\n    def path(self) -> str:\n        return self._rel_url.path\n\n    @reify\n    def path_qs(self) -> str:\n        return str(self._rel_url)\n\n    @reify\n    def raw_path(self) -> str:\n        return self._message.path\n\n    @reify\n    def query(self) -> MultiDictProxy[str]:\n        return self._rel_url.query\n\n    @reify\n    def query_string(self) -> str:\n        return self._rel_url.query_string\n\n    @reify\n    def headers(self) -> CIMultiDictProxy[str]:\n        return self._headers\n\n    @reify\n    def raw_headers(self) -> RawHeaders:\n        return self._message.raw_headers\n\n    @reify\n    def if_modified_since(self) -> Optional[datetime.datetime]:\n        return parse_http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))\n\n    @reify\n    def if_unmodified_since(self) -> Optional[datetime.datetime]:\n        return parse_http_date(self.headers.get(hdrs.IF_UNMODIFIED_SINCE))\n\n    @staticmethod\n    def _etag_values(etag_header: str) -> Iterator[ETag]:\n        if etag_header == ETAG_ANY:\n            yield ETag(\n                is_weak=False,\n                value=ETAG_ANY,\n            )\n        else:\n            for match in LIST_QUOTED_ETAG_RE.finditer(etag_header):\n                is_weak, value, garbage = match.group(2, 3, 4)\n                if garbage:\n                    break\n\n                yield ETag(\n                    is_weak=bool(is_weak),\n                    value=value,\n                )\n\n    @classmethod\n    def _if_match_or_none_impl(\n        cls, header_value: Optional[str]\n    ) -> Optional[Tuple[ETag, ...]]:\n        if not header_value:\n            return None\n\n        return tuple(cls._etag_values(header_value))\n\n    @reify\n    def if_match(self) -> Optional[Tuple[ETag, ...]]:\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_MATCH))\n\n    @reify\n    def if_none_match(self) -> Optional[Tuple[ETag, ...]]:\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_NONE_MATCH))\n\n    @reify\n    def if_range(self) -> Optional[datetime.datetime]:\n        return parse_http_date(self.headers.get(hdrs.IF_RANGE))\n\n    @reify\n    def keep_alive(self) -> bool:\n        return not self._message.should_close\n\n    @reify\n    def cookies(self) -> Mapping[str, str]:\n        raw = self.headers.get(hdrs.COOKIE, \"\")\n        parsed = SimpleCookie(raw)\n        return MappingProxyType({key: val.value for key, val in parsed.items()})\n\n    @reify\n    def http_range(self) -> slice:\n        rng = self._headers.get(hdrs.RANGE)\n        start, end = None, None\n        if rng is not None:\n            try:\n                pattern = r\"^bytes=(\\d*)-(\\d*)$\"\n                start, end = re.findall(pattern, rng)[0]\n            except IndexError:\n                raise ValueError(\"range not in acceptable format\")\n\n            end = int(end) if end else None\n            start = int(start) if start else None\n\n            if start is None and end is not None:\n                start = -end\n                end = None\n\n            if start is not None and end is not None:\n                end += 1\n\n                if start >= end:\n                    raise ValueError(\"start cannot be after end\")\n\n            if start is end is None:\n                raise ValueError(\"No start or end of range specified\")\n\n        return slice(start, end, 1)\n\n    @reify\n    def content(self) -> StreamReader:\n        return self._payload\n\n    @property\n    def can_read_body(self) -> bool:\n        return not self._payload.at_eof()\n\n    @reify\n    def body_exists(self) -> bool:\n        return type(self._payload) is not EmptyStreamReader\n\n    async def release(self) -> None:\n        while not self._payload.at_eof():\n            await self._payload.readany()\n\n    async def read(self) -> bytes:\n        if self._read_bytes is None:\n            body = bytearray()\n            while True:\n                chunk = await self._payload.readany()\n                body.extend(chunk)\n                if self._client_max_size:\n                    body_size = len(body)\n                    if body_size > self._client_max_size:\n                        raise HTTPRequestEntityTooLarge(\n                            max_size=self._client_max_size, actual_size=body_size\n                        )\n                if not chunk:\n                    break\n            self._read_bytes = bytes(body)\n        return self._read_bytes\n\n    async def text(self) -> str:\n        bytes_body = await self.read()\n        encoding = self.charset or \"utf-8\"\n        try:\n            return bytes_body.decode(encoding)\n        except LookupError:\n            raise HTTPUnsupportedMediaType()\n\n    async def json(\n        self,\n        *,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        content_type: Optional[str] = \"application/json\",\n    ) -> Any:\n        body = await self.text()\n        if content_type:\n            if not is_expected_content_type(self.content_type, content_type):\n                raise HTTPBadRequest(\n                    text=(\n                        \"Attempt to decode JSON with \"\n                        \"unexpected mimetype: %s\" % self.content_type\n                    )\n                )\n\n        return loads(body)\n\n    async def multipart(self) -> MultipartReader:\n        return MultipartReader(self._headers, self._payload)\n\n    async def post(self) -> \"MultiDictProxy[Union[str, bytes, FileField]]\":\n        if self._post is not None:\n            return self._post\n        if self._method not in self.POST_METHODS:\n            self._post = MultiDictProxy(MultiDict())\n            return self._post\n\n        content_type = self.content_type\n        if content_type not in (\n            \"\",\n            \"application/x-www-form-urlencoded\",\n            \"multipart/form-data\",\n        ):\n            self._post = MultiDictProxy(MultiDict())\n            return self._post\n\n        out: MultiDict[Union[str, bytes, FileField]] = MultiDict()\n\n        if content_type == \"multipart/form-data\":\n            multipart = await self.multipart()\n            max_size = self._client_max_size\n\n            field = await multipart.next()\n            while field is not None:\n                size = 0\n                field_ct = field.headers.get(hdrs.CONTENT_TYPE)\n\n                if isinstance(field, BodyPartReader):\n                    assert field.name is not None\n\n                    if field.filename:\n                        tmp = await self._loop.run_in_executor(\n                            None, tempfile.TemporaryFile\n                        )\n                        chunk = await field.read_chunk(size=2**16)\n                        while chunk:\n                            chunk = field.decode(chunk)\n                            await self._loop.run_in_executor(None, tmp.write, chunk)\n                            size += len(chunk)\n                            if 0 < max_size < size:\n                                await self._loop.run_in_executor(None, tmp.close)\n                                raise HTTPRequestEntityTooLarge(\n                                    max_size=max_size, actual_size=size\n                                )\n                            chunk = await field.read_chunk(size=2**16)\n                        await self._loop.run_in_executor(None, tmp.seek, 0)\n\n                        if field_ct is None:\n                            field_ct = \"application/octet-stream\"\n\n                        ff = FileField(\n                            field.name,\n                            field.filename,\n                            cast(io.BufferedReader, tmp),\n                            field_ct,\n                            field.headers,\n                        )\n                        out.add(field.name, ff)\n                    else:\n                        value = await field.read(decode=True)\n                        if field_ct is None or field_ct.startswith(\"text/\"):\n                            charset = field.get_charset(default=\"utf-8\")\n                            out.add(field.name, value.decode(charset))\n                        else:\n                            out.add(field.name, value)\n                        size += len(value)\n                        if 0 < max_size < size:\n                            raise HTTPRequestEntityTooLarge(\n                                max_size=max_size, actual_size=size\n                            )\n                else:\n                    raise ValueError(\n                        \"To decode nested multipart you need to use custom reader\",\n                    )\n\n                field = await multipart.next()\n        else:\n            data = await self.read()\n            if data:\n                charset = self.charset or \"utf-8\"\n                bytes_query = data.rstrip()\n                try:\n                    query = bytes_query.decode(charset)\n                except LookupError:\n                    raise HTTPUnsupportedMediaType()\n                out.extend(\n                    parse_qsl(qs=query, keep_blank_values=True, encoding=charset)\n                )\n\n        self._post = MultiDictProxy(out)\n        return self._post\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        transport = self._protocol.transport\n        if transport is None:\n            return default\n\n        return transport.get_extra_info(name, default)\n\n    def __repr__(self) -> str:\n        ascii_encodable_path = self.path.encode(\"ascii\", \"backslashreplace\").decode(\n            \"ascii\"\n        )\n        return \"<{} {} {} >\".format(\n            self.__class__.__name__, self._method, ascii_encodable_path\n        )\n\n    def __eq__(self, other: object) -> bool:\n        return id(self) == id(other)\n\n    def __bool__(self) -> bool:\n        return True\n\n    async def _prepare_hook(self, response: StreamResponse) -> None:\n        return\n\n    def _cancel(self, exc: BaseException) -> None:\n        set_exception(self._payload, exc)\n\n    def _finish(self) -> None:\n        if self._post is None or self.content_type != \"multipart/form-data\":\n            return\n\n        for file_name, file_field_object in self._post.items():\n            if isinstance(file_field_object, FileField):\n                file_field_object.file.close()"
    },
    {
      "chunk_id": 70,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_request.py",
      "content": "class Request(BaseRequest):\n\n    _match_info: Optional[\"UrlMappingMatchInfo\"] = None\n\n    def clone(\n        self,\n        *,\n        method: Union[str, _SENTINEL] = sentinel,\n        rel_url: Union[StrOrURL, _SENTINEL] = sentinel,\n        headers: Union[LooseHeaders, _SENTINEL] = sentinel,\n        scheme: Union[str, _SENTINEL] = sentinel,\n        host: Union[str, _SENTINEL] = sentinel,\n        remote: Union[str, _SENTINEL] = sentinel,\n        client_max_size: Union[int, _SENTINEL] = sentinel,\n    ) -> \"Request\":\n        ret = super().clone(\n            method=method,\n            rel_url=rel_url,\n            headers=headers,\n            scheme=scheme,\n            host=host,\n            remote=remote,\n            client_max_size=client_max_size,\n        )\n        new_ret = cast(Request, ret)\n        new_ret._match_info = self._match_info\n        return new_ret\n\n    @reify\n    def match_info(self) -> \"UrlMappingMatchInfo\":\n        match_info = self._match_info\n        assert match_info is not None\n        return match_info\n\n    @property\n    def app(self) -> \"Application\":\n        match_info = self._match_info\n        assert match_info is not None\n        return match_info.current_app\n\n    @property\n    def config_dict(self) -> ChainMapProxy:\n        match_info = self._match_info\n        assert match_info is not None\n        lst = match_info.apps\n        app = self.app\n        idx = lst.index(app)\n        sublist = list(reversed(lst[: idx + 1]))\n        return ChainMapProxy(sublist)\n\n    async def _prepare_hook(self, response: StreamResponse) -> None:\n        match_info = self._match_info\n        if match_info is None:\n            return\n        for app in match_info._apps:\n            if on_response_prepare := app.on_response_prepare:\n                await on_response_prepare.send(self, response)\n```"
    },
    {
      "chunk_id": 71,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "```python"
    },
    {
      "chunk_id": 72,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "import abc\nimport asyncio\nimport re\nimport string\nfrom contextlib import suppress\nfrom enum import IntEnum\nfrom typing import (\n    Any,\n    ClassVar,\n    Final,\n    Generic,\n    List,\n    Literal,\n    NamedTuple,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .base_protocol import BaseProtocol\nfrom .compression_utils import HAS_BROTLI, BrotliDecompressor, ZLibDecompressor\nfrom .helpers import (\n    _EXC_SENTINEL,\n    DEBUG,\n    EMPTY_BODY_METHODS,\n    EMPTY_BODY_STATUS_CODES,\n    NO_EXTENSIONS,\n    BaseTimerContext,\n    set_exception,\n)\nfrom .http_exceptions import (\n    BadHttpMessage,\n    BadHttpMethod,\n    BadStatusLine,\n    ContentEncodingError,\n    ContentLengthError,\n    InvalidHeader,\n    InvalidURLError,\n    LineTooLong,\n    TransferEncodingError,\n)\nfrom .http_writer import HttpVersion, HttpVersion10\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .typedefs import RawHeaders\n\n__all__ = (\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n)\n\n_SEP = Literal[b\"\\r\\n\", b\"\\n\"]\n\nASCIISET: Final[Set[str]] = set(string.printable)\n\n_TCHAR_SPECIALS: Final[str] = re.escape(\"!#$%&'*+-.^_`|~\")\nTOKENRE: Final[Pattern[str]] = re.compile(f\"[0-9A-Za-z{_TCHAR_SPECIALS}]+\")\nVERSRE: Final[Pattern[str]] = re.compile(r\"HTTP/(\\d)\\.(\\d)\", re.ASCII)\nDIGITS: Final[Pattern[str]] = re.compile(r\"\\d+\", re.ASCII)\nHEXDIGITS: Final[Pattern[bytes]] = re.compile(rb\"[0-9a-fA-F]+\")"
    },
    {
      "chunk_id": 73,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class RawRequestMessage(NamedTuple):\n    method: str\n    path: str\n    version: HttpVersion\n    headers: CIMultiDictProxy[str]\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool\n    url: URL"
    },
    {
      "chunk_id": 74,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class RawResponseMessage(NamedTuple):\n    version: HttpVersion\n    code: int\n    reason: str\n    headers: CIMultiDictProxy[str]\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool"
    },
    {
      "chunk_id": 75,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "_MsgT = TypeVar(\"_MsgT\", RawRequestMessage, RawResponseMessage)"
    },
    {
      "chunk_id": 76,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class ParseState(IntEnum):\n    PARSE_NONE = 0\n    PARSE_LENGTH = 1\n    PARSE_CHUNKED = 2\n    PARSE_UNTIL_EOF = 3"
    },
    {
      "chunk_id": 77,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class ChunkState(IntEnum):\n    PARSE_CHUNKED_SIZE = 0\n    PARSE_CHUNKED_CHUNK = 1\n    PARSE_CHUNKED_CHUNK_EOF = 2\n    PARSE_MAYBE_TRAILERS = 3\n    PARSE_TRAILERS = 4"
    },
    {
      "chunk_id": 78,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class HeadersParser:\n    def __init__(\n        self, max_line_size: int = 8190, max_field_size: int = 8190, lax: bool = False\n    ) -> None:\n        self.max_line_size = max_line_size\n        self.max_field_size = max_field_size\n        self._lax = lax\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            if len(bname) == 0:\n                raise InvalidHeader(bname)\n\n            if {bname[0], bname[-1]} & {32, 9}:  # {\" \", \"\\t\"}\n                raise InvalidHeader(line)\n\n            bvalue = bvalue.lstrip(b\" \\t\")\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"backslashreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            if not TOKENRE.fullmatch(name):\n                raise InvalidHeader(bname)\n\n            header_length = len(bvalue)\n\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            continuation = self._lax and line and line[0] in (32, 9)\n\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"backslashreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"backslashreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip(b\" \\t\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            if \"\\n\" in value or \"\\r\" in value or \"\\x00\" in value:\n                raise InvalidHeader(bvalue)\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))"
    },
    {
      "chunk_id": 79,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "def _is_supported_upgrade(headers: CIMultiDictProxy[str]) -> bool:\n    \"\"\"Check if the upgrade header is supported.\"\"\"\n    return headers.get(hdrs.UPGRADE, \"\").lower() in {\"tcp\", \"websocket\"}"
    },
    {
      "chunk_id": 80,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class HttpParser(abc.ABC, Generic[_MsgT]):\n    lax: ClassVar[bool] = False\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        loop: asyncio.AbstractEventLoop,\n        limit: int,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        timer: Optional[BaseTimerContext] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        payload_exception: Optional[Type[BaseException]] = None,\n        response_with_body: bool = True,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n    ) -> None:\n        self.protocol = protocol\n        self.loop = loop\n        self.max_line_size = max_line_size\n        self.max_field_size = max_field_size\n        self.timer = timer\n        self.code = code\n        self.method = method\n        self.payload_exception = payload_exception\n        self.response_with_body = response_with_body\n        self.read_until_eof = read_until_eof\n\n        self._lines: List[bytes] = []\n        self._tail = b\"\"\n        self._upgraded = False\n        self._payload = None\n        self._payload_parser: Optional[HttpPayloadParser] = None\n        self._auto_decompress = auto_decompress\n        self._limit = limit\n        self._headers_parser = HeadersParser(max_line_size, max_field_size, self.lax)\n\n    @abc.abstractmethod\n    def parse_message(self, lines: List[bytes]) -> _MsgT: ...\n\n    @abc.abstractmethod\n    def _is_chunked_te(self, te: str) -> bool: ...\n\n    def feed_eof(self) -> Optional[_MsgT]:\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n        else:\n            if self._tail:\n                self._lines.append(self._tail)\n\n            if self._lines:\n                if self._lines[-1] != \"\\r\\n\":\n                    self._lines.append(b\"\")\n                with suppress(Exception):\n                    return self.parse_message(self._lines)\n        return None\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: _SEP = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        should_close = False\n        while start_pos < data_len:\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + len(SEP)\n                    continue\n\n                if pos >= start_pos:\n                    if should_close:\n                        raise BadHttpMessage(\"Data after `Connection: close`\")\n\n                    line = data[start_pos:pos]\n                    if SEP == b\"\\n\":\n                        line = line.rstrip(b\"\\r\")\n                    self._lines.append(line)\n                    start_pos = pos + len(SEP)\n\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            if not DIGITS.fullmatch(length_hdr):\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade and _is_supported_upgrade(\n                            msg.headers\n                        )\n\n                        method = getattr(msg, \"method\", self.method)\n                        code = getattr(msg, \"code\", 0)\n\n                        assert self.protocol is not None\n                        empty_body = code in EMPTY_BODY_STATUS_CODES or bool(\n                            method and method in EMPTY_BODY_METHODS\n                        )\n                        if not empty_body and (\n                            ((length is not None and length > 0) or msg.chunked)\n                            and not self._upgraded\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                        elif not empty_body and length is None and self.read_until_eof:\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                                lax=self.lax,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        else:\n                            payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                        should_close = msg.should_close\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:], SEP)\n                except BaseException as underlying_exc:\n                    reraised_exc = underlying_exc\n                    if self.payload_exception is not None:\n                        reraised_exc = self.payload_exception(str(underlying_exc))\n\n                    set_exception(\n                        self._payload_parser.payload,\n                        reraised_exc,\n                        underlying_exc,\n                    )\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        singletons = (\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_LOCATION,\n            hdrs.CONTENT_RANGE,\n            hdrs.CONTENT_TYPE,\n            hdrs.ETAG,\n            hdrs.HOST,\n            hdrs.MAX_FORWARDS,\n            hdrs.SERVER,\n            hdrs.TRANSFER_ENCODING,\n            hdrs.USER_AGENT,\n        )\n        bad_hdr = next((h for h in singletons if len(headers.getall(h, ())) > 1), None)\n        if bad_hdr is not None:\n            raise BadHttpMessage(f\"Duplicate '{bad_hdr}' header found.\")\n\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            elif v == \"upgrade\" and headers.get(hdrs.UPGRADE):\n                upgrade = True\n\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te is not None:\n            if self._is_chunked_te(te):\n                chunked = True\n\n            if hdrs.CONTENT_LENGTH in headers:\n                raise BadHttpMessage(\n                    \"Transfer-Encoding can't be present with Content-Length\",\n                )\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\n\n    def set_upgraded(self, val: bool) -> None:\n        \"\"\"Set connection upgraded (to websocket) mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._upgraded = val"
    },
    {
      "chunk_id": 81,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class HttpRequestParser(HttpParser[RawRequestMessage]):\n    \"\"\"Read request status line.\n\n    Exception .http_exceptions.BadStatusLine\n    could be raised in case of any errors in status line.\n    Returns RawRequestMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(\" \", maxsplit=2)\n        except ValueError:\n            raise BadHttpMethod(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        if not TOKENRE.fullmatch(method):\n            raise BadHttpMethod(method)\n\n        match = VERSRE.fullmatch(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        if method == \"CONNECT\":\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        elif path == \"*\" and method == \"OPTIONS\":\n            url = URL(path, encoded=True)\n        else:\n            url = URL(path, encoded=True)\n            if url.scheme == \"\":\n                raise InvalidURLError(\n                    path.encode(errors=\"surrogateescape\").decode(\"latin1\")\n                )\n\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:\n            if version_o <= HttpVersion10:\n                close = True\n            else:\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n\n    def _is_chunked_te(self, te: str) -> bool:\n        if te.rsplit(\",\", maxsplit=1)[-1].strip(\" \\t\").lower() == \"chunked\":\n            return True\n        raise BadHttpMessage(\"Request has invalid `Transfer-Encoding`\")"
    },
    {
      "chunk_id": 82,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class HttpResponseParser(HttpParser[RawResponseMessage]):\n    \"\"\"Read response status line and headers.\n\n    BadStatusLine could be raised in case of any errors in status line.\n    Returns RawResponseMessage.\n    \"\"\"\n\n    lax = not DEBUG\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: Optional[_SEP] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Tuple[List[Tuple[RawResponseMessage, StreamReader]], bool, bytes]:\n        if SEP is None:\n            SEP = b\"\\r\\n\" if DEBUG else b\"\\n\"\n            assert SEP is not None\n        return super().feed_data(data, SEP, *args, **kwargs)\n\n    def parse_message(self, lines: List[bytes]) -> RawResponseMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            version, status = line.split(maxsplit=1)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        try:\n            status, reason = status.split(maxsplit=1)\n        except ValueError:\n            status = status.strip()\n            reason = \"\"\n\n        if len(reason) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(reason))\n            )\n\n        match = VERSRE.fullmatch(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        if len(status) != 3 or not DIGITS.fullmatch(status):\n            raise BadStatusLine(line)\n        status_i = int(status)\n\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:\n            if version_o <= HttpVersion10:\n                close = True\n            elif 100 <= status_i < 200 or status_i in {204, 304}:\n                close = False\n            elif hdrs.CONTENT_LENGTH in headers or hdrs.TRANSFER_ENCODING in headers:\n                close = False\n            else:\n                close = True\n\n        return RawResponseMessage(\n            version_o,\n            status_i,\n            reason.strip(),\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        )\n\n    def _is_chunked_te(self, te: str) -> bool:\n        return te.rsplit(\",\", maxsplit=1)[-1].strip(\" \\t\").lower() == \"chunked\""
    },
    {
      "chunk_id": 83,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class HttpPayloadParser:\n    def __init__(\n        self,\n        payload: StreamReader,\n        length: Optional[int] = None,\n        chunked: bool = False,\n        compression: Optional[str] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        response_with_body: bool = True,\n        auto_decompress: bool = True,\n        lax: bool = False,\n    ) -> None:\n        self._length = 0\n        self._type = ParseState.PARSE_UNTIL_EOF\n        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n        self._chunk_size = 0\n        self._chunk_tail = b\"\"\n        self._auto_decompress = auto_decompress\n        self._lax = lax\n        self.done = False\n\n        if response_with_body and compression and self._auto_decompress:\n            real_payload: Union[StreamReader, DeflateBuffer] = DeflateBuffer(\n                payload, compression\n            )\n        else:\n            real_payload = payload\n\n        if not response_with_body:\n            self._type = ParseState.PARSE_NONE\n            real_payload.feed_eof()\n            self.done = True\n        elif chunked:\n            self._type = ParseState.PARSE_CHUNKED\n        elif length is not None:\n            self._type = ParseState.PARSE_LENGTH\n            self._length = length\n            if self._length == 0:\n                real_payload.feed_eof()\n                self.done = True\n\n        self.payload = real_payload\n\n    def feed_eof(self) -> None:\n        if self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_eof()\n        elif self._type == ParseState.PARSE_LENGTH:\n            raise ContentLengthError(\n                \"Not enough data for satisfy content length header.\"\n            )\n        elif self._type == ParseState.PARSE_CHUNKED:\n            raise TransferEncodingError(\n                \"Not enough data for satisfy transfer length header.\"\n            )\n\n    def feed_data(\n        self, chunk: bytes, SEP: _SEP = b\"\\r\\n\", CHUNK_EXT: bytes = b\";\"\n    ) -> Tuple[bool, bytes]:\n        if self._type == ParseState.PARSE_LENGTH:\n            required = self._length\n            self._length = max(required - len(chunk), 0)\n            self.payload.feed_data(chunk[:required])\n            if self._length == 0:\n                self.payload.feed_eof()\n                return True, chunk[required:]\n\n        elif self._type == ParseState.PARSE_CHUNKED:\n            if self._chunk_tail:\n                chunk = self._chunk_tail + chunk\n                self._chunk_tail = b\"\"\n\n            while chunk:\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        i = chunk.find(CHUNK_EXT, 0, pos)\n                        if i >= 0:\n                            size_b = chunk[:i]\n                            if b\"\\n\" in (ext := chunk[i:pos]):\n                                exc = BadHttpMessage(\n                                    f\"Unexpected LF in chunk-extension: {ext!r}\"\n                                )\n                                set_exception(self.payload, exc)\n                                raise exc\n                        else:\n                            size_b = chunk[:pos]\n\n                        if self._lax:\n                            size_b = size_b.strip()\n\n                        if not re.fullmatch(HEXDIGITS, size_b):\n                            exc = TransferEncodingError(\n                                chunk[:pos].decode(\"ascii\", \"surrogateescape\")\n                            )\n                            set_exception(self.payload, exc)\n                            raise exc\n                        size = int(bytes(size_b), 16)\n\n                        chunk = chunk[pos + len(SEP) :]\n                        if size == 0:\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                            if self._lax and chunk.startswith(b\"\\r\"):\n                                chunk = chunk[1:]\n                        else:\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\n                            self._chunk_size = size\n                            self.payload.begin_http_chunk_receiving()\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\n                    required = self._chunk_size\n                    self._chunk_size = max(required - len(chunk), 0)\n                    self.payload.feed_data(chunk[:required])\n\n                    if self._chunk_size:\n                        return False, b\"\"\n                    chunk = chunk[required:]\n                    self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\n                    self.payload.end_http_chunk_receiving()\n\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\n                    if self._lax and chunk.startswith(b\"\\r\"):\n                        chunk = chunk[1:]\n                    if chunk[: len(SEP)] == SEP:\n                        chunk = chunk[len(SEP) :]\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\n                    head = chunk[: len(SEP)]\n                    if head == SEP:\n                        self.payload.feed_eof()\n                        return True, chunk[len(SEP) :]\n                    if not head:\n                        return False, b\"\"\n                    if head == SEP[:1]:\n                        self._chunk_tail = head\n                        return False, b\"\"\n                    self._chunk = ChunkState.PARSE_TRAILERS\n\n                if self._chunk == ChunkState.PARSE_TRAILERS:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        chunk = chunk[pos + len(SEP) :]\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_data(chunk)\n\n        return False, b\"\""
    },
    {
      "chunk_id": 84,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "class DeflateBuffer:\n    \"\"\"DeflateStream decompress stream and feed data into specified stream.\"\"\"\n\n    def __init__(self, out: StreamReader, encoding: Optional[str]) -> None:\n        self.out = out\n        self.size = 0\n        self.encoding = encoding\n        self._started_decoding = False\n\n        self.decompressor: Union[BrotliDecompressor, ZLibDecompressor]\n        if encoding == \"br\":\n            if not HAS_BROTLI:\n                raise ContentEncodingError(\n                    \"Can not decode content-encoding: brotli (br). \"\n                    \"Please install `Brotli`\"\n                )\n            self.decompressor = BrotliDecompressor()\n        else:\n            self.decompressor = ZLibDecompressor(encoding=encoding)\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        set_exception(self.out, exc, exc_cause)\n\n    def feed_data(self, chunk: bytes) -> None:\n        if not chunk:\n            return\n\n        self.size += len(chunk)\n\n        if (\n            not self._started_decoding\n            and self.encoding == \"deflate\"\n            and chunk[0] & 0xF != 8\n        ):\n            self.decompressor = ZLibDecompressor(\n                encoding=self.encoding, suppress_deflate_header=True\n            )\n\n        try:\n            chunk = self.decompressor.decompress_sync(chunk)\n        except Exception:\n            raise ContentEncodingError(\n                \"Can not decode content-encoding: %s\" % self.encoding\n            )\n\n        self._started_decoding = True\n\n        if chunk:\n            self.out.feed_data(chunk)\n\n    def feed_eof(self) -> None:\n        chunk = self.decompressor.flush()\n\n        if chunk or self.size > 0:\n            self.out.feed_data(chunk)\n            if self.encoding == \"deflate\" and not self.decompressor.eof:\n                raise ContentEncodingError(\"deflate\")\n\n        self.out.feed_eof()\n\n    def begin_http_chunk_receiving(self) -> None:\n        self.out.begin_http_chunk_receiving()\n\n    def end_http_chunk_receiving(self) -> None:\n        self.out.end_http_chunk_receiving()"
    },
    {
      "chunk_id": 85,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "HttpRequestParserPy = HttpRequestParser\nHttpResponseParserPy = HttpResponseParser\nRawRequestMessagePy = RawRequestMessage\nRawResponseMessagePy = RawResponseMessage"
    },
    {
      "chunk_id": 86,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_parser.py",
      "content": "try:\n    if not NO_EXTENSIONS:\n        from ._http_parser import (  # type: ignore[import-not-found,no-redef]\n            HttpRequestParser,\n            HttpResponseParser,\n            RawRequestMessage,\n            RawResponseMessage,\n        )\n\n        HttpRequestParserC = HttpRequestParser\n        HttpResponseParserC = HttpResponseParser\n        RawRequestMessageC = RawRequestMessage\n        RawResponseMessageC = RawResponseMessage\nexcept ImportError:  # pragma: no cover\n    pass\n```"
    },
    {
      "chunk_id": 87,
      "source": "__internal__/data_repo/aiohttp/aiohttp/cookiejar.py",
      "content": "import calendar\nimport contextlib\nimport datetime\nimport heapq\nimport itertools\nimport os  # noqa\nimport pathlib\nimport pickle\nimport re\nimport time\nimport warnings\nfrom collections import defaultdict\nfrom http.cookies import BaseCookie, Morsel, SimpleCookie\nfrom typing import (\n    DefaultDict,\n    Dict,\n    FrozenSet,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom yarl import URL\n\nfrom .abc import AbstractCookieJar, ClearCookiePredicate\nfrom .helpers import is_ip_address\nfrom .typedefs import LooseCookies, PathLike, StrOrURL\n\n__all__ = (\"CookieJar\", \"DummyCookieJar\")"
    },
    {
      "chunk_id": 88,
      "source": "__internal__/data_repo/aiohttp/aiohttp/cookiejar.py",
      "content": "CookieItem = Union[str, \"Morsel[str]\"]\n\n_FORMAT_PATH = \"{}/{}\".format\n_FORMAT_DOMAIN_REVERSED = \"{1}.{0}\".format\n\n_MIN_SCHEDULED_COOKIE_EXPIRATION = 100"
    },
    {
      "chunk_id": 89,
      "source": "__internal__/data_repo/aiohttp/aiohttp/cookiejar.py",
      "content": "class CookieJar(AbstractCookieJar):\n    \"\"\"Implements cookie storage adhering to RFC 6265.\"\"\"\n\n    DATE_TOKENS_RE = re.compile(\n        r\"[\\x09\\x20-\\x2F\\x3B-\\x40\\x5B-\\x60\\x7B-\\x7E]*\"\n        r\"(?P<token>[\\x00-\\x08\\x0A-\\x1F\\d:a-zA-Z\\x7F-\\xFF]+)\"\n    )\n\n    DATE_HMS_TIME_RE = re.compile(r\"(\\d{1,2}):(\\d{1,2}):(\\d{1,2})\")\n\n    DATE_DAY_OF_MONTH_RE = re.compile(r\"(\\d{1,2})\")\n\n    DATE_MONTH_RE = re.compile(\n        \"(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)|(aug)|(sep)|(oct)|(nov)|(dec)\",\n        re.I,\n    )\n\n    DATE_YEAR_RE = re.compile(r\"(\\d{2,4})\")\n\n    MAX_TIME = (\n        int(datetime.datetime.max.replace(tzinfo=datetime.timezone.utc).timestamp()) - 1\n    )\n    try:\n        calendar.timegm(time.gmtime(MAX_TIME))\n    except (OSError, ValueError):\n        MAX_TIME = calendar.timegm((3000, 12, 31, 23, 59, 59, -1, -1, -1))\n    except OverflowError:\n        MAX_TIME = 2**31 - 1\n    SUB_MAX_TIME = MAX_TIME - 1\n\n    def __init__(\n        self,\n        *,\n        unsafe: bool = False,\n        quote_cookie: bool = True,\n        treat_as_secure_origin: Union[StrOrURL, Iterable[StrOrURL], None] = None,\n    ) -> None:\n        self._cookies: DefaultDict[Tuple[str, str], SimpleCookie] = defaultdict(\n            SimpleCookie\n        )\n        self._morsel_cache: DefaultDict[Tuple[str, str], Dict[str, Morsel[str]]] = (\n            defaultdict(dict)\n        )\n        self._host_only_cookies: Set[Tuple[str, str]] = set()\n        self._unsafe = unsafe\n        self._quote_cookie = quote_cookie\n        if treat_as_secure_origin is None:\n            self._treat_as_secure_origin: FrozenSet[URL] = frozenset()\n        elif isinstance(treat_as_secure_origin, URL):\n            self._treat_as_secure_origin = frozenset({treat_as_secure_origin.origin()})\n        elif isinstance(treat_as_secure_origin, str):\n            self._treat_as_secure_origin = frozenset(\n                {URL(treat_as_secure_origin).origin()}\n            )\n        else:\n            self._treat_as_secure_origin = frozenset(\n                {\n                    URL(url).origin() if isinstance(url, str) else url.origin()\n                    for url in treat_as_secure_origin\n                }\n            )\n        self._expire_heap: List[Tuple[float, Tuple[str, str, str]]] = []\n        self._expirations: Dict[Tuple[str, str, str], float] = {}\n\n    @property\n    def quote_cookie(self) -> bool:\n        return self._quote_cookie\n\n    def save(self, file_path: PathLike) -> None:\n        file_path = pathlib.Path(file_path)\n        with file_path.open(mode=\"wb\") as f:\n            pickle.dump(self._cookies, f, pickle.HIGHEST_PROTOCOL)\n\n    def load(self, file_path: PathLike) -> None:\n        file_path = pathlib.Path(file_path)\n        with file_path.open(mode=\"rb\") as f:\n            self._cookies = pickle.load(f)\n\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        if predicate is None:\n            self._expire_heap.clear()\n            self._cookies.clear()\n            self._morsel_cache.clear()\n            self._host_only_cookies.clear()\n            self._expirations.clear()\n            return\n\n        now = time.time()\n        to_del = [\n            key\n            for (domain, path), cookie in self._cookies.items()\n            for name, morsel in cookie.items()\n            if (\n                (key := (domain, path, name)) in self._expirations\n                and self._expirations[key] <= now\n            )\n            or predicate(morsel)\n        ]\n        if to_del:\n            self._delete_cookies(to_del)\n\n    def clear_domain(self, domain: str) -> None:\n        self.clear(lambda x: self._is_domain_match(domain, x[\"domain\"]))\n\n    def __iter__(self) -> \"Iterator[Morsel[str]]\":\n        self._do_expiration()\n        for val in self._cookies.values():\n            yield from val.values()\n\n    def __len__(self) -> int:\n        \"\"\"Return number of cookies.\n\n        This function does not iterate self to avoid unnecessary expiration\n        checks.\n        \"\"\"\n        return sum(len(cookie.values()) for cookie in self._cookies.values())\n\n    def _do_expiration(self) -> None:\n        \"\"\"Remove expired cookies.\"\"\"\n        if not (expire_heap_len := len(self._expire_heap)):\n            return\n\n        if (\n            expire_heap_len > _MIN_SCHEDULED_COOKIE_EXPIRATION\n            and expire_heap_len > len(self._expirations) * 2\n        ):\n            self._expire_heap = [\n                entry\n                for entry in self._expire_heap\n                if self._expirations.get(entry[1]) == entry[0]\n            ]\n            heapq.heapify(self._expire_heap)\n\n        now = time.time()\n        to_del: List[Tuple[str, str, str]] = []\n        while self._expire_heap:\n            when, cookie_key = self._expire_heap[0]\n            if when > now:\n                break\n            heapq.heappop(self._expire_heap)\n            if self._expirations.get(cookie_key) == when:\n                to_del.append(cookie_key)\n\n        if to_del:\n            self._delete_cookies(to_del)\n\n    def _delete_cookies(self, to_del: List[Tuple[str, str, str]]) -> None:\n        for domain, path, name in to_del:\n            self._host_only_cookies.discard((domain, name))\n            self._cookies[(domain, path)].pop(name, None)\n            self._morsel_cache[(domain, path)].pop(name, None)\n            self._expirations.pop((domain, path, name), None)\n\n    def _expire_cookie(self, when: float, domain: str, path: str, name: str) -> None:\n        cookie_key = (domain, path, name)\n        if self._expirations.get(cookie_key) == when:\n            return\n        heapq.heappush(self._expire_heap, (when, cookie_key))\n        self._expirations[cookie_key] = when\n\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        \"\"\"Update cookies.\"\"\"\n        hostname = response_url.raw_host\n\n        if not self._unsafe and is_ip_address(hostname):\n            return\n\n        if isinstance(cookies, Mapping):\n            cookies = cookies.items()\n\n        for name, cookie in cookies:\n            if not isinstance(cookie, Morsel):\n                tmp = SimpleCookie()\n                tmp[name] = cookie  # type: ignore[assignment]\n                cookie = tmp[name]\n\n            domain = cookie[\"domain\"]\n\n            if domain and domain[-1] == \".\":\n                domain = \"\"\n                del cookie[\"domain\"]\n\n            if not domain and hostname is not None:\n                self._host_only_cookies.add((hostname, name))\n                domain = cookie[\"domain\"] = hostname\n\n            if domain and domain[0] == \".\":\n                domain = domain[1:]\n                cookie[\"domain\"] = domain\n\n            if hostname and not self._is_domain_match(domain, hostname):\n                continue\n\n            path = cookie[\"path\"]\n            if not path or path[0] != \"/\":\n                path = response_url.path\n                if not path.startswith(\"/\"):\n                    path = \"/\"\n                else:\n                    path = \"/\" + path[1 : path.rfind(\"/\")]\n                cookie[\"path\"] = path\n            path = path.rstrip(\"/\")\n\n            if max_age := cookie[\"max-age\"]:\n                try:\n                    delta_seconds = int(max_age)\n                    max_age_expiration = min(time.time() + delta_seconds, self.MAX_TIME)\n                    self._expire_cookie(max_age_expiration, domain, path, name)\n                except ValueError:\n                    cookie[\"max-age\"] = \"\"\n\n            elif expires := cookie[\"expires\"]:\n                if expire_time := self._parse_date(expires):\n                    self._expire_cookie(expire_time, domain, path, name)\n                else:\n                    cookie[\"expires\"] = \"\"\n\n            key = (domain, path)\n            if self._cookies[key].get(name) != cookie:\n                self._cookies[key][name] = cookie\n                self._morsel_cache[key].pop(name, None)\n\n        self._do_expiration()\n\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        \"\"\"Returns this jar's cookies filtered by their attributes.\"\"\"\n        if not isinstance(request_url, URL):\n            warnings.warn(  # type: ignore[unreachable]\n                \"The method accepts yarl.URL instances only, got {}\".format(\n                    type(request_url)\n                ),\n                DeprecationWarning,\n            )\n            request_url = URL(request_url)\n        filtered: Union[SimpleCookie, \"BaseCookie[str]\"] = (\n            SimpleCookie() if self._quote_cookie else BaseCookie()\n        )\n        if not self._cookies:\n            return filtered\n        self._do_expiration()\n        if not self._cookies:\n            return filtered\n        hostname = request_url.raw_host or \"\"\n\n        is_not_secure = request_url.scheme not in (\"https\", \"wss\")\n        if is_not_secure and self._treat_as_secure_origin:\n            request_origin = URL()\n            with contextlib.suppress(ValueError):\n                request_origin = request_url.origin()\n            is_not_secure = request_origin not in self._treat_as_secure_origin\n\n        for c in self._cookies[(\"\", \"\")].values():\n            filtered[c.key] = c.value\n\n        if is_ip_address(hostname):\n            if not self._unsafe:\n                return filtered\n            domains: Iterable[str] = (hostname,)\n        else:\n            domains = itertools.accumulate(\n                reversed(hostname.split(\".\")), _FORMAT_DOMAIN_REVERSED\n            )\n\n        paths = itertools.accumulate(request_url.path.split(\"/\"), _FORMAT_PATH)\n        pairs = itertools.product(domains, paths)\n\n        path_len = len(request_url.path)\n        for p in pairs:\n            for name, cookie in self._cookies[p].items():\n                domain = cookie[\"domain\"]\n\n                if (domain, name) in self._host_only_cookies and domain != hostname:\n                    continue\n\n                if len(cookie[\"path\"]) > path_len:\n                    continue\n\n                if is_not_secure and cookie[\"secure\"]:\n                    continue\n\n                if name in self._morsel_cache[p]:\n                    filtered[name] = self._morsel_cache[p][name]\n                    continue\n\n                mrsl_val = cast(\"Morsel[str]\", cookie.get(cookie.key, Morsel()))\n                mrsl_val.set(cookie.key, cookie.value, cookie.coded_value)\n                self._morsel_cache[p][name] = mrsl_val\n                filtered[name] = mrsl_val\n\n        return filtered\n\n    @staticmethod\n    def _is_domain_match(domain: str, hostname: str) -> bool:\n        \"\"\"Implements domain matching adhering to RFC 6265.\"\"\"\n        if hostname == domain:\n            return True\n\n        if not hostname.endswith(domain):\n            return False\n\n        non_matching = hostname[: -len(domain)]\n\n        if not non_matching.endswith(\".\"):\n            return False\n\n        return not is_ip_address(hostname)\n\n    @classmethod\n    def _parse_date(cls, date_str: str) -> Optional[int]:\n        \"\"\"Implements date string parsing adhering to RFC 6265.\"\"\"\n        if not date_str:\n            return None\n\n        found_time = False\n        found_day = False\n        found_month = False\n        found_year = False\n\n        hour = minute = second = 0\n        day = 0\n        month = 0\n        year = 0\n\n        for token_match in cls.DATE_TOKENS_RE.finditer(date_str):\n            token = token_match.group(\"token\")\n\n            if not found_time:\n                time_match = cls.DATE_HMS_TIME_RE.match(token)\n                if time_match:\n                    found_time = True\n                    hour, minute, second = (int(s) for s in time_match.groups())\n                    continue\n\n            if not found_day:\n                day_match = cls.DATE_DAY_OF_MONTH_RE.match(token)\n                if day_match:\n                    found_day = True\n                    day = int(day_match.group())\n                    continue\n\n            if not found_month:\n                month_match = cls.DATE_MONTH_RE.match(token)\n                if month_match:\n                    found_month = True\n                    assert month_match.lastindex is not None\n                    month = month_match.lastindex\n                    continue\n\n            if not found_year:\n                year_match = cls.DATE_YEAR_RE.match(token)\n                if year_match:\n                    found_year = True\n                    year = int(year_match.group())\n\n        if 70 <= year <= 99:\n            year += 1900\n        elif 0 <= year <= 69:\n            year += 2000\n\n        if False in (found_day, found_month, found_year, found_time):\n            return None\n\n        if not 1 <= day <= 31:\n            return None\n\n        if year < 1601 or hour > 23 or minute > 59 or second > 59:\n            return None\n\n        return calendar.timegm((year, month, day, hour, minute, second, -1, -1, -1))"
    },
    {
      "chunk_id": 90,
      "source": "__internal__/data_repo/aiohttp/aiohttp/cookiejar.py",
      "content": "class DummyCookieJar(AbstractCookieJar):\n    \"\"\"Implements a dummy cookie storage.\n\n    It can be used with the ClientSession when no cookie processing is needed.\n\n    \"\"\"\n\n    def __iter__(self) -> \"Iterator[Morsel[str]]\":\n        while False:\n            yield None  # type: ignore[unreachable]\n\n    def __len__(self) -> int:\n        return 0\n\n    @property\n    def quote_cookie(self) -> bool:\n        return True\n\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        pass\n\n    def clear_domain(self, domain: str) -> None:\n        pass\n\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        pass\n\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        return SimpleCookie()"
    },
    {
      "chunk_id": 91,
      "source": "__internal__/data_repo/aiohttp/aiohttp/log.py",
      "content": "import logging"
    },
    {
      "chunk_id": 92,
      "source": "__internal__/data_repo/aiohttp/aiohttp/log.py",
      "content": "access_logger = logging.getLogger(\"aiohttp.access\")\nclient_logger = logging.getLogger(\"aiohttp.client\")\ninternal_logger = logging.getLogger(\"aiohttp.internal\")\nserver_logger = logging.getLogger(\"aiohttp.server\")\nweb_logger = logging.getLogger(\"aiohttp.web\")\nws_logger = logging.getLogger(\"aiohttp.websocket\")"
    },
    {
      "chunk_id": 93,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "import logging\nimport socket\nimport zlib\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sized\nfrom http.cookies import BaseCookie, Morsel\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    TypedDict,\n    Union,\n)\n\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom .typedefs import LooseCookies\n\nif TYPE_CHECKING:\n    from .web_app import Application\n    from .web_exceptions import HTTPException\n    from .web_request import BaseRequest, Request\n    from .web_response import StreamResponse\nelse:\n    BaseRequest = Request = Application = StreamResponse = None\n    HTTPException = None"
    },
    {
      "chunk_id": 94,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractRouter(ABC):\n    def __init__(self) -> None:\n        self._frozen = False\n\n    def post_init(self, app: Application) -> None:\n        \"\"\"Post init stage.\n\n        Not an abstract method for sake of backward compatibility,\n        but if the router wants to be aware of the application\n        it can override this.\n        \"\"\"\n\n    @property\n    def frozen(self) -> bool:\n        return self._frozen\n\n    def freeze(self) -> None:\n        \"\"\"Freeze router.\"\"\"\n        self._frozen = True\n\n    @abstractmethod\n    async def resolve(self, request: Request) -> \"AbstractMatchInfo\":\n        \"\"\"Return MATCH_INFO for given request\"\"\""
    },
    {
      "chunk_id": 95,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractMatchInfo(ABC):\n\n    __slots__ = ()\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def handler(self) -> Callable[[Request], Awaitable[StreamResponse]]:\n        \"\"\"Execute matched request handler\"\"\"\n\n    @property\n    @abstractmethod\n    def expect_handler(\n        self,\n    ) -> Callable[[Request], Awaitable[Optional[StreamResponse]]]:\n        \"\"\"Expect handler for 100-continue processing\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def http_exception(self) -> Optional[HTTPException]:\n        \"\"\"HTTPException instance raised on router's resolving, or None\"\"\"\n\n    @abstractmethod  # pragma: no branch\n    def get_info(self) -> Dict[str, Any]:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def apps(self) -> Tuple[Application, ...]:\n        \"\"\"Stack of nested applications.\n\n        Top level application is left-most element.\n\n        \"\"\"\n\n    @abstractmethod\n    def add_app(self, app: Application) -> None:\n        \"\"\"Add application to the nested apps stack.\"\"\"\n\n    @abstractmethod\n    def freeze(self) -> None:\n        \"\"\"Freeze the match info.\n\n        The method is called after route resolution.\n\n        After the call .add_app() is forbidden.\n\n        \"\"\""
    },
    {
      "chunk_id": 96,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractView(ABC):\n    \"\"\"Abstract class based view.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self._request = request\n\n    @property\n    def request(self) -> Request:\n        \"\"\"Request instance.\"\"\"\n        return self._request\n\n    @abstractmethod\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\n        \"\"\"Execute the view handler.\"\"\""
    },
    {
      "chunk_id": 97,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class ResolveResult(TypedDict):\n    \"\"\"Resolve result.\n\n    This is the result returned from an AbstractResolver's\n    resolve method.\n\n    :param hostname: The hostname that was provided.\n    :param host: The IP address that was resolved.\n    :param port: The port that was resolved.\n    :param family: The address family that was resolved.\n    :param proto: The protocol that was resolved.\n    :param flags: The flags that were resolved.\n    \"\"\"\n\n    hostname: str\n    host: str\n    port: int\n    family: int\n    proto: int\n    flags: int"
    },
    {
      "chunk_id": 98,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractResolver(ABC):\n    \"\"\"Abstract DNS resolver.\"\"\"\n\n    @abstractmethod\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        \"\"\"Return IP address for given hostname\"\"\"\n\n    @abstractmethod\n    async def close(self) -> None:\n        \"\"\"Release resolver\"\"\""
    },
    {
      "chunk_id": 99,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "if TYPE_CHECKING:\n    IterableBase = Iterable[Morsel[str]]\nelse:\n    IterableBase = Iterable"
    },
    {
      "chunk_id": 100,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "ClearCookiePredicate = Callable[[\"Morsel[str]\"], bool]"
    },
    {
      "chunk_id": 101,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractCookieJar(Sized, IterableBase):\n    \"\"\"Abstract Cookie Jar.\"\"\"\n\n    @property\n    @abstractmethod\n    def quote_cookie(self) -> bool:\n        \"\"\"Return True if cookies should be quoted.\"\"\"\n\n    @abstractmethod\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        \"\"\"Clear all cookies if no predicate is passed.\"\"\"\n\n    @abstractmethod\n    def clear_domain(self, domain: str) -> None:\n        \"\"\"Clear all cookies for domain and all subdomains.\"\"\"\n\n    @abstractmethod\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        \"\"\"Update cookies.\"\"\"\n\n    @abstractmethod\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        \"\"\"Return the jar's cookies filtered by their attributes.\"\"\""
    },
    {
      "chunk_id": 102,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractStreamWriter(ABC):\n    \"\"\"Abstract stream writer.\"\"\"\n\n    buffer_size: int = 0\n    output_size: int = 0\n    length: Optional[int] = 0\n\n    @abstractmethod\n    async def write(self, chunk: Union[bytes, bytearray, memoryview]) -> None:\n        \"\"\"Write chunk into stream.\"\"\"\n\n    @abstractmethod\n    async def write_eof(self, chunk: bytes = b\"\") -> None:\n        \"\"\"Write last chunk.\"\"\"\n\n    @abstractmethod\n    async def drain(self) -> None:\n        \"\"\"Flush the write buffer.\"\"\"\n\n    @abstractmethod\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: int = zlib.Z_DEFAULT_STRATEGY\n    ) -> None:\n        \"\"\"Enable HTTP body compression\"\"\"\n\n    @abstractmethod\n    def enable_chunking(self) -> None:\n        \"\"\"Enable HTTP chunked mode\"\"\"\n\n    @abstractmethod\n    async def write_headers(\n        self, status_line: str, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        \"\"\"Write HTTP headers\"\"\""
    },
    {
      "chunk_id": 103,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractAccessLogger(ABC):\n    \"\"\"Abstract writer to access log.\"\"\"\n\n    __slots__ = (\"logger\", \"log_format\")\n\n    def __init__(self, logger: logging.Logger, log_format: str) -> None:\n        self.logger = logger\n        self.log_format = log_format\n\n    @abstractmethod\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        return True"
    },
    {
      "chunk_id": 104,
      "source": "__internal__/data_repo/aiohttp/aiohttp/abc.py",
      "content": "class AbstractAsyncAccessLogger(ABC):\n    \"\"\"Abstract asynchronous writer to access log.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def log(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        return True"
    },
    {
      "chunk_id": 105,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "```python"
    },
    {
      "chunk_id": 106,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "import asyncio\nimport collections\nimport warnings\nfrom typing import (\n    Awaitable,\n    Callable,\n    Deque,\n    Final,\n    Generic,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom .base_protocol import BaseProtocol\nfrom .helpers import (\n    _EXC_SENTINEL,\n    BaseTimerContext,\n    TimerNoop,\n    set_exception,\n    set_result,\n)\nfrom .log import internal_logger\n\n__all__ = (\n    \"EMPTY_PAYLOAD\",\n    \"EofStream\",\n    \"StreamReader\",\n    \"DataQueue\",\n)\n\n_T = TypeVar(\"_T\")"
    },
    {
      "chunk_id": 107,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class EofStream(Exception):\n    \"\"\"eof stream indication.\"\"\""
    },
    {
      "chunk_id": 108,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class AsyncStreamIterator(Generic[_T]):\n\n    __slots__ = (\"read_func\",)\n\n    def __init__(self, read_func: Callable[[], Awaitable[_T]]) -> None:\n        self.read_func = read_func\n\n    def __aiter__(self) -> \"AsyncStreamIterator[_T]\":\n        return self\n\n    async def __anext__(self) -> _T:\n        try:\n            rv = await self.read_func()\n        except EofStream:\n            raise StopAsyncIteration\n        if rv == b\"\":\n            raise StopAsyncIteration\n        return rv"
    },
    {
      "chunk_id": 109,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class ChunkTupleAsyncStreamIterator:\n\n    __slots__ = (\"_stream\",)\n\n    def __init__(self, stream: \"StreamReader\") -> None:\n        self._stream = stream\n\n    def __aiter__(self) -> \"ChunkTupleAsyncStreamIterator\":\n        return self\n\n    async def __anext__(self) -> Tuple[bytes, bool]:\n        rv = await self._stream.readchunk()\n        if rv == (b\"\", False):\n            raise StopAsyncIteration\n        return rv"
    },
    {
      "chunk_id": 110,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class AsyncStreamReaderMixin:\n\n    __slots__ = ()\n\n    def __aiter__(self) -> AsyncStreamIterator[bytes]:\n        return AsyncStreamIterator(self.readline)  # type: ignore[attr-defined]\n\n    def iter_chunked(self, n: int) -> AsyncStreamIterator[bytes]:\n        \"\"\"Returns an asynchronous iterator that yields chunks of size n.\"\"\"\n        return AsyncStreamIterator(lambda: self.read(n))  # type: ignore[attr-defined]\n\n    def iter_any(self) -> AsyncStreamIterator[bytes]:\n        \"\"\"Yield all available data as soon as it is received.\"\"\"\n        return AsyncStreamIterator(self.readany)  # type: ignore[attr-defined]\n\n    def iter_chunks(self) -> ChunkTupleAsyncStreamIterator:\n        \"\"\"Yield chunks of data as they are received by the server.\n\n        The yielded objects are tuples\n        of (bytes, bool) as returned by the StreamReader.readchunk method.\n        \"\"\"\n        return ChunkTupleAsyncStreamIterator(self)  # type: ignore[arg-type]"
    },
    {
      "chunk_id": 111,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class StreamReader(AsyncStreamReaderMixin):\n    \"\"\"An enhancement of asyncio.StreamReader.\n\n    Supports asynchronous iteration by line, chunk or as available::\n\n        async for line in reader:\n            ...\n        async for chunk in reader.iter_chunked(1024):\n            ...\n        async for slice in reader.iter_any():\n            ...\n\n    \"\"\"\n\n    __slots__ = (\n        \"_protocol\",\n        \"_low_water\",\n        \"_high_water\",\n        \"_loop\",\n        \"_size\",\n        \"_cursor\",\n        \"_http_chunk_splits\",\n        \"_buffer\",\n        \"_buffer_offset\",\n        \"_eof\",\n        \"_waiter\",\n        \"_eof_waiter\",\n        \"_exception\",\n        \"_timer\",\n        \"_eof_callbacks\",\n        \"_eof_counter\",\n        \"total_bytes\",\n    )\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        limit: int,\n        *,\n        timer: Optional[BaseTimerContext] = None,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        self._protocol = protocol\n        self._low_water = limit\n        self._high_water = limit * 2\n        self._loop = loop\n        self._size = 0\n        self._cursor = 0\n        self._http_chunk_splits: Optional[List[int]] = None\n        self._buffer: Deque[bytes] = collections.deque()\n        self._buffer_offset = 0\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._eof_waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Optional[Union[Type[BaseException], BaseException]] = None\n        self._timer = TimerNoop() if timer is None else timer\n        self._eof_callbacks: List[Callable[[], None]] = []\n        self._eof_counter = 0\n        self.total_bytes = 0\n\n    def __repr__(self) -> str:\n        info = [self.__class__.__name__]\n        if self._size:\n            info.append(\"%d bytes\" % self._size)\n        if self._eof:\n            info.append(\"eof\")\n        if self._low_water != 2**16:  # default limit\n            info.append(\"low=%d high=%d\" % (self._low_water, self._high_water))\n        if self._waiter:\n            info.append(\"w=%r\" % self._waiter)\n        if self._exception:\n            info.append(\"e=%r\" % self._exception)\n        return \"<%s>\" % \" \".join(info)\n\n    def get_read_buffer_limits(self) -> Tuple[int, int]:\n        return (self._low_water, self._high_water)\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._exception = exc\n        self._eof_callbacks.clear()\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n        waiter = self._eof_waiter\n        if waiter is not None:\n            self._eof_waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def on_eof(self, callback: Callable[[], None]) -> None:\n        if self._eof:\n            try:\n                callback()\n            except Exception:\n                internal_logger.exception(\"Exception in eof callback\")\n        else:\n            self._eof_callbacks.append(callback)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n        waiter = self._eof_waiter\n        if waiter is not None:\n            self._eof_waiter = None\n            set_result(waiter, None)\n\n        if self._protocol._reading_paused:\n            self._protocol.resume_reading()\n\n        for cb in self._eof_callbacks:\n            try:\n                cb()\n            except Exception:\n                internal_logger.exception(\"Exception in eof callback\")\n\n        self._eof_callbacks.clear()\n\n    def is_eof(self) -> bool:\n        \"\"\"Return True if  'feed_eof' was called.\"\"\"\n        return self._eof\n\n    def at_eof(self) -> bool:\n        \"\"\"Return True if the buffer is empty and 'feed_eof' was called.\"\"\"\n        return self._eof and not self._buffer\n\n    async def wait_eof(self) -> None:\n        if self._eof:\n            return\n\n        assert self._eof_waiter is None\n        self._eof_waiter = self._loop.create_future()\n        try:\n            await self._eof_waiter\n        finally:\n            self._eof_waiter = None\n\n    def unread_data(self, data: bytes) -> None:\n        \"\"\"rollback reading some data from stream, inserting it to buffer head.\"\"\"\n        warnings.warn(\n            \"unread_data() is deprecated \"\n            \"and will be removed in future releases (#3260)\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if not data:\n            return\n\n        if self._buffer_offset:\n            self._buffer[0] = self._buffer[0][self._buffer_offset :]\n            self._buffer_offset = 0\n        self._size += len(data)\n        self._cursor -= len(data)\n        self._buffer.appendleft(data)\n        self._eof_counter = 0\n\n    def feed_data(self, data: bytes) -> None:\n        assert not self._eof, \"feed_data after feed_eof\"\n\n        if not data:\n            return\n\n        data_len = len(data)\n        self._size += data_len\n        self._buffer.append(data)\n        self.total_bytes += data_len\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n        if self._size > self._high_water and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    def begin_http_chunk_receiving(self) -> None:\n        if self._http_chunk_splits is None:\n            if self.total_bytes:\n                raise RuntimeError(\n                    \"Called begin_http_chunk_receiving when some data was already fed\"\n                )\n            self._http_chunk_splits = []\n\n    def end_http_chunk_receiving(self) -> None:\n        if self._http_chunk_splits is None:\n            raise RuntimeError(\n                \"Called end_chunk_receiving without calling \"\n                \"begin_chunk_receiving first\"\n            )\n\n        pos = self._http_chunk_splits[-1] if self._http_chunk_splits else 0\n\n        if self.total_bytes == pos:\n            return\n\n        self._http_chunk_splits.append(self.total_bytes)\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    async def _wait(self, func_name: str) -> None:\n        if not self._protocol.connected:\n            raise RuntimeError(\"Connection closed.\")\n\n        if self._waiter is not None:\n            raise RuntimeError(\n                \"%s() called while another coroutine is \"\n                \"already waiting for incoming data\" % func_name\n            )\n\n        waiter = self._waiter = self._loop.create_future()\n        try:\n            with self._timer:\n                await waiter\n        finally:\n            self._waiter = None\n\n    async def readline(self) -> bytes:\n        return await self.readuntil()\n\n    async def readuntil(self, separator: bytes = b\"\\n\") -> bytes:\n        seplen = len(separator)\n        if seplen == 0:\n            raise ValueError(\"Separator should be at least one-byte string\")\n\n        if self._exception is not None:\n            raise self._exception\n\n        chunk = b\"\"\n        chunk_size = 0\n        not_enough = True\n\n        while not_enough:\n            while self._buffer and not_enough:\n                offset = self._buffer_offset\n                ichar = self._buffer[0].find(separator, offset) + 1\n                data = self._read_nowait_chunk(\n                    ichar - offset + seplen - 1 if ichar else -1\n                )\n                chunk += data\n                chunk_size += len(data)\n                if ichar:\n                    not_enough = False\n\n                if chunk_size > self._high_water:\n                    raise ValueError(\"Chunk too big\")\n\n            if self._eof:\n                break\n\n            if not_enough:\n                await self._wait(\"readuntil\")\n\n        return chunk\n\n    async def read(self, n: int = -1) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        if not n:\n            return b\"\"\n\n        if n < 0:\n            blocks = []\n            while True:\n                block = await self.readany()\n                if not block:\n                    break\n                blocks.append(block)\n            return b\"\".join(blocks)\n\n        while not self._buffer and not self._eof:\n            await self._wait(\"read\")\n\n        return self._read_nowait(n)\n\n    async def readany(self) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        while not self._buffer and not self._eof:\n            await self._wait(\"readany\")\n\n        return self._read_nowait(-1)\n\n    async def readchunk(self) -> Tuple[bytes, bool]:\n        \"\"\"Returns a tuple of (data, end_of_http_chunk).\n\n        When chunked transfer\n        encoding is used, end_of_http_chunk is a boolean indicating if the end\n        of the data corresponds to the end of a HTTP chunk , otherwise it is\n        always False.\n        \"\"\"\n        while True:\n            if self._exception is not None:\n                raise self._exception\n\n            while self._http_chunk_splits:\n                pos = self._http_chunk_splits.pop(0)\n                if pos == self._cursor:\n                    return (b\"\", True)\n                if pos > self._cursor:\n                    return (self._read_nowait(pos - self._cursor), True)\n                internal_logger.warning(\n                    \"Skipping HTTP chunk end due to data \"\n                    \"consumption beyond chunk boundary\"\n                )\n\n            if self._buffer:\n                return (self._read_nowait_chunk(-1), False)\n\n            if self._eof:\n                return (b\"\", False)\n\n            await self._wait(\"readchunk\")\n\n    async def readexactly(self, n: int) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        blocks: List[bytes] = []\n        while n > 0:\n            block = await self.read(n)\n            if not block:\n                partial = b\"\".join(blocks)\n                raise asyncio.IncompleteReadError(partial, len(partial) + n)\n            blocks.append(block)\n            n -= len(block)\n\n        return b\"\".join(blocks)\n\n    def read_nowait(self, n: int = -1) -> bytes:\n        if self._exception is not None:\n            raise self._exception\n\n        if self._waiter and not self._waiter.done():\n            raise RuntimeError(\n                \"Called while some coroutine is waiting for incoming data.\"\n            )\n\n        return self._read_nowait(n)\n\n    def _read_nowait_chunk(self, n: int) -> bytes:\n        first_buffer = self._buffer[0]\n        offset = self._buffer_offset\n        if n != -1 and len(first_buffer) - offset > n:\n            data = first_buffer[offset : offset + n]\n            self._buffer_offset += n\n\n        elif offset:\n            self._buffer.popleft()\n            data = first_buffer[offset:]\n            self._buffer_offset = 0\n\n        else:\n            data = self._buffer.popleft()\n\n        data_len = len(data)\n        self._size -= data_len\n        self._cursor += data_len\n\n        chunk_splits = self._http_chunk_splits\n        while chunk_splits and chunk_splits[0] < self._cursor:\n            chunk_splits.pop(0)\n\n        if self._size < self._low_water and self._protocol._reading_paused:\n            self._protocol.resume_reading()\n        return data\n\n    def _read_nowait(self, n: int) -> bytes:\n        \"\"\"Read not more than n bytes, or whole buffer if n == -1\"\"\"\n        self._timer.assert_timeout()\n\n        chunks = []\n        while self._buffer:\n            chunk = self._read_nowait_chunk(n)\n            chunks.append(chunk)\n            if n != -1:\n                n -= len(chunk)\n                if n == 0:\n                    break\n\n        return b\"\".join(chunks) if chunks else b\"\""
    },
    {
      "chunk_id": 112,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class EmptyStreamReader(StreamReader):  # lgtm [py/missing-call-to-init]\n\n    __slots__ = (\"_read_eof_chunk\",)\n\n    def __init__(self) -> None:\n        self._read_eof_chunk = False\n\n    def __repr__(self) -> str:\n        return \"<%s>\" % self.__class__.__name__\n\n    def exception(self) -> Optional[BaseException]:\n        return None\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        pass\n\n    def on_eof(self, callback: Callable[[], None]) -> None:\n        try:\n            callback()\n        except Exception:\n            internal_logger.exception(\"Exception in eof callback\")\n\n    def feed_eof(self) -> None:\n        pass\n\n    def is_eof(self) -> bool:\n        return True\n\n    def at_eof(self) -> bool:\n        return True\n\n    async def wait_eof(self) -> None:\n        return\n\n    def feed_data(self, data: bytes) -> None:\n        pass\n\n    async def readline(self) -> bytes:\n        return b\"\"\n\n    async def read(self, n: int = -1) -> bytes:\n        return b\"\"\n\n    async def readany(self) -> bytes:\n        return b\"\"\n\n    async def readchunk(self) -> Tuple[bytes, bool]:\n        if not self._read_eof_chunk:\n            self._read_eof_chunk = True\n            return (b\"\", False)\n\n        return (b\"\", True)\n\n    async def readexactly(self, n: int) -> bytes:\n        raise asyncio.IncompleteReadError(b\"\", n)\n\n    def read_nowait(self, n: int = -1) -> bytes:\n        return b\"\""
    },
    {
      "chunk_id": 113,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "EMPTY_PAYLOAD: Final[StreamReader] = EmptyStreamReader()"
    },
    {
      "chunk_id": 114,
      "source": "__internal__/data_repo/aiohttp/aiohttp/streams.py",
      "content": "class DataQueue(Generic[_T]):\n    \"\"\"DataQueue is a general-purpose blocking queue with one reader.\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Union[Type[BaseException], BaseException, None] = None\n        self._buffer: Deque[_T] = collections.deque()\n\n    def __len__(self) -> int:\n        return len(self._buffer)\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def at_eof(self) -> bool:\n        return self._eof and not self._buffer\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def feed_data(self, data: _T) -> None:\n        self._buffer.append(data)\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_result(waiter, None)\n\n    async def read(self) -> _T:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n        if self._buffer:\n            return self._buffer.popleft()\n        if self._exception is not None:\n            raise self._exception\n        raise EofStream\n\n    def __aiter__(self) -> AsyncStreamIterator[_T]:\n        return AsyncStreamIterator(self.read)\n```"
    },
    {
      "chunk_id": 115,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_ws.py",
      "content": "```python"
    },
    {
      "chunk_id": 116,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_ws.py",
      "content": "\"\"\"WebSocket client for asyncio.\"\"\"\n\nimport asyncio\nimport sys\nfrom types import TracebackType\nfrom typing import Any, Final, Optional, Type\n\nfrom ._websocket.reader import WebSocketDataQueue\nfrom .client_exceptions import ClientError, ServerTimeoutError, WSMessageTypeError\nfrom .client_reqrep import ClientResponse\nfrom .helpers import calculate_timeout_when, frozen_dataclass_decorator, set_result\nfrom .http import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .http_websocket import _INTERNAL_RECEIVE_TYPES, WebSocketWriter, WSMessageError\nfrom .streams import EofStream\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    DEFAULT_JSON_ENCODER,\n    JSONDecoder,\n    JSONEncoder,\n)\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout"
    },
    {
      "chunk_id": 117,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_ws.py",
      "content": "@frozen_dataclass_decorator\nclass ClientWSTimeout:\n    ws_receive: Optional[float] = None\n    ws_close: Optional[float] = None\n\n\nDEFAULT_WS_CLIENT_TIMEOUT: Final[ClientWSTimeout] = ClientWSTimeout(\n    ws_receive=None, ws_close=10.0\n)"
    },
    {
      "chunk_id": 118,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_ws.py",
      "content": "class ClientWebSocketResponse:\n    def __init__(\n        self,\n        reader: WebSocketDataQueue,\n        writer: WebSocketWriter,\n        protocol: Optional[str],\n        response: ClientResponse,\n        timeout: ClientWSTimeout,\n        autoclose: bool,\n        autoping: bool,\n        loop: asyncio.AbstractEventLoop,\n        *,\n        heartbeat: Optional[float] = None,\n        compress: int = 0,\n        client_notakeover: bool = False,\n    ) -> None:\n        self._response = response\n        self._conn = response.connection\n\n        self._writer = writer\n        self._reader = reader\n        self._protocol = protocol\n        self._closed = False\n        self._closing = False\n        self._close_code: Optional[int] = None\n        self._timeout = timeout\n        self._autoclose = autoclose\n        self._autoping = autoping\n        self._heartbeat = heartbeat\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\n        self._heartbeat_when: float = 0.0\n        if heartbeat is not None:\n            self._pong_heartbeat = heartbeat / 2.0\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\n        self._loop = loop\n        self._waiting: bool = False\n        self._close_wait: Optional[asyncio.Future[None]] = None\n        self._exception: Optional[BaseException] = None\n        self._compress = compress\n        self._client_notakeover = client_notakeover\n        self._ping_task: Optional[asyncio.Task[None]] = None\n\n        self._reset_heartbeat()\n\n    def _cancel_heartbeat(self) -> None:\n        self._cancel_pong_response_cb()\n        if self._heartbeat_cb is not None:\n            self._heartbeat_cb.cancel()\n            self._heartbeat_cb = None\n        if self._ping_task is not None:\n            self._ping_task.cancel()\n            self._ping_task = None\n\n    def _cancel_pong_response_cb(self) -> None:\n        if self._pong_response_cb is not None:\n            self._pong_response_cb.cancel()\n            self._pong_response_cb = None\n\n    def _reset_heartbeat(self) -> None:\n        if self._heartbeat is None:\n            return\n        self._cancel_pong_response_cb()\n        loop = self._loop\n        assert loop is not None\n        conn = self._conn\n        timeout_ceil_threshold = (\n            conn._connector._timeout_ceil_threshold if conn is not None else 5\n        )\n        now = loop.time()\n        when = calculate_timeout_when(now, self._heartbeat, timeout_ceil_threshold)\n        self._heartbeat_when = when\n        if self._heartbeat_cb is None:\n            self._heartbeat_cb = loop.call_at(when, self._send_heartbeat)\n\n    def _send_heartbeat(self) -> None:\n        self._heartbeat_cb = None\n        loop = self._loop\n        now = loop.time()\n        if now < self._heartbeat_when:\n            self._heartbeat_cb = loop.call_at(\n                self._heartbeat_when, self._send_heartbeat\n            )\n            return\n\n        conn = self._conn\n        timeout_ceil_threshold = (\n            conn._connector._timeout_ceil_threshold if conn is not None else 5\n        )\n        when = calculate_timeout_when(now, self._pong_heartbeat, timeout_ceil_threshold)\n        self._cancel_pong_response_cb()\n        self._pong_response_cb = loop.call_at(when, self._pong_not_received)\n\n        coro = self._writer.send_frame(b\"\", WSMsgType.PING)\n        if sys.version_info >= (3, 12):\n            ping_task = asyncio.Task(coro, loop=loop, eager_start=True)\n        else:\n            ping_task = loop.create_task(coro)\n\n        if not ping_task.done():\n            self._ping_task = ping_task\n            ping_task.add_done_callback(self._ping_task_done)\n        else:\n            self._ping_task_done(ping_task)\n\n    def _ping_task_done(self, task: \"asyncio.Task[None]\") -> None:\n        \"\"\"Callback for when the ping task completes.\"\"\"\n        if not task.cancelled() and (exc := task.exception()):\n            self._handle_ping_pong_exception(exc)\n        self._ping_task = None\n\n    def _pong_not_received(self) -> None:\n        self._handle_ping_pong_exception(ServerTimeoutError())\n\n    def _handle_ping_pong_exception(self, exc: BaseException) -> None:\n        \"\"\"Handle exceptions raised during ping/pong processing.\"\"\"\n        if self._closed:\n            return\n        self._set_closed()\n        self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n        self._exception = exc\n        self._response.close()\n        if self._waiting and not self._closing:\n            self._reader.feed_data(WSMessageError(data=exc, extra=None))\n\n    def _set_closed(self) -> None:\n        \"\"\"Set the connection to closed.\n\n        Cancel any heartbeat timers and set the closed flag.\n        \"\"\"\n        self._closed = True\n        self._cancel_heartbeat()\n\n    def _set_closing(self) -> None:\n        \"\"\"Set the connection to closing.\n\n        Cancel any heartbeat timers and set the closing flag.\n        \"\"\"\n        self._closing = True\n        self._cancel_heartbeat()\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def close_code(self) -> Optional[int]:\n        return self._close_code\n\n    @property\n    def protocol(self) -> Optional[str]:\n        return self._protocol\n\n    @property\n    def compress(self) -> int:\n        return self._compress\n\n    @property\n    def client_notakeover(self) -> bool:\n        return self._client_notakeover\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"extra info from connection transport\"\"\"\n        conn = self._response.connection\n        if conn is None:\n            return default\n        transport = conn.transport\n        if transport is None:\n            return default\n        return transport.get_extra_info(name, default)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    async def ping(self, message: bytes = b\"\") -> None:\n        await self._writer.send_frame(message, WSMsgType.PING)\n\n    async def pong(self, message: bytes = b\"\") -> None:\n        await self._writer.send_frame(message, WSMsgType.PONG)\n\n    async def send_frame(\n        self, message: bytes, opcode: WSMsgType, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket.\"\"\"\n        await self._writer.send_frame(message, opcode, compress)\n\n    async def send_str(self, data: str, compress: Optional[int] = None) -> None:\n        if not isinstance(data, str):\n            raise TypeError(\"data argument must be str (%r)\" % type(data))\n        await self._writer.send_frame(\n            data.encode(\"utf-8\"), WSMsgType.TEXT, compress=compress\n        )\n\n    async def send_bytes(self, data: bytes, compress: Optional[int] = None) -> None:\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data argument must be byte-ish (%r)\" % type(data))\n        await self._writer.send_frame(data, WSMsgType.BINARY, compress=compress)\n\n    async def send_json(\n        self,\n        data: Any,\n        compress: Optional[int] = None,\n        *,\n        dumps: JSONEncoder = DEFAULT_JSON_ENCODER,\n    ) -> None:\n        await self.send_str(dumps(data), compress=compress)\n\n    async def close(self, *, code: int = WSCloseCode.OK, message: bytes = b\"\") -> bool:\n        # we need to break `receive()` cycle first,\n        # `close()` may be called from different task\n        if self._waiting and not self._closing:\n            assert self._loop is not None\n            self._close_wait = self._loop.create_future()\n            self._set_closing()\n            self._reader.feed_data(WS_CLOSING_MESSAGE)\n            await self._close_wait\n\n        if self._closed:\n            return False\n\n        self._set_closed()\n        try:\n            await self._writer.close(code, message)\n        except asyncio.CancelledError:\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n            self._response.close()\n            raise\n        except Exception as exc:\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n            self._exception = exc\n            self._response.close()\n            return True\n\n        if self._close_code:\n            self._response.close()\n            return True\n\n        while True:\n            try:\n                async with async_timeout.timeout(self._timeout.ws_close):\n                    msg = await self._reader.read()\n            except asyncio.CancelledError:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._response.close()\n                raise\n            except Exception as exc:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._exception = exc\n                self._response.close()\n                return True\n\n            if msg.type is WSMsgType.CLOSE:\n                self._close_code = msg.data\n                self._response.close()\n                return True\n\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\n        receive_timeout = timeout or self._timeout.ws_receive\n\n        while True:\n            if self._waiting:\n                raise RuntimeError(\"Concurrent call to receive() is not allowed\")\n\n            if self._closed:\n                return WS_CLOSED_MESSAGE\n            elif self._closing:\n                await self.close()\n                return WS_CLOSED_MESSAGE\n\n            try:\n                self._waiting = True\n                try:\n                    if receive_timeout:\n                        async with async_timeout.timeout(receive_timeout):\n                            msg = await self._reader.read()\n                    else:\n                        msg = await self._reader.read()\n                    self._reset_heartbeat()\n                finally:\n                    self._waiting = False\n                    if self._close_wait:\n                        set_result(self._close_wait, None)\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                raise\n            except EofStream:\n                self._close_code = WSCloseCode.OK\n                await self.close()\n                return WS_CLOSED_MESSAGE\n            except ClientError:\n                self._set_closed()\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                return WS_CLOSED_MESSAGE\n            except WebSocketError as exc:\n                self._close_code = exc.code\n                await self.close(code=exc.code)\n                return WSMessageError(data=exc)\n            except Exception as exc:\n                self._exception = exc\n                self._set_closing()\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                await self.close()\n                return WSMessageError(data=exc)\n\n            if msg.type not in _INTERNAL_RECEIVE_TYPES:\n                return msg\n\n            if msg.type is WSMsgType.CLOSE:\n                self._set_closing()\n                self._close_code = msg.data\n                if not self._closed and self._autoclose:\n                    await self.close()\n            elif msg.type is WSMsgType.CLOSING:\n                self._set_closing()\n            elif msg.type is WSMsgType.PING and self._autoping:\n                await self.pong(msg.data)\n                continue\n            elif msg.type is WSMsgType.PONG and self._autoping:\n                continue\n\n            return msg\n\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.TEXT:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.TEXT\"\n            )\n        return msg.data\n\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.BINARY:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.BINARY\"\n            )\n        return msg.data\n\n    async def receive_json(\n        self,\n        *,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        timeout: Optional[float] = None,\n    ) -> Any:\n        data = await self.receive_str(timeout=timeout)\n        return loads(data)\n\n    def __aiter__(self) -> \"ClientWebSocketResponse\":\n        return self\n\n    async def __anext__(self) -> WSMessage:\n        msg = await self.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            raise StopAsyncIteration\n        return msg\n\n    async def __aenter__(self) -> \"ClientWebSocketResponse\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        await self.close()\n```"
    },
    {
      "chunk_id": 119,
      "source": "__internal__/data_repo/aiohttp/aiohttp/hdrs.py",
      "content": "\"\"\"HTTP Headers constants.\"\"\""
    },
    {
      "chunk_id": 120,
      "source": "__internal__/data_repo/aiohttp/aiohttp/hdrs.py",
      "content": "import itertools\nfrom typing import Final, Set\n\nfrom multidict import istr"
    },
    {
      "chunk_id": 121,
      "source": "__internal__/data_repo/aiohttp/aiohttp/hdrs.py",
      "content": "METH_ANY: Final[str] = \"*\"\nMETH_CONNECT: Final[str] = \"CONNECT\"\nMETH_HEAD: Final[str] = \"HEAD\"\nMETH_GET: Final[str] = \"GET\"\nMETH_DELETE: Final[str] = \"DELETE\"\nMETH_OPTIONS: Final[str] = \"OPTIONS\"\nMETH_PATCH: Final[str] = \"PATCH\"\nMETH_POST: Final[str] = \"POST\"\nMETH_PUT: Final[str] = \"PUT\"\nMETH_TRACE: Final[str] = \"TRACE\"\n\nMETH_ALL: Final[Set[str]] = {\n    METH_CONNECT,\n    METH_HEAD,\n    METH_GET,\n    METH_DELETE,\n    METH_OPTIONS,\n    METH_PATCH,\n    METH_POST,\n    METH_PUT,\n    METH_TRACE,\n}"
    },
    {
      "chunk_id": 122,
      "source": "__internal__/data_repo/aiohttp/aiohttp/hdrs.py",
      "content": "ACCEPT: Final[istr] = istr(\"Accept\")\nACCEPT_CHARSET: Final[istr] = istr(\"Accept-Charset\")\nACCEPT_ENCODING: Final[istr] = istr(\"Accept-Encoding\")\nACCEPT_LANGUAGE: Final[istr] = istr(\"Accept-Language\")\nACCEPT_RANGES: Final[istr] = istr(\"Accept-Ranges\")\nACCESS_CONTROL_MAX_AGE: Final[istr] = istr(\"Access-Control-Max-Age\")\nACCESS_CONTROL_ALLOW_CREDENTIALS: Final[istr] = istr(\"Access-Control-Allow-Credentials\")\nACCESS_CONTROL_ALLOW_HEADERS: Final[istr] = istr(\"Access-Control-Allow-Headers\")\nACCESS_CONTROL_ALLOW_METHODS: Final[istr] = istr(\"Access-Control-Allow-Methods\")\nACCESS_CONTROL_ALLOW_ORIGIN: Final[istr] = istr(\"Access-Control-Allow-Origin\")\nACCESS_CONTROL_EXPOSE_HEADERS: Final[istr] = istr(\"Access-Control-Expose-Headers\")\nACCESS_CONTROL_REQUEST_HEADERS: Final[istr] = istr(\"Access-Control-Request-Headers\")\nACCESS_CONTROL_REQUEST_METHOD: Final[istr] = istr(\"Access-Control-Request-Method\")\nAGE: Final[istr] = istr(\"Age\")\nALLOW: Final[istr] = istr(\"Allow\")\nAUTHORIZATION: Final[istr] = istr(\"Authorization\")\nCACHE_CONTROL: Final[istr] = istr(\"Cache-Control\")\nCONNECTION: Final[istr] = istr(\"Connection\")\nCONTENT_DISPOSITION: Final[istr] = istr(\"Content-Disposition\")\nCONTENT_ENCODING: Final[istr] = istr(\"Content-Encoding\")\nCONTENT_LANGUAGE: Final[istr] = istr(\"Content-Language\")\nCONTENT_LENGTH: Final[istr] = istr(\"Content-Length\")\nCONTENT_LOCATION: Final[istr] = istr(\"Content-Location\")\nCONTENT_MD5: Final[istr] = istr(\"Content-MD5\")\nCONTENT_RANGE: Final[istr] = istr(\"Content-Range\")\nCONTENT_TRANSFER_ENCODING: Final[istr] = istr(\"Content-Transfer-Encoding\")\nCONTENT_TYPE: Final[istr] = istr(\"Content-Type\")\nCOOKIE: Final[istr] = istr(\"Cookie\")\nDATE: Final[istr] = istr(\"Date\")\nDESTINATION: Final[istr] = istr(\"Destination\")\nDIGEST: Final[istr] = istr(\"Digest\")\nETAG: Final[istr] = istr(\"Etag\")\nEXPECT: Final[istr] = istr(\"Expect\")\nEXPIRES: Final[istr] = istr(\"Expires\")\nFORWARDED: Final[istr] = istr(\"Forwarded\")\nFROM: Final[istr] = istr(\"From\")\nHOST: Final[istr] = istr(\"Host\")\nIF_MATCH: Final[istr] = istr(\"If-Match\")\nIF_MODIFIED_SINCE: Final[istr] = istr(\"If-Modified-Since\")\nIF_NONE_MATCH: Final[istr] = istr(\"If-None-Match\")\nIF_RANGE: Final[istr] = istr(\"If-Range\")\nIF_UNMODIFIED_SINCE: Final[istr] = istr(\"If-Unmodified-Since\")\nKEEP_ALIVE: Final[istr] = istr(\"Keep-Alive\")\nLAST_EVENT_ID: Final[istr] = istr(\"Last-Event-ID\")\nLAST_MODIFIED: Final[istr] = istr(\"Last-Modified\")\nLINK: Final[istr] = istr(\"Link\")\nLOCATION: Final[istr] = istr(\"Location\")\nMAX_FORWARDS: Final[istr] = istr(\"Max-Forwards\")\nORIGIN: Final[istr] = istr(\"Origin\")\nPRAGMA: Final[istr] = istr(\"Pragma\")\nPROXY_AUTHENTICATE: Final[istr] = istr(\"Proxy-Authenticate\")\nPROXY_AUTHORIZATION: Final[istr] = istr(\"Proxy-Authorization\")\nRANGE: Final[istr] = istr(\"Range\")\nREFERER: Final[istr] = istr(\"Referer\")\nRETRY_AFTER: Final[istr] = istr(\"Retry-After\")\nSEC_WEBSOCKET_ACCEPT: Final[istr] = istr(\"Sec-WebSocket-Accept\")\nSEC_WEBSOCKET_VERSION: Final[istr] = istr(\"Sec-WebSocket-Version\")\nSEC_WEBSOCKET_PROTOCOL: Final[istr] = istr(\"Sec-WebSocket-Protocol\")\nSEC_WEBSOCKET_EXTENSIONS: Final[istr] = istr(\"Sec-WebSocket-Extensions\")\nSEC_WEBSOCKET_KEY: Final[istr] = istr(\"Sec-WebSocket-Key\")\nSEC_WEBSOCKET_KEY1: Final[istr] = istr(\"Sec-WebSocket-Key1\")\nSERVER: Final[istr] = istr(\"Server\")\nSET_COOKIE: Final[istr] = istr(\"Set-Cookie\")\nTE: Final[istr] = istr(\"TE\")\nTRAILER: Final[istr] = istr(\"Trailer\")\nTRANSFER_ENCODING: Final[istr] = istr(\"Transfer-Encoding\")\nUPGRADE: Final[istr] = istr(\"Upgrade\")\nURI: Final[istr] = istr(\"URI\")\nUSER_AGENT: Final[istr] = istr(\"User-Agent\")\nVARY: Final[istr] = istr(\"Vary\")\nVIA: Final[istr] = istr(\"Via\")\nWANT_DIGEST: Final[istr] = istr(\"Want-Digest\")\nWARNING: Final[istr] = istr(\"Warning\")\nWWW_AUTHENTICATE: Final[istr] = istr(\"WWW-Authenticate\")\nX_FORWARDED_FOR: Final[istr] = istr(\"X-Forwarded-For\")\nX_FORWARDED_HOST: Final[istr] = istr(\"X-Forwarded-Host\")\nX_FORWARDED_PROTO: Final[istr] = istr(\"X-Forwarded-Proto\")"
    },
    {
      "chunk_id": 123,
      "source": "__internal__/data_repo/aiohttp/aiohttp/hdrs.py",
      "content": "METH_HEAD_ALL: Final = frozenset(\n    map(\"\".join, itertools.product(*zip(METH_HEAD.upper(), METH_HEAD.lower())))\n)\nMETH_CONNECT_ALL: Final = frozenset(\n    map(\"\".join, itertools.product(*zip(METH_CONNECT.upper(), METH_CONNECT.lower())))\n)\nHOST_ALL: Final = frozenset(\n    map(\"\".join, itertools.product(*zip(HOST.upper(), HOST.lower())))\n)"
    },
    {
      "chunk_id": 124,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "import warnings\nfrom http import HTTPStatus\nfrom typing import Any, Iterable, Optional, Set, Tuple\n\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .helpers import CookieMixin\nfrom .typedefs import LooseHeaders, StrOrURL\n\n__all__ = (\n    \"HTTPException\",\n    \"HTTPError\",\n    \"HTTPRedirection\",\n    \"HTTPSuccessful\",\n    \"HTTPOk\",\n    \"HTTPCreated\",\n    \"HTTPAccepted\",\n    \"HTTPNonAuthoritativeInformation\",\n    \"HTTPNoContent\",\n    \"HTTPResetContent\",\n    \"HTTPPartialContent\",\n    \"HTTPMove\",\n    \"HTTPMultipleChoices\",\n    \"HTTPMovedPermanently\",\n    \"HTTPFound\",\n    \"HTTPSeeOther\",\n    \"HTTPNotModified\",\n    \"HTTPUseProxy\",\n    \"HTTPTemporaryRedirect\",\n    \"HTTPPermanentRedirect\",\n    \"HTTPClientError\",\n    \"HTTPBadRequest\",\n    \"HTTPUnauthorized\",\n    \"HTTPPaymentRequired\",\n    \"HTTPForbidden\",\n    \"HTTPNotFound\",\n    \"HTTPMethodNotAllowed\",\n    \"HTTPNotAcceptable\",\n    \"HTTPProxyAuthenticationRequired\",\n    \"HTTPRequestTimeout\",\n    \"HTTPConflict\",\n    \"HTTPGone\",\n    \"HTTPLengthRequired\",\n    \"HTTPPreconditionFailed\",\n    \"HTTPRequestEntityTooLarge\",\n    \"HTTPRequestURITooLong\",\n    \"HTTPUnsupportedMediaType\",\n    \"HTTPRequestRangeNotSatisfiable\",\n    \"HTTPExpectationFailed\",\n    \"HTTPMisdirectedRequest\",\n    \"HTTPUnprocessableEntity\",\n    \"HTTPFailedDependency\",\n    \"HTTPUpgradeRequired\",\n    \"HTTPPreconditionRequired\",\n    \"HTTPTooManyRequests\",\n    \"HTTPRequestHeaderFieldsTooLarge\",\n    \"HTTPUnavailableForLegalReasons\",\n    \"HTTPServerError\",\n    \"HTTPInternalServerError\",\n    \"HTTPNotImplemented\",\n    \"HTTPBadGateway\",\n    \"HTTPServiceUnavailable\",\n    \"HTTPGatewayTimeout\",\n    \"HTTPVersionNotSupported\",\n    \"HTTPVariantAlsoNegotiates\",\n    \"HTTPInsufficientStorage\",\n    \"HTTPNotExtended\",\n    \"HTTPNetworkAuthenticationRequired\",\n)"
    },
    {
      "chunk_id": 125,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class NotAppKeyWarning(UserWarning):\n    \"\"\"Warning when not using AppKey in Application.\"\"\""
    },
    {
      "chunk_id": 126,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPException(CookieMixin, Exception):\n    # You should set in subclasses:\n    # status = 200\n\n    status_code = -1\n    empty_body = False\n    default_reason = \"\"  # Initialized at the end of the module\n\n    def __init__(\n        self,\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        if reason is None:\n            reason = self.default_reason\n        elif \"\\n\" in reason:\n            raise ValueError(\"Reason cannot contain \\\\n\")\n\n        if text is None:\n            if not self.empty_body:\n                text = f\"{self.status_code}: {reason}\"\n        else:\n            if self.empty_body:\n                warnings.warn(\n                    \"text argument is deprecated for HTTP status {} \"\n                    \"since 4.0 and scheduled for removal in 5.0 (#3462),\"\n                    \"the response should be provided without a body\".format(\n                        self.status_code\n                    ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n        if headers is not None:\n            real_headers = CIMultiDict(headers)\n        else:\n            real_headers = CIMultiDict()\n\n        if content_type is not None:\n            if not text:\n                warnings.warn(\n                    \"content_type without text is deprecated \"\n                    \"since 4.0 and scheduled for removal in 5.0 \"\n                    \"(#3462)\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            real_headers[hdrs.CONTENT_TYPE] = content_type\n        elif hdrs.CONTENT_TYPE not in real_headers and text:\n            real_headers[hdrs.CONTENT_TYPE] = \"text/plain\"\n\n        self._reason = reason\n        self._text = text\n        self._headers = real_headers\n        self.args = ()\n\n    def __bool__(self) -> bool:\n        return True\n\n    @property\n    def status(self) -> int:\n        return self.status_code\n\n    @property\n    def reason(self) -> str:\n        return self._reason\n\n    @property\n    def text(self) -> Optional[str]:\n        return self._text\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        return self._headers\n\n    def __str__(self) -> str:\n        return self.reason\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {self.reason}>\"\n\n    __reduce__ = object.__reduce__\n\n    def __getnewargs__(self) -> Tuple[Any, ...]:\n        return self.args"
    },
    {
      "chunk_id": 127,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPError(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 400s and 500s.\"\"\""
    },
    {
      "chunk_id": 128,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRedirection(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 300s.\"\"\""
    },
    {
      "chunk_id": 129,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPSuccessful(HTTPException):\n    \"\"\"Base class for exceptions with status codes in the 200s.\"\"\""
    },
    {
      "chunk_id": 130,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPOk(HTTPSuccessful):\n    status_code = 200"
    },
    {
      "chunk_id": 131,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPCreated(HTTPSuccessful):\n    status_code = 201"
    },
    {
      "chunk_id": 132,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPAccepted(HTTPSuccessful):\n    status_code = 202"
    },
    {
      "chunk_id": 133,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNonAuthoritativeInformation(HTTPSuccessful):\n    status_code = 203"
    },
    {
      "chunk_id": 134,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNoContent(HTTPSuccessful):\n    status_code = 204\n    empty_body = True"
    },
    {
      "chunk_id": 135,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPResetContent(HTTPSuccessful):\n    status_code = 205\n    empty_body = True"
    },
    {
      "chunk_id": 136,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPPartialContent(HTTPSuccessful):\n    status_code = 206"
    },
    {
      "chunk_id": 137,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPMove(HTTPRedirection):\n    def __init__(\n        self,\n        location: StrOrURL,\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        if not location:\n            raise ValueError(\"HTTP redirects need a location to redirect to.\")\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self._location = URL(location)\n        self.headers[\"Location\"] = str(self.location)\n\n    @property\n    def location(self) -> URL:\n        return self._location"
    },
    {
      "chunk_id": 138,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPMultipleChoices(HTTPMove):\n    status_code = 300"
    },
    {
      "chunk_id": 139,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPMovedPermanently(HTTPMove):\n    status_code = 301"
    },
    {
      "chunk_id": 140,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPFound(HTTPMove):\n    status_code = 302"
    },
    {
      "chunk_id": 141,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPSeeOther(HTTPMove):\n    status_code = 303"
    },
    {
      "chunk_id": 142,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNotModified(HTTPRedirection):\n    # FIXME: this should include a date or etag header\n    status_code = 304\n    empty_body = True"
    },
    {
      "chunk_id": 143,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUseProxy(HTTPMove):\n    # Not a move, but looks a little like one\n    status_code = 305"
    },
    {
      "chunk_id": 144,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPTemporaryRedirect(HTTPMove):\n    status_code = 307"
    },
    {
      "chunk_id": 145,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPPermanentRedirect(HTTPMove):\n    status_code = 308"
    },
    {
      "chunk_id": 146,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPClientError(HTTPError):\n    pass"
    },
    {
      "chunk_id": 147,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPBadRequest(HTTPClientError):\n    status_code = 400"
    },
    {
      "chunk_id": 148,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUnauthorized(HTTPClientError):\n    status_code = 401"
    },
    {
      "chunk_id": 149,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPPaymentRequired(HTTPClientError):\n    status_code = 402"
    },
    {
      "chunk_id": 150,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPForbidden(HTTPClientError):\n    status_code = 403"
    },
    {
      "chunk_id": 151,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNotFound(HTTPClientError):\n    status_code = 404"
    },
    {
      "chunk_id": 152,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPMethodNotAllowed(HTTPClientError):\n    status_code = 405\n\n    def __init__(\n        self,\n        method: str,\n        allowed_methods: Iterable[str],\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        allow = \",\".join(sorted(allowed_methods))\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self.headers[\"Allow\"] = allow\n        self._allowed: Set[str] = set(allowed_methods)\n        self._method = method\n\n    @property\n    def allowed_methods(self) -> Set[str]:\n        return self._allowed\n\n    @property\n    def method(self) -> str:\n        return self._method"
    },
    {
      "chunk_id": 153,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNotAcceptable(HTTPClientError):\n    status_code = 406"
    },
    {
      "chunk_id": 154,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPProxyAuthenticationRequired(HTTPClientError):\n    status_code = 407"
    },
    {
      "chunk_id": 155,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRequestTimeout(HTTPClientError):\n    status_code = 408"
    },
    {
      "chunk_id": 156,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPConflict(HTTPClientError):\n    status_code = 409"
    },
    {
      "chunk_id": 157,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPGone(HTTPClientError):\n    status_code = 410"
    },
    {
      "chunk_id": 158,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPLengthRequired(HTTPClientError):\n    status_code = 411"
    },
    {
      "chunk_id": 159,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPPreconditionFailed(HTTPClientError):\n    status_code = 412"
    },
    {
      "chunk_id": 160,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRequestEntityTooLarge(HTTPClientError):\n    status_code = 413\n\n    def __init__(self, max_size: int, actual_size: int, **kwargs: Any) -> None:\n        kwargs.setdefault(\n            \"text\",\n            \"Maximum request body size {} exceeded, \"\n            \"actual body size {}\".format(max_size, actual_size),\n        )\n        super().__init__(**kwargs)"
    },
    {
      "chunk_id": 161,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRequestURITooLong(HTTPClientError):\n    status_code = 414"
    },
    {
      "chunk_id": 162,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUnsupportedMediaType(HTTPClientError):\n    status_code = 415"
    },
    {
      "chunk_id": 163,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRequestRangeNotSatisfiable(HTTPClientError):\n    status_code = 416"
    },
    {
      "chunk_id": 164,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPExpectationFailed(HTTPClientError):\n    status_code = 417"
    },
    {
      "chunk_id": 165,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPMisdirectedRequest(HTTPClientError):\n    status_code = 421"
    },
    {
      "chunk_id": 166,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUnprocessableEntity(HTTPClientError):\n    status_code = 422"
    },
    {
      "chunk_id": 167,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPFailedDependency(HTTPClientError):\n    status_code = 424"
    },
    {
      "chunk_id": 168,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUpgradeRequired(HTTPClientError):\n    status_code = 426"
    },
    {
      "chunk_id": 169,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPPreconditionRequired(HTTPClientError):\n    status_code = 428"
    },
    {
      "chunk_id": 170,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPTooManyRequests(HTTPClientError):\n    status_code = 429"
    },
    {
      "chunk_id": 171,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPRequestHeaderFieldsTooLarge(HTTPClientError):\n    status_code = 431"
    },
    {
      "chunk_id": 172,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPUnavailableForLegalReasons(HTTPClientError):\n    status_code = 451\n\n    def __init__(\n        self,\n        link: Optional[StrOrURL],\n        *,\n        headers: Optional[LooseHeaders] = None,\n        reason: Optional[str] = None,\n        text: Optional[str] = None,\n        content_type: Optional[str] = None,\n    ) -> None:\n        super().__init__(\n            headers=headers, reason=reason, text=text, content_type=content_type\n        )\n        self._link = None\n        if link:\n            self._link = URL(link)\n            self.headers[\"Link\"] = f'<{str(self._link)}>; rel=\"blocked-by\"'\n\n    @property\n    def link(self) -> Optional[URL]:\n        return self._link"
    },
    {
      "chunk_id": 173,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPServerError(HTTPError):\n    pass"
    },
    {
      "chunk_id": 174,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPInternalServerError(HTTPServerError):\n    status_code = 500"
    },
    {
      "chunk_id": 175,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNotImplemented(HTTPServerError):\n    status_code = 501"
    },
    {
      "chunk_id": 176,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPBadGateway(HTTPServerError):\n    status_code = 502"
    },
    {
      "chunk_id": 177,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPServiceUnavailable(HTTPServerError):\n    status_code = 503"
    },
    {
      "chunk_id": 178,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPGatewayTimeout(HTTPServerError):\n    status_code = 504"
    },
    {
      "chunk_id": 179,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPVersionNotSupported(HTTPServerError):\n    status_code = 505"
    },
    {
      "chunk_id": 180,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPVariantAlsoNegotiates(HTTPServerError):\n    status_code = 506"
    },
    {
      "chunk_id": 181,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPInsufficientStorage(HTTPServerError):\n    status_code = 507"
    },
    {
      "chunk_id": 182,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNotExtended(HTTPServerError):\n    status_code = 510"
    },
    {
      "chunk_id": 183,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "class HTTPNetworkAuthenticationRequired(HTTPServerError):\n    status_code = 511"
    },
    {
      "chunk_id": 184,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "def _initialize_default_reason() -> None:\n    for obj in globals().values():\n        if isinstance(obj, type) and issubclass(obj, HTTPException):\n            if obj.status_code >= 0:\n                try:\n                    status = HTTPStatus(obj.status_code)\n                    obj.default_reason = status.phrase\n                except ValueError:\n                    pass"
    },
    {
      "chunk_id": 185,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_exceptions.py",
      "content": "_initialize_default_reason()\ndel _initialize_default_reason"
    },
    {
      "chunk_id": 186,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "```python"
    },
    {
      "chunk_id": 187,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "from types import SimpleNamespace\nfrom typing import TYPE_CHECKING, Any, Awaitable, Generic, Protocol, TypeVar, overload\n\nfrom aiosignal import Signal\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom .client_reqrep import ClientResponse\nfrom .helpers import frozen_dataclass_decorator\n\nif TYPE_CHECKING:\n    from .client import ClientSession\n\n    _ParamT_contra = TypeVar(\"_ParamT_contra\", contravariant=True)\n\n    class _SignalCallback(Protocol[_ParamT_contra]):\n        def __call__(\n            self,\n            __client_session: ClientSession,\n            __trace_config_ctx: SimpleNamespace,\n            __params: _ParamT_contra,\n        ) -> Awaitable[None]: ..."
    },
    {
      "chunk_id": 188,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "__all__ = (\n    \"TraceConfig\",\n    \"TraceRequestStartParams\",\n    \"TraceRequestEndParams\",\n    \"TraceRequestExceptionParams\",\n    \"TraceConnectionQueuedStartParams\",\n    \"TraceConnectionQueuedEndParams\",\n    \"TraceConnectionCreateStartParams\",\n    \"TraceConnectionCreateEndParams\",\n    \"TraceConnectionReuseconnParams\",\n    \"TraceDnsResolveHostStartParams\",\n    \"TraceDnsResolveHostEndParams\",\n    \"TraceDnsCacheHitParams\",\n    \"TraceDnsCacheMissParams\",\n    \"TraceRequestRedirectParams\",\n    \"TraceRequestChunkSentParams\",\n    \"TraceResponseChunkReceivedParams\",\n    \"TraceRequestHeadersSentParams\",\n)"
    },
    {
      "chunk_id": 189,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "_T = TypeVar(\"_T\", covariant=True)\n\nclass _Factory(Protocol[_T]):\n    def __call__(self, **kwargs: Any) -> _T: ..."
    },
    {
      "chunk_id": 190,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "class TraceConfig(Generic[_T]):\n    \"\"\"First-class used to trace requests launched via ClientSession objects.\"\"\"\n\n    @overload\n    def __init__(self: \"TraceConfig[SimpleNamespace]\") -> None: ...\n    @overload\n    def __init__(self, trace_config_ctx_factory: _Factory[_T]) -> None: ...\n    def __init__(\n        self, trace_config_ctx_factory: _Factory[Any] = SimpleNamespace\n    ) -> None:\n        self._on_request_start: Signal[_SignalCallback[TraceRequestStartParams]] = (\n            Signal(self)\n        )\n        self._on_request_chunk_sent: Signal[\n            _SignalCallback[TraceRequestChunkSentParams]\n        ] = Signal(self)\n        self._on_response_chunk_received: Signal[\n            _SignalCallback[TraceResponseChunkReceivedParams]\n        ] = Signal(self)\n        self._on_request_end: Signal[_SignalCallback[TraceRequestEndParams]] = Signal(\n            self\n        )\n        self._on_request_exception: Signal[\n            _SignalCallback[TraceRequestExceptionParams]\n        ] = Signal(self)\n        self._on_request_redirect: Signal[\n            _SignalCallback[TraceRequestRedirectParams]\n        ] = Signal(self)\n        self._on_connection_queued_start: Signal[\n            _SignalCallback[TraceConnectionQueuedStartParams]\n        ] = Signal(self)\n        self._on_connection_queued_end: Signal[\n            _SignalCallback[TraceConnectionQueuedEndParams]\n        ] = Signal(self)\n        self._on_connection_create_start: Signal[\n            _SignalCallback[TraceConnectionCreateStartParams]\n        ] = Signal(self)\n        self._on_connection_create_end: Signal[\n            _SignalCallback[TraceConnectionCreateEndParams]\n        ] = Signal(self)\n        self._on_connection_reuseconn: Signal[\n            _SignalCallback[TraceConnectionReuseconnParams]\n        ] = Signal(self)\n        self._on_dns_resolvehost_start: Signal[\n            _SignalCallback[TraceDnsResolveHostStartParams]\n        ] = Signal(self)\n        self._on_dns_resolvehost_end: Signal[\n            _SignalCallback[TraceDnsResolveHostEndParams]\n        ] = Signal(self)\n        self._on_dns_cache_hit: Signal[_SignalCallback[TraceDnsCacheHitParams]] = (\n            Signal(self)\n        )\n        self._on_dns_cache_miss: Signal[_SignalCallback[TraceDnsCacheMissParams]] = (\n            Signal(self)\n        )\n        self._on_request_headers_sent: Signal[\n            _SignalCallback[TraceRequestHeadersSentParams]\n        ] = Signal(self)\n\n        self._trace_config_ctx_factory: _Factory[_T] = trace_config_ctx_factory\n\n    def trace_config_ctx(self, trace_request_ctx: Any = None) -> _T:\n        \"\"\"Return a new trace_config_ctx instance\"\"\"\n        return self._trace_config_ctx_factory(trace_request_ctx=trace_request_ctx)\n\n    def freeze(self) -> None:\n        self._on_request_start.freeze()\n        self._on_request_chunk_sent.freeze()\n        self._on_response_chunk_received.freeze()\n        self._on_request_end.freeze()\n        self._on_request_exception.freeze()\n        self._on_request_redirect.freeze()\n        self._on_connection_queued_start.freeze()\n        self._on_connection_queued_end.freeze()\n        self._on_connection_create_start.freeze()\n        self._on_connection_create_end.freeze()\n        self._on_connection_reuseconn.freeze()\n        self._on_dns_resolvehost_start.freeze()\n        self._on_dns_resolvehost_end.freeze()\n        self._on_dns_cache_hit.freeze()\n        self._on_dns_cache_miss.freeze()\n        self._on_request_headers_sent.freeze()\n\n    @property\n    def on_request_start(self) -> \"Signal[_SignalCallback[TraceRequestStartParams]]\":\n        return self._on_request_start\n\n    @property\n    def on_request_chunk_sent(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestChunkSentParams]]\":\n        return self._on_request_chunk_sent\n\n    @property\n    def on_response_chunk_received(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceResponseChunkReceivedParams]]\":\n        return self._on_response_chunk_received\n\n    @property\n    def on_request_end(self) -> \"Signal[_SignalCallback[TraceRequestEndParams]]\":\n        return self._on_request_end\n\n    @property\n    def on_request_exception(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestExceptionParams]]\":\n        return self._on_request_exception\n\n    @property\n    def on_request_redirect(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestRedirectParams]]\":\n        return self._on_request_redirect\n\n    @property\n    def on_connection_queued_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionQueuedStartParams]]\":\n        return self._on_connection_queued_start\n\n    @property\n    def on_connection_queued_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionQueuedEndParams]]\":\n        return self._on_connection_queued_end\n\n    @property\n    def on_connection_create_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionCreateStartParams]]\":\n        return self._on_connection_create_start\n\n    @property\n    def on_connection_create_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionCreateEndParams]]\":\n        return self._on_connection_create_end\n\n    @property\n    def on_connection_reuseconn(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceConnectionReuseconnParams]]\":\n        return self._on_connection_reuseconn\n\n    @property\n    def on_dns_resolvehost_start(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceDnsResolveHostStartParams]]\":\n        return self._on_dns_resolvehost_start\n\n    @property\n    def on_dns_resolvehost_end(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceDnsResolveHostEndParams]]\":\n        return self._on_dns_resolvehost_end\n\n    @property\n    def on_dns_cache_hit(self) -> \"Signal[_SignalCallback[TraceDnsCacheHitParams]]\":\n        return self._on_dns_cache_hit\n\n    @property\n    def on_dns_cache_miss(self) -> \"Signal[_SignalCallback[TraceDnsCacheMissParams]]\":\n        return self._on_dns_cache_miss\n\n    @property\n    def on_request_headers_sent(\n        self,\n    ) -> \"Signal[_SignalCallback[TraceRequestHeadersSentParams]]\":\n        return self._on_request_headers_sent"
    },
    {
      "chunk_id": 191,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestStartParams:\n    \"\"\"Parameters sent by the `on_request_start` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\""
    },
    {
      "chunk_id": 192,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestChunkSentParams:\n    \"\"\"Parameters sent by the `on_request_chunk_sent` signal\"\"\"\n\n    method: str\n    url: URL\n    chunk: bytes"
    },
    {
      "chunk_id": 193,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceResponseChunkReceivedParams:\n    \"\"\"Parameters sent by the `on_response_chunk_received` signal\"\"\"\n\n    method: str\n    url: URL\n    chunk: bytes"
    },
    {
      "chunk_id": 194,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestEndParams:\n    \"\"\"Parameters sent by the `on_request_end` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    response: ClientResponse"
    },
    {
      "chunk_id": 195,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestExceptionParams:\n    \"\"\"Parameters sent by the `on_request_exception` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    exception: BaseException"
    },
    {
      "chunk_id": 196,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestRedirectParams:\n    \"\"\"Parameters sent by the `on_request_redirect` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\"\n    response: ClientResponse"
    },
    {
      "chunk_id": 197,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceConnectionQueuedStartParams:\n    \"\"\"Parameters sent by the `on_connection_queued_start` signal\"\"\""
    },
    {
      "chunk_id": 198,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceConnectionQueuedEndParams:\n    \"\"\"Parameters sent by the `on_connection_queued_end` signal\"\"\""
    },
    {
      "chunk_id": 199,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceConnectionCreateStartParams:\n    \"\"\"Parameters sent by the `on_connection_create_start` signal\"\"\""
    },
    {
      "chunk_id": 200,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceConnectionCreateEndParams:\n    \"\"\"Parameters sent by the `on_connection_create_end` signal\"\"\""
    },
    {
      "chunk_id": 201,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceConnectionReuseconnParams:\n    \"\"\"Parameters sent by the `on_connection_reuseconn` signal\"\"\""
    },
    {
      "chunk_id": 202,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceDnsResolveHostStartParams:\n    \"\"\"Parameters sent by the `on_dns_resolvehost_start` signal\"\"\"\n\n    host: str"
    },
    {
      "chunk_id": 203,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceDnsResolveHostEndParams:\n    \"\"\"Parameters sent by the `on_dns_resolvehost_end` signal\"\"\"\n\n    host: str"
    },
    {
      "chunk_id": 204,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceDnsCacheHitParams:\n    \"\"\"Parameters sent by the `on_dns_cache_hit` signal\"\"\"\n\n    host: str"
    },
    {
      "chunk_id": 205,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceDnsCacheMissParams:\n    \"\"\"Parameters sent by the `on_dns_cache_miss` signal\"\"\"\n\n    host: str"
    },
    {
      "chunk_id": 206,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "@frozen_dataclass_decorator\nclass TraceRequestHeadersSentParams:\n    \"\"\"Parameters sent by the `on_request_headers_sent` signal\"\"\"\n\n    method: str\n    url: URL\n    headers: \"CIMultiDict[str]\""
    },
    {
      "chunk_id": 207,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tracing.py",
      "content": "class Trace:\n    \"\"\"Internal dependency holder class.\n\n    Used to keep together the main dependencies used\n    at the moment of send a signal.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: \"ClientSession\",\n        trace_config: TraceConfig[object],\n        trace_config_ctx: Any,\n    ) -> None:\n        self._trace_config = trace_config\n        self._trace_config_ctx = trace_config_ctx\n        self._session = session\n\n    async def send_request_start(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        return await self._trace_config.on_request_start.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestStartParams(method, url, headers),\n        )\n\n    async def send_request_chunk_sent(\n        self, method: str, url: URL, chunk: bytes\n    ) -> None:\n        return await self._trace_config.on_request_chunk_sent.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestChunkSentParams(method, url, chunk),\n        )\n\n    async def send_response_chunk_received(\n        self, method: str, url: URL, chunk: bytes\n    ) -> None:\n        return await self._trace_config.on_response_chunk_received.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceResponseChunkReceivedParams(method, url, chunk),\n        )\n\n    async def send_request_end(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        response: ClientResponse,\n    ) -> None:\n        return await self._trace_config.on_request_end.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestEndParams(method, url, headers, response),\n        )\n\n    async def send_request_exception(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        exception: BaseException,\n    ) -> None:\n        return await self._trace_config.on_request_exception.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestExceptionParams(method, url, headers, exception),\n        )\n\n    async def send_request_redirect(\n        self,\n        method: str,\n        url: URL,\n        headers: \"CIMultiDict[str]\",\n        response: ClientResponse,\n    ) -> None:\n        return await self._trace_config._on_request_redirect.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestRedirectParams(method, url, headers, response),\n        )\n\n    async def send_connection_queued_start(self) -> None:\n        return await self._trace_config.on_connection_queued_start.send(\n            self._session, self._trace_config_ctx, TraceConnectionQueuedStartParams()\n        )\n\n    async def send_connection_queued_end(self) -> None:\n        return await self._trace_config.on_connection_queued_end.send(\n            self._session, self._trace_config_ctx, TraceConnectionQueuedEndParams()\n        )\n\n    async def send_connection_create_start(self) -> None:\n        return await self._trace_config.on_connection_create_start.send(\n            self._session, self._trace_config_ctx, TraceConnectionCreateStartParams()\n        )\n\n    async def send_connection_create_end(self) -> None:\n        return await self._trace_config.on_connection_create_end.send(\n            self._session, self._trace_config_ctx, TraceConnectionCreateEndParams()\n        )\n\n    async def send_connection_reuseconn(self) -> None:\n        return await self._trace_config.on_connection_reuseconn.send(\n            self._session, self._trace_config_ctx, TraceConnectionReuseconnParams()\n        )\n\n    async def send_dns_resolvehost_start(self, host: str) -> None:\n        return await self._trace_config.on_dns_resolvehost_start.send(\n            self._session, self._trace_config_ctx, TraceDnsResolveHostStartParams(host)\n        )\n\n    async def send_dns_resolvehost_end(self, host: str) -> None:\n        return await self._trace_config.on_dns_resolvehost_end.send(\n            self._session, self._trace_config_ctx, TraceDnsResolveHostEndParams(host)\n        )\n\n    async def send_dns_cache_hit(self, host: str) -> None:\n        return await self._trace_config.on_dns_cache_hit.send(\n            self._session, self._trace_config_ctx, TraceDnsCacheHitParams(host)\n        )\n\n    async def send_dns_cache_miss(self, host: str) -> None:\n        return await self._trace_config.on_dns_cache_miss.send(\n            self._session, self._trace_config_ctx, TraceDnsCacheMissParams(host)\n        )\n\n    async def send_request_headers(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        return await self._trace_config._on_request_headers_sent.send(\n            self._session,\n            self._trace_config_ctx,\n            TraceRequestHeadersSentParams(method, url, headers),\n        )\n```"
    },
    {
      "chunk_id": 208,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "\"\"\"Http related parsers and protocol.\"\"\"\n\nimport asyncio\nimport zlib\nfrom typing import (  # noqa\n    Any,\n    Awaitable,\n    Callable,\n    Iterable,\n    List,\n    NamedTuple,\n    Optional,\n    Union,\n)\n\nfrom multidict import CIMultiDict\n\nfrom .abc import AbstractStreamWriter\nfrom .base_protocol import BaseProtocol\nfrom .client_exceptions import ClientConnectionResetError\nfrom .compression_utils import ZLibCompressor\nfrom .helpers import NO_EXTENSIONS\n\n__all__ = (\"StreamWriter\", \"HttpVersion\", \"HttpVersion10\", \"HttpVersion11\")"
    },
    {
      "chunk_id": 209,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "class HttpVersion(NamedTuple):\n    major: int\n    minor: int\n\n\nHttpVersion10 = HttpVersion(1, 0)\nHttpVersion11 = HttpVersion(1, 1)"
    },
    {
      "chunk_id": 210,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\n_T_OnHeadersSent = Optional[Callable[[\"CIMultiDict[str]\"], Awaitable[None]]]"
    },
    {
      "chunk_id": 211,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "class StreamWriter(AbstractStreamWriter):\n\n    length: Optional[int] = None\n    chunked: bool = False\n    _eof: bool = False\n    _compress: Optional[ZLibCompressor] = None\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        loop: asyncio.AbstractEventLoop,\n        on_chunk_sent: _T_OnChunkSent = None,\n        on_headers_sent: _T_OnHeadersSent = None,\n    ) -> None:\n        self._protocol = protocol\n        self.loop = loop\n        self._on_chunk_sent: _T_OnChunkSent = on_chunk_sent\n        self._on_headers_sent: _T_OnHeadersSent = on_headers_sent\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        return self._protocol.transport\n\n    @property\n    def protocol(self) -> BaseProtocol:\n        return self._protocol\n\n    def enable_chunking(self) -> None:\n        self.chunked = True\n\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: int = zlib.Z_DEFAULT_STRATEGY\n    ) -> None:\n        self._compress = ZLibCompressor(encoding=encoding, strategy=strategy)\n\n    def _write(self, chunk: Union[bytes, bytearray, memoryview]) -> None:\n        size = len(chunk)\n        self.buffer_size += size\n        self.output_size += size\n        transport = self._protocol.transport\n        if transport is None or transport.is_closing():\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n        transport.write(chunk)\n\n    def _writelines(self, chunks: Iterable[bytes]) -> None:\n        size = 0\n        for chunk in chunks:\n            size += len(chunk)\n        self.buffer_size += size\n        self.output_size += size\n        transport = self._protocol.transport\n        if transport is None or transport.is_closing():\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n        transport.write(b\"\".join(chunks))\n\n    async def write(\n        self,\n        chunk: Union[bytes, bytearray, memoryview],\n        *,\n        drain: bool = True,\n        LIMIT: int = 0x10000,\n    ) -> None:\n        \"\"\"Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.\n        \"\"\"\n        if self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if isinstance(chunk, memoryview):\n            if chunk.nbytes != len(chunk):\n                # just reshape it\n                chunk = chunk.cast(\"c\")\n\n        if self._compress is not None:\n            chunk = await self._compress.compress(chunk)\n            if not chunk:\n                return\n\n        if self.length is not None:\n            chunk_len = len(chunk)\n            if self.length >= chunk_len:\n                self.length = self.length - chunk_len\n            else:\n                chunk = chunk[: self.length]\n                self.length = 0\n                if not chunk:\n                    return\n\n        if chunk:\n            if self.chunked:\n                self._writelines(\n                    (f\"{len(chunk):x}\\r\\n\".encode(\"ascii\"), chunk, b\"\\r\\n\")\n                )\n            else:\n                self._write(chunk)\n\n            if self.buffer_size > LIMIT and drain:\n                self.buffer_size = 0\n                await self.drain()\n\n    async def write_headers(\n        self, status_line: str, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        \"\"\"Write request/response status and headers.\"\"\"\n        if self._on_headers_sent is not None:\n            await self._on_headers_sent(headers)\n\n        # status + headers\n        buf = _serialize_headers(status_line, headers)\n        self._write(buf)\n\n    def set_eof(self) -> None:\n        \"\"\"Indicate that the message is complete.\"\"\"\n        self._eof = True\n\n    async def write_eof(self, chunk: bytes = b\"\") -> None:\n        if self._eof:\n            return\n\n        if chunk and self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if self._compress:\n            chunks: List[bytes] = []\n            chunks_len = 0\n            if chunk and (compressed_chunk := await self._compress.compress(chunk)):\n                chunks_len = len(compressed_chunk)\n                chunks.append(compressed_chunk)\n\n            flush_chunk = self._compress.flush()\n            chunks_len += len(flush_chunk)\n            chunks.append(flush_chunk)\n            assert chunks_len\n\n            if self.chunked:\n                chunk_len_pre = f\"{chunks_len:x}\\r\\n\".encode(\"ascii\")\n                self._writelines((chunk_len_pre, *chunks, b\"\\r\\n0\\r\\n\\r\\n\"))\n            elif len(chunks) > 1:\n                self._writelines(chunks)\n            else:\n                self._write(chunks[0])\n        elif self.chunked:\n            if chunk:\n                chunk_len_pre = f\"{len(chunk):x}\\r\\n\".encode(\"ascii\")\n                self._writelines((chunk_len_pre, chunk, b\"\\r\\n0\\r\\n\\r\\n\"))\n            else:\n                self._write(b\"0\\r\\n\\r\\n\")\n        elif chunk:\n            self._write(chunk)\n\n        await self.drain()\n\n        self._eof = True\n\n    async def drain(self) -> None:\n        \"\"\"Flush the write buffer.\n\n        The intended use is to write\n\n          await w.write(data)\n          await w.drain()\n        \"\"\"\n        protocol = self._protocol\n        if protocol.transport is not None and protocol._paused:\n            await protocol._drain_helper()"
    },
    {
      "chunk_id": 212,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "def _safe_header(string: str) -> str:\n    if \"\\r\" in string or \"\\n\" in string:\n        raise ValueError(\n            \"Newline or carriage return detected in headers. \"\n            \"Potential header injection attack.\"\n        )\n    return string"
    },
    {
      "chunk_id": 213,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "def _py_serialize_headers(status_line: str, headers: \"CIMultiDict[str]\") -> bytes:\n    headers_gen = (_safe_header(k) + \": \" + _safe_header(v) for k, v in headers.items())\n    line = status_line + \"\\r\\n\" + \"\\r\\n\".join(headers_gen) + \"\\r\\n\\r\\n\"\n    return line.encode(\"utf-8\")"
    },
    {
      "chunk_id": 214,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_writer.py",
      "content": "_serialize_headers = _py_serialize_headers\n\ntry:\n    import aiohttp._http_writer as _http_writer  # type: ignore[import-not-found]\n\n    _c_serialize_headers = _http_writer._serialize_headers\n    if not NO_EXTENSIONS:\n        _serialize_headers = _c_serialize_headers\nexcept ImportError:\n    pass"
    },
    {
      "chunk_id": 215,
      "source": "__internal__/data_repo/aiohttp/aiohttp/formdata.py",
      "content": "import io\nfrom typing import Any, Iterable, List, Optional\nfrom urllib.parse import urlencode\n\nfrom multidict import MultiDict, MultiDictProxy\n\nfrom . import hdrs, multipart, payload\nfrom .helpers import guess_filename\nfrom .payload import Payload\n\n__all__ = (\"FormData\",)"
    },
    {
      "chunk_id": 216,
      "source": "__internal__/data_repo/aiohttp/aiohttp/formdata.py",
      "content": "class FormData:\n    \"\"\"Helper class for form body generation.\n\n    Supports multipart/form-data and application/x-www-form-urlencoded.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: Iterable[Any] = (),\n        quote_fields: bool = True,\n        charset: Optional[str] = None,\n        boundary: Optional[str] = None,\n        *,\n        default_to_multipart: bool = False,\n    ) -> None:\n        self._boundary = boundary\n        self._writer = multipart.MultipartWriter(\"form-data\", boundary=self._boundary)\n        self._fields: List[Any] = []\n        self._is_multipart = default_to_multipart\n        self._is_processed = False\n        self._quote_fields = quote_fields\n        self._charset = charset\n\n        if isinstance(fields, dict):\n            fields = list(fields.items())\n        elif not isinstance(fields, (list, tuple)):\n            fields = (fields,)\n        self.add_fields(*fields)\n\n    @property\n    def is_multipart(self) -> bool:\n        return self._is_multipart\n\n    def add_field(\n        self,\n        name: str,\n        value: Any,\n        *,\n        content_type: Optional[str] = None,\n        filename: Optional[str] = None,\n    ) -> None:\n        if isinstance(value, (io.IOBase, bytes, bytearray, memoryview)):\n            self._is_multipart = True\n\n        type_options: MultiDict[str] = MultiDict({\"name\": name})\n        if filename is not None and not isinstance(filename, str):\n            raise TypeError(\"filename must be an instance of str. Got: %s\" % filename)\n        if filename is None and isinstance(value, io.IOBase):\n            filename = guess_filename(value, name)\n        if filename is not None:\n            type_options[\"filename\"] = filename\n            self._is_multipart = True\n\n        headers = {}\n        if content_type is not None:\n            if not isinstance(content_type, str):\n                raise TypeError(\n                    \"content_type must be an instance of str. Got: %s\" % content_type\n                )\n            headers[hdrs.CONTENT_TYPE] = content_type\n            self._is_multipart = True\n\n        self._fields.append((type_options, headers, value))\n\n    def add_fields(self, *fields: Any) -> None:\n        to_add = list(fields)\n\n        while to_add:\n            rec = to_add.pop(0)\n\n            if isinstance(rec, io.IOBase):\n                k = guess_filename(rec, \"unknown\")\n                self.add_field(k, rec)  # type: ignore[arg-type]\n\n            elif isinstance(rec, (MultiDictProxy, MultiDict)):\n                to_add.extend(rec.items())\n\n            elif isinstance(rec, (list, tuple)) and len(rec) == 2:\n                k, fp = rec\n                self.add_field(k, fp)  # type: ignore[arg-type]\n\n            else:\n                raise TypeError(\n                    \"Only io.IOBase, multidict and (name, file) \"\n                    \"pairs allowed, use .add_field() for passing \"\n                    \"more complex parameters, got {!r}\".format(rec)\n                )\n\n    def _gen_form_urlencoded(self) -> payload.BytesPayload:\n        # form data (x-www-form-urlencoded)\n        data = []\n        for type_options, _, value in self._fields:\n            if not isinstance(value, str):\n                raise TypeError(f\"expected str, got {value!r}\")\n            data.append((type_options[\"name\"], value))\n\n        charset = self._charset if self._charset is not None else \"utf-8\"\n\n        if charset == \"utf-8\":\n            content_type = \"application/x-www-form-urlencoded\"\n        else:\n            content_type = \"application/x-www-form-urlencoded; charset=%s\" % charset\n\n        return payload.BytesPayload(\n            urlencode(data, doseq=True, encoding=charset).encode(),\n            content_type=content_type,\n        )\n\n    def _gen_form_data(self) -> multipart.MultipartWriter:\n        \"\"\"Encode a list of fields using the multipart/form-data MIME format\"\"\"\n        if self._is_processed:\n            raise RuntimeError(\"Form data has been processed already\")\n        for dispparams, headers, value in self._fields:\n            try:\n                if hdrs.CONTENT_TYPE in headers:\n                    part = payload.get_payload(\n                        value,\n                        content_type=headers[hdrs.CONTENT_TYPE],\n                        headers=headers,\n                        encoding=self._charset,\n                    )\n                else:\n                    part = payload.get_payload(\n                        value, headers=headers, encoding=self._charset\n                    )\n            except Exception as exc:\n                raise TypeError(\n                    \"Can not serialize value type: %r\\n \"\n                    \"headers: %r\\n value: %r\" % (type(value), headers, value)\n                ) from exc\n\n            if dispparams:\n                part.set_content_disposition(\n                    \"form-data\", quote_fields=self._quote_fields, **dispparams\n                )\n                # FIXME cgi.FieldStorage doesn't likes body parts with\n                # Content-Length which were sent via chunked transfer encoding\n                assert part.headers is not None\n                part.headers.popall(hdrs.CONTENT_LENGTH, None)\n\n            self._writer.append_payload(part)\n\n        self._is_processed = True\n        return self._writer\n\n    def __call__(self) -> Payload:\n        if self._is_multipart:\n            return self._gen_form_data()\n        else:\n            return self._gen_form_urlencoded()"
    },
    {
      "chunk_id": 217,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "```python"
    },
    {
      "chunk_id": 218,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "\"\"\"Various helper functions\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport contextlib\nimport dataclasses\nimport datetime\nimport enum\nimport functools\nimport inspect\nimport netrc\nimport os\nimport platform\nimport re\nimport sys\nimport time\nimport warnings\nimport weakref\nfrom collections import namedtuple\nfrom contextlib import suppress\nfrom email.parser import HeaderParser\nfrom email.utils import parsedate\nfrom http.cookies import SimpleCookie\nfrom math import ceil\nfrom pathlib import Path\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ContextManager,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    final,\n    get_args,\n    overload,\n)\nfrom urllib.parse import quote\nfrom urllib.request import getproxies, proxy_bypass\n\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy, MultiMapping\nfrom propcache.api import under_cached_property as reify\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .log import client_logger\nfrom .typedefs import PathLike  # noqa\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\nif TYPE_CHECKING:\n    from dataclasses import dataclass as frozen_dataclass_decorator\nelif sys.version_info < (3, 10):\n    frozen_dataclass_decorator = functools.partial(dataclasses.dataclass, frozen=True)\nelse:\n    frozen_dataclass_decorator = functools.partial(\n        dataclasses.dataclass, frozen=True, slots=True\n    )\n\n__all__ = (\"BasicAuth\", \"ChainMapProxy\", \"ETag\", \"frozen_dataclass_decorator\", \"reify\")\n\nPY_310 = sys.version_info >= (3, 10)\n\nCOOKIE_MAX_LENGTH = 4096\n\n_T = TypeVar(\"_T\")\n_S = TypeVar(\"_S\")\n\n_SENTINEL = enum.Enum(\"_SENTINEL\", \"sentinel\")\nsentinel = _SENTINEL.sentinel\n\nNO_EXTENSIONS = bool(os.environ.get(\"AIOHTTP_NO_EXTENSIONS\"))\n\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\nEMPTY_BODY_STATUS_CODES = frozenset((204, 304, *range(100, 200)))\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.2\nEMPTY_BODY_METHODS = hdrs.METH_HEAD_ALL\n\nDEBUG = sys.flags.dev_mode or (\n    not sys.flags.ignore_environment and bool(os.environ.get(\"PYTHONASYNCIODEBUG\"))\n)\n\n\nCHAR = {chr(i) for i in range(0, 128)}\nCTL = {chr(i) for i in range(0, 32)} | {\n    chr(127),\n}\nSEPARATORS = {\n    \"(\",\n    \")\",\n    \"<\",\n    \">\",\n    \"@\",\n    \",\",\n    \";\",\n    \":\",\n    \"\\\\\",\n    '\"',\n    \"/\",\n    \"[\",\n    \"]\",\n    \"?\",\n    \"=\",\n    \"{\",\n    \"}\",\n    \" \",\n    chr(9),\n}\nTOKEN = CHAR ^ CTL ^ SEPARATORS\n\n\njson_re = re.compile(r\"(?:application/|[\\w.-]+/[\\w.+-]+?\\+)json$\", re.IGNORECASE)"
    },
    {
      "chunk_id": 219,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class BasicAuth(namedtuple(\"BasicAuth\", [\"login\", \"password\", \"encoding\"])):\n    \"\"\"Http basic authentication helper.\"\"\"\n\n    def __new__(\n        cls, login: str, password: str = \"\", encoding: str = \"latin1\"\n    ) -> \"BasicAuth\":\n        if login is None:\n            raise ValueError(\"None is not allowed as login value\")\n\n        if password is None:\n            raise ValueError(\"None is not allowed as password value\")\n\n        if \":\" in login:\n            raise ValueError('A \":\" is not allowed in login (RFC 1945#section-11.1)')\n\n        return super().__new__(cls, login, password, encoding)\n\n    @classmethod\n    def decode(cls, auth_header: str, encoding: str = \"latin1\") -> \"BasicAuth\":\n        \"\"\"Create a BasicAuth object from an Authorization HTTP header.\"\"\"\n        try:\n            auth_type, encoded_credentials = auth_header.split(\" \", 1)\n        except ValueError:\n            raise ValueError(\"Could not parse authorization header.\")\n\n        if auth_type.lower() != \"basic\":\n            raise ValueError(\"Unknown authorization method %s\" % auth_type)\n\n        try:\n            decoded = base64.b64decode(\n                encoded_credentials.encode(\"ascii\"), validate=True\n            ).decode(encoding)\n        except binascii.Error:\n            raise ValueError(\"Invalid base64 encoding.\")\n\n        try:\n            # RFC 2617 HTTP Authentication\n            # https://www.ietf.org/rfc/rfc2617.txt\n            # the colon must be present, but the username and password may be\n            # otherwise blank.\n            username, password = decoded.split(\":\", 1)\n        except ValueError:\n            raise ValueError(\"Invalid credentials.\")\n\n        return cls(username, password, encoding=encoding)\n\n    @classmethod\n    def from_url(cls, url: URL, *, encoding: str = \"latin1\") -> Optional[\"BasicAuth\"]:\n        \"\"\"Create BasicAuth from url.\"\"\"\n        if not isinstance(url, URL):\n            raise TypeError(\"url should be yarl.URL instance\")\n        # Check raw_user and raw_password first as yarl is likely\n        # to already have these values parsed from the netloc in the cache.\n        if url.raw_user is None and url.raw_password is None:\n            return None\n        return cls(url.user or \"\", url.password or \"\", encoding=encoding)\n\n    def encode(self) -> str:\n        \"\"\"Encode credentials.\"\"\"\n        creds = (f\"{self.login}:{self.password}\").encode(self.encoding)\n        return \"Basic %s\" % base64.b64encode(creds).decode(self.encoding)"
    },
    {
      "chunk_id": 220,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def strip_auth_from_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    \"\"\"Remove user and password from URL if present and return BasicAuth object.\"\"\"\n    # Check raw_user and raw_password first as yarl is likely\n    # to already have these values parsed from the netloc in the cache.\n    if url.raw_user is None and url.raw_password is None:\n        return url, None\n    return url.with_user(None), BasicAuth(url.user or \"\", url.password or \"\")"
    },
    {
      "chunk_id": 221,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def netrc_from_env() -> Optional[netrc.netrc]:\n    \"\"\"Load netrc from file.\n\n    Attempt to load it from the path specified by the env-var\n    NETRC or in the default location in the user's home directory.\n\n    Returns None if it couldn't be found or fails to parse.\n    \"\"\"\n    netrc_env = os.environ.get(\"NETRC\")\n\n    if netrc_env is not None:\n        netrc_path = Path(netrc_env)\n    else:\n        try:\n            home_dir = Path.home()\n        except RuntimeError as e:  # pragma: no cover\n            # if pathlib can't resolve home, it may raise a RuntimeError\n            client_logger.debug(\n                \"Could not resolve home directory when \"\n                \"trying to look for .netrc file: %s\",\n                e,\n            )\n            return None\n\n        netrc_path = home_dir / (\n            \"_netrc\" if platform.system() == \"Windows\" else \".netrc\"\n        )\n\n    try:\n        return netrc.netrc(str(netrc_path))\n    except netrc.NetrcParseError as e:\n        client_logger.warning(\"Could not parse .netrc file: %s\", e)\n    except OSError as e:\n        netrc_exists = False\n        with contextlib.suppress(OSError):\n            netrc_exists = netrc_path.is_file()\n        # we couldn't read the file (doesn't exist, permissions, etc.)\n        if netrc_env or netrc_exists:\n            # only warn if the environment wanted us to load it,\n            # or it appears like the default file does actually exist\n            client_logger.warning(\"Could not read .netrc file: %s\", e)\n\n    return None"
    },
    {
      "chunk_id": 222,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@frozen_dataclass_decorator\nclass ProxyInfo:\n    proxy: URL\n    proxy_auth: Optional[BasicAuth]"
    },
    {
      "chunk_id": 223,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def basicauth_from_netrc(netrc_obj: Optional[netrc.netrc], host: str) -> BasicAuth:\n    \"\"\"\n    Return :py:class:`~aiohttp.BasicAuth` credentials for ``host`` from ``netrc_obj``.\n\n    :raises LookupError: if ``netrc_obj`` is :py:data:`None` or if no\n            entry is found for the ``host``.\n    \"\"\"\n    if netrc_obj is None:\n        raise LookupError(\"No .netrc file found\")\n    auth_from_netrc = netrc_obj.authenticators(host)\n\n    if auth_from_netrc is None:\n        raise LookupError(f\"No entry for {host!s} found in the `.netrc` file.\")\n    login, account, password = auth_from_netrc\n\n    # TODO(PY311): username = login or account\n    # Up to python 3.10, account could be None if not specified,\n    # and login will be empty string if not specified. From 3.11,\n    # login and account will be empty string if not specified.\n    username = login if (login or account is None) else account\n\n    # TODO(PY311): Remove this, as password will be empty string\n    # if not specified\n    if password is None:\n        password = \"\"  # type: ignore[unreachable]\n\n    return BasicAuth(username, password)"
    },
    {
      "chunk_id": 224,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def proxies_from_env() -> Dict[str, ProxyInfo]:\n    proxy_urls = {\n        k: URL(v)\n        for k, v in getproxies().items()\n        if k in (\"http\", \"https\", \"ws\", \"wss\")\n    }\n    netrc_obj = netrc_from_env()\n    stripped = {k: strip_auth_from_url(v) for k, v in proxy_urls.items()}\n    ret = {}\n    for proto, val in stripped.items():\n        proxy, auth = val\n        if proxy.scheme in (\"https\", \"wss\"):\n            client_logger.warning(\n                \"%s proxies %s are not supported, ignoring\", proxy.scheme.upper(), proxy\n            )\n            continue\n        if netrc_obj and auth is None:\n            if proxy.host is not None:\n                try:\n                    auth = basicauth_from_netrc(netrc_obj, proxy.host)\n                except LookupError:\n                    auth = None\n        ret[proto] = ProxyInfo(proxy, auth)\n    return ret"
    },
    {
      "chunk_id": 225,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def get_env_proxy_for_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    \"\"\"Get a permitted proxy for the given URL from the env.\"\"\"\n    if url.host is not None and proxy_bypass(url.host):\n        raise LookupError(f\"Proxying is disallowed for `{url.host!r}`\")\n\n    proxies_in_env = proxies_from_env()\n    try:\n        proxy_info = proxies_in_env[url.scheme]\n    except KeyError:\n        raise LookupError(f\"No proxies found for `{url!s}` in the env\")\n    else:\n        return proxy_info.proxy, proxy_info.proxy_auth"
    },
    {
      "chunk_id": 226,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@frozen_dataclass_decorator\nclass MimeType:\n    type: str\n    subtype: str\n    suffix: str\n    parameters: \"MultiDictProxy[str]\""
    },
    {
      "chunk_id": 227,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@functools.lru_cache(maxsize=56)\ndef parse_mimetype(mimetype: str) -> MimeType:\n    \"\"\"Parses a MIME type into its components.\n\n    mimetype is a MIME type string.\n\n    Returns a MimeType object.\n\n    Example:\n\n    >>> parse_mimetype('text/html; charset=utf-8')\n    MimeType(type='text', subtype='html', suffix='',\n             parameters={'charset': 'utf-8'})\n\n    \"\"\"\n    if not mimetype:\n        return MimeType(\n            type=\"\", subtype=\"\", suffix=\"\", parameters=MultiDictProxy(MultiDict())\n        )\n\n    parts = mimetype.split(\";\")\n    params: MultiDict[str] = MultiDict()\n    for item in parts[1:]:\n        if not item:\n            continue\n        key, _, value = item.partition(\"=\")\n        params.add(key.lower().strip(), value.strip(' \"'))\n\n    fulltype = parts[0].strip().lower()\n    if fulltype == \"*\":\n        fulltype = \"*/*\"\n\n    mtype, _, stype = fulltype.partition(\"/\")\n    stype, _, suffix = stype.partition(\"+\")\n\n    return MimeType(\n        type=mtype, subtype=stype, suffix=suffix, parameters=MultiDictProxy(params)\n    )"
    },
    {
      "chunk_id": 228,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def guess_filename(obj: Any, default: Optional[str] = None) -> Optional[str]:\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, str) and name[0] != \"<\" and name[-1] != \">\":\n        return Path(name).name\n    return default"
    },
    {
      "chunk_id": 229,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "not_qtext_re = re.compile(r\"[^\\041\\043-\\133\\135-\\176]\")\nQCONTENT = {chr(i) for i in range(0x20, 0x7F)} | {\"\\t\"}"
    },
    {
      "chunk_id": 230,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def quoted_string(content: str) -> str:\n    \"\"\"Return 7-bit content as quoted-string.\n\n    Format content into a quoted-string as defined in RFC5322 for\n    Internet Message Format. Notice that this is not the 8-bit HTTP\n    format, but the 7-bit email format. Content must be in usascii or\n    a ValueError is raised.\n    \"\"\"\n    if not (QCONTENT > set(content)):\n        raise ValueError(f\"bad content for quoted-string {content!r}\")\n    return not_qtext_re.sub(lambda x: \"\\\\\" + x.group(0), content)"
    },
    {
      "chunk_id": 231,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def content_disposition_header(\n    disptype: str,\n    quote_fields: bool = True,\n    _charset: str = \"utf-8\",\n    params: Optional[Dict[str, str]] = None,\n) -> str:\n    \"\"\"Sets ``Content-Disposition`` header for MIME.\n\n    This is the MIME payload Content-Disposition header from RFC 2183\n    and RFC 7579 section 4.2, not the HTTP Content-Disposition from\n    RFC 6266.\n\n    disptype is a disposition type: inline, attachment, form-data.\n    Should be valid extension token (see RFC 2183)\n\n    quote_fields performs value quoting to 7-bit MIME headers\n    according to RFC 7578. Set to quote_fields to False if recipient\n    can take 8-bit file names and field values.\n\n    _charset specifies the charset to use when quote_fields is True.\n\n    params is a dict with disposition params.\n    \"\"\"\n    if not disptype or not (TOKEN > set(disptype)):\n        raise ValueError(f\"bad content disposition type {disptype!r}\")\n\n    value = disptype\n    if params:\n        lparams = []\n        for key, val in params.items():\n            if not key or not (TOKEN > set(key)):\n                raise ValueError(f\"bad content disposition parameter {key!r}={val!r}\")\n            if quote_fields:\n                if key.lower() == \"filename\":\n                    qval = quote(val, \"\", encoding=_charset)\n                    lparams.append((key, '\"%s\"' % qval))\n                else:\n                    try:\n                        qval = quoted_string(val)\n                    except ValueError:\n                        qval = \"\".join(\n                            (_charset, \"''\", quote(val, \"\", encoding=_charset))\n                        )\n                        lparams.append((key + \"*\", qval))\n                    else:\n                        lparams.append((key, '\"%s\"' % qval))\n            else:\n                qval = val.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n                lparams.append((key, '\"%s\"' % qval))\n        sparams = \"; \".join(\"=\".join(pair) for pair in lparams)\n        value = \"; \".join((value, sparams))\n    return value"
    },
    {
      "chunk_id": 232,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def is_expected_content_type(\n    response_content_type: str, expected_content_type: str\n) -> bool:\n    \"\"\"Checks if received content type is processable as an expected one.\n\n    Both arguments should be given without parameters.\n    \"\"\"\n    if expected_content_type == \"application/json\":\n        return json_re.match(response_content_type) is not None\n    return expected_content_type in response_content_type"
    },
    {
      "chunk_id": 233,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def is_ip_address(host: Optional[str]) -> bool:\n    \"\"\"Check if host looks like an IP Address.\n\n    This check is only meant as a heuristic to ensure that\n    a host is not a domain name.\n    \"\"\"\n    if not host:\n        return False\n    # For a host to be an ipv4 address, it must be all numeric.\n    # The host must contain a colon to be an IPv6 address.\n    return \":\" in host or host.replace(\".\", \"\").isdigit()"
    },
    {
      "chunk_id": 234,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "_cached_current_datetime: Optional[int] = None\n_cached_formatted_datetime = \"\""
    },
    {
      "chunk_id": 235,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def rfc822_formatted_time() -> str:\n    global _cached_current_datetime\n    global _cached_formatted_datetime\n\n    now = int(time.time())\n    if now != _cached_current_datetime:\n        # Weekday and month names for HTTP date/time formatting;\n        # always English!\n        # Tuples are constants stored in codeobject!\n        _weekdayname = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n        _monthname = (\n            \"\",  # Dummy so we can use 1-based month numbers\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        )\n\n        year, month, day, hh, mm, ss, wd, *tail = time.gmtime(now)\n        _cached_formatted_datetime = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n            _weekdayname[wd],\n            day,\n            _monthname[month],\n            year,\n            hh,\n            mm,\n            ss,\n        )\n        _cached_current_datetime = now\n    return _cached_formatted_datetime"
    },
    {
      "chunk_id": 236,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def _weakref_handle(info: \"Tuple[weakref.ref[object], str]\") -> None:\n    ref, name = info\n    ob = ref()\n    if ob is not None:\n        with suppress(Exception):\n            getattr(ob, name)()"
    },
    {
      "chunk_id": 237,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def weakref_handle(\n    ob: object,\n    name: str,\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is not None and timeout > 0:\n        when = loop.time() + timeout\n        if timeout >= timeout_ceil_threshold:\n            when = ceil(when)\n\n        return loop.call_at(when, _weakref_handle, (weakref.ref(ob), name))\n    return None"
    },
    {
      "chunk_id": 238,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def call_later(\n    cb: Callable[[], Any],\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is None or timeout <= 0:\n        return None\n    now = loop.time()\n    when = calculate_timeout_when(now, timeout, timeout_ceil_threshold)\n    return loop.call_at(when, cb)"
    },
    {
      "chunk_id": 239,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def calculate_timeout_when(\n    loop_time: float,\n    timeout: float,\n    timeout_ceiling_threshold: float,\n) -> float:\n    \"\"\"Calculate when to execute a timeout.\"\"\"\n    when = loop_time + timeout\n    if timeout > timeout_ceiling_threshold:\n        return ceil(when)\n    return when"
    },
    {
      "chunk_id": 240,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class TimeoutHandle:\n    \"\"\"Timeout handle\"\"\"\n\n    __slots__ = (\"_timeout\", \"_loop\", \"_ceil_threshold\", \"_callbacks\")\n\n    def __init__(\n        self,\n        loop: asyncio.AbstractEventLoop,\n        timeout: Optional[float],\n        ceil_threshold: float = 5,\n    ) -> None:\n        self._timeout = timeout\n        self._loop = loop\n        self._ceil_threshold = ceil_threshold\n        self._callbacks: List[\n            Tuple[Callable[..., None], Tuple[Any, ...], Dict[str, Any]]\n        ] = []\n\n    def register(\n        self, callback: Callable[..., None], *args: Any, **kwargs: Any\n    ) -> None:\n        self._callbacks.append((callback, args, kwargs))\n\n    def close(self) -> None:\n        self._callbacks.clear()\n\n    def start(self) -> Optional[asyncio.TimerHandle]:\n        timeout = self._timeout\n        if timeout is not None and timeout > 0:\n            when = self._loop.time() + timeout\n            if timeout >= self._ceil_threshold:\n                when = ceil(when)\n            return self._loop.call_at(when, self.__call__)\n        else:\n            return None\n\n    def timer(self) -> \"BaseTimerContext\":\n        if self._timeout is not None and self._timeout > 0:\n            timer = TimerContext(self._loop)\n            self.register(timer.timeout)\n            return timer\n        else:\n            return TimerNoop()\n\n    def __call__(self) -> None:\n        for cb, args, kwargs in self._callbacks:\n            with suppress(Exception):\n                cb(*args, **kwargs)\n\n        self._callbacks.clear()"
    },
    {
      "chunk_id": 241,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class BaseTimerContext(ContextManager[\"BaseTimerContext\"]):\n\n    __slots__ = ()\n\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timeout has been exceeded.\"\"\""
    },
    {
      "chunk_id": 242,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class TimerNoop(BaseTimerContext):\n\n    __slots__ = ()\n\n    def __enter__(self) -> BaseTimerContext:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        return"
    },
    {
      "chunk_id": 243,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class TimerContext(BaseTimerContext):\n    \"\"\"Low resolution timeout context manager\"\"\"\n\n    __slots__ = (\"_loop\", \"_tasks\", \"_cancelled\", \"_cancelling\")\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._tasks: List[asyncio.Task[Any]] = []\n        self._cancelled = False\n        self._cancelling = 0\n\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timer has already been cancelled.\"\"\"\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n    def __enter__(self) -> BaseTimerContext:\n        task = asyncio.current_task(loop=self._loop)\n        if task is None:\n            raise RuntimeError(\"Timeout context manager should be used inside a task\")\n\n        if sys.version_info >= (3, 11):\n            # Remember if the task was already cancelling\n            # so when we __exit__ we can decide if we should\n            # raise asyncio.TimeoutError or let the cancellation propagate\n            self._cancelling = task.cancelling()\n\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n        self._tasks.append(task)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        enter_task: Optional[asyncio.Task[Any]] = None\n        if self._tasks:\n            enter_task = self._tasks.pop()\n\n        if exc_type is asyncio.CancelledError and self._cancelled:\n            assert enter_task is not None\n            # The timeout was hit, and the task was cancelled\n            # so we need to uncancel the last task that entered the context manager\n            # since the cancellation should not leak out of the context manager\n            if sys.version_info >= (3, 11):\n                # If the task was already cancelling don't raise\n                # asyncio.TimeoutError and instead return None\n                # to allow the cancellation to propagate\n                if enter_task.uncancel() > self._cancelling:\n                    return None\n            raise asyncio.TimeoutError from exc_val\n        return None\n\n    def timeout(self) -> None:\n        if not self._cancelled:\n            for task in set(self._tasks):\n                task.cancel()\n\n            self._cancelled = True"
    },
    {
      "chunk_id": 244,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def ceil_timeout(\n    delay: Optional[float], ceil_threshold: float = 5\n) -> async_timeout.Timeout:\n    if delay is None or delay <= 0:\n        return async_timeout.timeout(None)\n\n    loop = asyncio.get_running_loop()\n    now = loop.time()\n    when = now + delay\n    if delay > ceil_threshold:\n        when = ceil(when)\n    return async_timeout.timeout_at(when)"
    },
    {
      "chunk_id": 245,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class HeadersMixin:\n    \"\"\"Mixin for handling headers.\"\"\"\n\n    _headers: MultiMapping[str]\n    _content_type: Optional[str] = None\n    _content_dict: Optional[Dict[str, str]] = None\n    _stored_content_type: Union[str, None, _SENTINEL] = sentinel\n\n    def _parse_content_type(self, raw: Optional[str]) -> None:\n        self._stored_content_type = raw\n        if raw is None:\n            # default value according to RFC 2616\n            self._content_type = \"application/octet-stream\"\n            self._content_dict = {}\n        else:\n            msg = HeaderParser().parsestr(\"Content-Type: \" + raw)\n            self._content_type = msg.get_content_type()\n            params = msg.get_params(())\n            self._content_dict = dict(params[1:])  # First element is content type again\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"The value of content part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        assert self._content_type is not None\n        return self._content_type\n\n    @property\n    def charset(self) -> Optional[str]:\n        \"\"\"The value of charset part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        assert self._content_dict is not None\n        return self._content_dict.get(\"charset\")\n\n    @property\n    def content_length(self) -> Optional[int]:\n        \"\"\"The value of Content-Length HTTP header.\"\"\"\n        content_length = self._headers.get(hdrs.CONTENT_LENGTH)\n        return None if content_length is None else int(content_length)"
    },
    {
      "chunk_id": 246,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def set_result(fut: \"asyncio.Future[_T]\", result: _T) -> None:\n    if not fut.done():\n        fut.set_result(result)"
    },
    {
      "chunk_id": 247,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "_EXC_SENTINEL = BaseException()"
    },
    {
      "chunk_id": 248,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class ErrorableProtocol(Protocol):\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = ...,\n    ) -> None: ...  # pragma: no cover"
    },
    {
      "chunk_id": 249,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def set_exception(\n    fut: Union[\"asyncio.Future[_T]\", ErrorableProtocol],\n    exc: Union[Type[BaseException], BaseException],\n    exc_cause: BaseException = _EXC_SENTINEL,\n) -> None:\n    \"\"\"Set future exception.\n\n    If the future is marked as complete, this function is a no-op.\n\n    :param exc_cause: An exception that is a direct cause of ``exc``.\n                      Only set if provided.\n    \"\"\"\n    if asyncio.isfuture(fut) and fut.done():\n        return\n\n    exc_is_sentinel = exc_cause is _EXC_SENTINEL\n    exc_causes_itself = exc is exc_cause\n    if not exc_is_sentinel and not exc_causes_itself:\n        exc.__cause__ = exc_cause\n\n    fut.set_exception(exc)"
    },
    {
      "chunk_id": 250,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@functools.total_ordering\nclass AppKey(Generic[_T]):\n    \"\"\"Keys for static typing support in Application.\"\"\"\n\n    __slots__ = (\"_name\", \"_t\", \"__orig_class__\")\n\n    # This may be set by Python when instantiating with a generic type. We need to\n    # support this, in order to support types that are not concrete classes,\n    # like Iterable, which can't be passed as the second parameter to __init__.\n    __orig_class__: Type[object]\n\n    # TODO(PY314): Change Type to TypeForm (this should resolve unreachable below).\n    def __init__(self, name: str, t: Optional[Type[_T]] = None):\n        # Prefix with module name to help deduplicate key names.\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_name == \"<module>\":\n                module: str = frame.f_globals[\"__name__\"]\n                break\n            frame = frame.f_back\n        else:\n            raise RuntimeError(\"Failed to get module name.\")\n\n        # https://github.com/python/mypy/issues/14209\n        self._name = module + \".\" + name  # type: ignore[possibly-undefined]\n        self._t = t\n\n    def __lt__(self, other: object) -> bool:\n        if isinstance(other, AppKey):\n            return self._name < other._name\n        return True  # Order AppKey above other types.\n\n    def __repr__(self) -> str:\n        t = self._t\n        if t is None:\n            with suppress(AttributeError):\n                # Set to type arg.\n                t = get_args(self.__orig_class__)[0]\n\n        if t is None:\n            t_repr = \"<<Unknown>>\"\n        elif isinstance(t, type):\n            if t.__module__ == \"builtins\":\n                t_repr = t.__qualname__\n            else:\n                t_repr = f\"{t.__module__}.{t.__qualname__}\"\n        else:\n            t_repr = repr(t)  # type: ignore[unreachable]\n        return f\"<AppKey({self._name}, type={t_repr})>\""
    },
    {
      "chunk_id": 251,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@final\nclass ChainMapProxy(Mapping[Union[str, AppKey[Any]], Any]):\n    __slots__ = (\"_maps\",)\n\n    def __init__(self, maps: Iterable[Mapping[Union[str, AppKey[Any]], Any]]) -> None:\n        self._maps = tuple(maps)\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(\n            \"Inheritance class {} from ChainMapProxy \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    @overload  # type: ignore[override]\n    def __getitem__(self, key: AppKey[_T]) -> _T: ...\n\n    @overload\n    def __getitem__(self, key: str) -> Any: ...\n\n    def __getitem__(self, key: Union[str, AppKey[_T]]) -> Any:\n        for mapping in self._maps:\n            try:\n                return mapping[key]\n            except KeyError:\n                pass\n        raise KeyError(key)\n\n    @overload  # type: ignore[override]\n    def get(self, key: AppKey[_T], default: _S) -> Union[_T, _S]: ...\n\n    @overload\n    def get(self, key: AppKey[_T], default: None = ...) -> Optional[_T]: ...\n\n    @overload\n    def get(self, key: str, default: Any = ...) -> Any: ...\n\n    def get(self, key: Union[str, AppKey[_T]], default: Any = None) -> Any:\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __len__(self) -> int:\n        # reuses stored hash values if possible\n        return len(set().union(*self._maps))\n\n    def __iter__(self) -> Iterator[Union[str, AppKey[Any]]]:\n        d: Dict[Union[str, AppKey[Any]], Any] = {}\n        for mapping in reversed(self._maps):\n            # reuses stored hash values if possible\n            d.update(mapping)\n        return iter(d)\n\n    def __contains__(self, key: object) -> bool:\n        return any(key in m for m in self._maps)\n\n    def __bool__(self) -> bool:\n        return any(self._maps)\n\n    def __repr__(self) -> str:\n        content = \", \".join(map(repr, self._maps))\n        return f\"ChainMapProxy({content})\""
    },
    {
      "chunk_id": 252,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "class CookieMixin:\n    \"\"\"Mixin for handling cookies.\"\"\"\n\n    _cookies: Optional[SimpleCookie] = None\n\n    @property\n    def cookies(self) -> SimpleCookie:\n        if self._cookies is None:\n            self._cookies = SimpleCookie()\n        return self._cookies\n\n    def set_cookie(\n        self,\n        name: str,\n        value: str,\n        *,\n        expires: Optional[str] = None,\n        domain: Optional[str] = None,\n        max_age: Optional[Union[int, str]] = None,\n        path: str = \"/\",\n        secure: Optional[bool] = None,\n        httponly: Optional[bool] = None,\n        samesite: Optional[str] = None,\n    ) -> None:\n        \"\"\"Set or update response cookie.\n\n        Sets new cookie or updates existent with new value.\n        Also updates only those params which are not None.\n        \"\"\"\n        if self._cookies is None:\n            self._cookies = SimpleCookie()\n\n        self._cookies[name] = value\n        c = self._cookies[name]\n\n        if expires is not None:\n            c[\"expires\"] = expires\n        elif c.get(\"expires\") == \"Thu, 01 Jan 1970 00:00:00 GMT\":\n            del c[\"expires\"]\n\n        if domain is not None:\n            c[\"domain\"] = domain\n\n        if max_age is not None:\n            c[\"max-age\"] = str(max_age)\n        elif \"max-age\" in c:\n            del c[\"max-age\"]\n\n        c[\"path\"] = path\n\n        if secure is not None:\n            c[\"secure\"] = secure\n        if httponly is not None:\n            c[\"httponly\"] = httponly\n        if samesite is not None:\n            c[\"samesite\"] = samesite\n\n        if DEBUG:\n            cookie_length = len(c.output(header=\"\")[1:])\n            if cookie_length > COOKIE_MAX_LENGTH:\n                warnings.warn(\n                    \"The size of is too large, it might get ignored by the client.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n    def del_cookie(\n        self,\n        name: str,\n        *,\n        domain: Optional[str] = None,\n        path: str = \"/\",\n        secure: Optional[bool] = None,\n        httponly: Optional[bool] = None,\n        samesite: Optional[str] = None,\n    ) -> None:\n        \"\"\"Delete cookie.\n\n        Creates new empty expired cookie.\n        \"\"\"\n        # TODO: do we need domain/path here?\n        if self._cookies is not None:\n            self._cookies.pop(name, None)\n        self.set_cookie(\n            name,\n            \"\",\n            max_age=0,\n            expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n            domain=domain,\n            path=path,\n            secure=secure,\n            httponly=httponly,\n            samesite=samesite,\n        )"
    },
    {
      "chunk_id": 253,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def populate_with_cookies(headers: \"CIMultiDict[str]\", cookies: SimpleCookie) -> None:\n    for cookie in cookies.values():\n        value = cookie.output(header=\"\")[1:]\n        headers.add(hdrs.SET_COOKIE, value)"
    },
    {
      "chunk_id": 254,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "# https://tools.ietf.org/html/rfc7232#section-2.3\n_ETAGC = r\"[!\\x23-\\x7E\\x80-\\xff]+\"\n_ETAGC_RE = re.compile(_ETAGC)\n_QUOTED_ETAG = rf'(W/)?\"({_ETAGC})\"'\nQUOTED_ETAG_RE = re.compile(_QUOTED_ETAG)\nLIST_QUOTED_ETAG_RE = re.compile(rf\"({_QUOTED_ETAG})(?:\\s*,\\s*|$)|(.)\")\n\nETAG_ANY = \"*\""
    },
    {
      "chunk_id": 255,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@frozen_dataclass_decorator\nclass ETag:\n    value: str\n    is_weak: bool = False"
    },
    {
      "chunk_id": 256,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def validate_etag_value(value: str) -> None:\n    if value != ETAG_ANY and not _ETAGC_RE.fullmatch(value):\n        raise ValueError(\n            f\"Value {value!r} is not a valid etag. Maybe it contains '\\\"'?\"\n        )"
    },
    {
      "chunk_id": 257,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def parse_http_date(date_str: Optional[str]) -> Optional[datetime.datetime]:\n    \"\"\"Process a date string, return a datetime object\"\"\"\n    if date_str is not None:\n        timetuple = parsedate(date_str)\n        if timetuple is not None:\n            with suppress(ValueError):\n                return datetime.datetime(*timetuple[:6], tzinfo=datetime.timezone.utc)\n    return None"
    },
    {
      "chunk_id": 258,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "@functools.lru_cache\ndef must_be_empty_body(method: str, code: int) -> bool:\n    \"\"\"Check if a request must return an empty body.\"\"\"\n    return (\n        code in EMPTY_BODY_STATUS_CODES\n        or method in EMPTY_BODY_METHODS\n        or (200 <= code < 300 and method in hdrs.METH_CONNECT_ALL)\n    )"
    },
    {
      "chunk_id": 259,
      "source": "__internal__/data_repo/aiohttp/aiohttp/helpers.py",
      "content": "def should_remove_content_length(method: str, code: int) -> bool:\n    \"\"\"Check if a Content-Length header should be removed.\n\n    This should always be a subset of must_be_empty_body\n    \"\"\"\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.6-8\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-15.4.5-4\n    return code in EMPTY_BODY_STATUS_CODES or (\n        200 <= code < 300 and method in hdrs.METH_CONNECT_ALL\n    )\n```"
    },
    {
      "chunk_id": 260,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "import asyncio\nimport enum\nimport io\nimport json\nimport mimetypes\nimport os\nimport sys\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom itertools import chain\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Final,\n    Iterable,\n    Optional,\n    TextIO,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom multidict import CIMultiDict\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import (\n    _SENTINEL,\n    content_disposition_header,\n    guess_filename,\n    parse_mimetype,\n    sentinel,\n)\nfrom .streams import StreamReader\nfrom .typedefs import JSONEncoder, _CIMultiDict"
    },
    {
      "chunk_id": 261,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class LookupError(Exception):\n    pass"
    },
    {
      "chunk_id": 262,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class Order(str, enum.Enum):\n    normal = \"normal\"\n    try_first = \"try_first\"\n    try_last = \"try_last\""
    },
    {
      "chunk_id": 263,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "def get_payload(data: Any, *args: Any, **kwargs: Any) -> \"Payload\":\n    return PAYLOAD_REGISTRY.get(data, *args, **kwargs)"
    },
    {
      "chunk_id": 264,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "def register_payload(\n    factory: Type[\"Payload\"], type: Any, *, order: Order = Order.normal\n) -> None:\n    PAYLOAD_REGISTRY.register(factory, type, order=order)"
    },
    {
      "chunk_id": 265,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class payload_type:\n    def __init__(self, type: Any, *, order: Order = Order.normal) -> None:\n        self.type = type\n        self.order = order\n\n    def __call__(self, factory: Type[\"Payload\"]) -> Type[\"Payload\"]:\n        register_payload(factory, self.type, order=self.order)\n        return factory"
    },
    {
      "chunk_id": 266,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class PayloadRegistry:\n    \"\"\"Payload registry.\n\n    note: we need zope.interface for more efficient adapter search\n    \"\"\"\n\n    __slots__ = (\"_first\", \"_normal\", \"_last\", \"_normal_lookup\")\n\n    def __init__(self) -> None:\n        self._first: List[_PayloadRegistryItem] = []\n        self._normal: List[_PayloadRegistryItem] = []\n        self._last: List[_PayloadRegistryItem] = []\n        self._normal_lookup: Dict[Any, PayloadType] = {}\n\n    def get(\n        self,\n        data: Any,\n        *args: Any,\n        _CHAIN: \"Type[chain[_PayloadRegistryItem]]\" = chain,\n        **kwargs: Any,\n    ) -> \"Payload\":\n        if self._first:\n            for factory, type_ in self._first:\n                if isinstance(data, type_):\n                    return factory(data, *args, **kwargs)\n        # Try the fast lookup first\n        if lookup_factory := self._normal_lookup.get(type(data)):\n            return lookup_factory(data, *args, **kwargs)\n        # Bail early if its already a Payload\n        if isinstance(data, Payload):\n            return data\n        # Fallback to the slower linear search\n        for factory, type_ in _CHAIN(self._normal, self._last):\n            if isinstance(data, type_):\n                return factory(data, *args, **kwargs)\n        raise LookupError()\n\n    def register(\n        self, factory: PayloadType, type: Any, *, order: Order = Order.normal\n    ) -> None:\n        if order is Order.try_first:\n            self._first.append((factory, type))\n        elif order is Order.normal:\n            self._normal.append((factory, type))\n            if isinstance(type, Iterable):\n                for t in type:\n                    self._normal_lookup[t] = factory\n            else:\n                self._normal_lookup[type] = factory\n        elif order is Order.try_last:\n            self._last.append((factory, type))\n        else:\n            raise ValueError(f\"Unsupported order {order!r}\")"
    },
    {
      "chunk_id": 267,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class Payload(ABC):\n    _default_content_type: str = \"application/octet-stream\"\n    _size: Optional[int] = None\n\n    def __init__(\n        self,\n        value: Any,\n        headers: Optional[\n            Union[_CIMultiDict, Dict[str, str], Iterable[Tuple[str, str]]]\n        ] = None,\n        content_type: Union[None, str, _SENTINEL] = sentinel,\n        filename: Optional[str] = None,\n        encoding: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._encoding = encoding\n        self._filename = filename\n        self._headers: _CIMultiDict = CIMultiDict()\n        self._value = value\n        if content_type is not sentinel and content_type is not None:\n            assert isinstance(content_type, str)\n            self._headers[hdrs.CONTENT_TYPE] = content_type\n        elif self._filename is not None:\n            if sys.version_info >= (3, 13):\n                guesser = mimetypes.guess_file_type\n            else:\n                guesser = mimetypes.guess_type\n            content_type = guesser(self._filename)[0]\n            if content_type is None:\n                content_type = self._default_content_type\n            self._headers[hdrs.CONTENT_TYPE] = content_type\n        else:\n            self._headers[hdrs.CONTENT_TYPE] = self._default_content_type\n        if headers:\n            self._headers.update(headers)\n\n    @property\n    def size(self) -> Optional[int]:\n        \"\"\"Size of the payload.\"\"\"\n        return self._size\n\n    @property\n    def filename(self) -> Optional[str]:\n        \"\"\"Filename of the payload.\"\"\"\n        return self._filename\n\n    @property\n    def headers(self) -> _CIMultiDict:\n        \"\"\"Custom item headers\"\"\"\n        return self._headers\n\n    @property\n    def _binary_headers(self) -> bytes:\n        return (\n            \"\".join([k + \": \" + v + \"\\r\\n\" for k, v in self.headers.items()]).encode(\n                \"utf-8\"\n            )\n            + b\"\\r\\n\"\n        )\n\n    @property\n    def encoding(self) -> Optional[str]:\n        \"\"\"Payload encoding\"\"\"\n        return self._encoding\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"Content type\"\"\"\n        return self._headers[hdrs.CONTENT_TYPE]\n\n    def set_content_disposition(\n        self,\n        disptype: str,\n        quote_fields: bool = True,\n        _charset: str = \"utf-8\",\n        **params: str,\n    ) -> None:\n        \"\"\"Sets ``Content-Disposition`` header.\"\"\"\n        self._headers[hdrs.CONTENT_DISPOSITION] = content_disposition_header(\n            disptype, quote_fields=quote_fields, _charset=_charset, params=params\n        )\n\n    @abstractmethod\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        \"\"\"Return string representation of the value.\n\n        This is named decode() to allow compatibility with bytes objects.\n        \"\"\"\n\n    @abstractmethod\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        \"\"\"Write payload.\n\n        writer is an AbstractStreamWriter instance:\n        \"\"\""
    },
    {
      "chunk_id": 268,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class BytesPayload(Payload):\n    _value: bytes\n\n    def __init__(\n        self, value: Union[bytes, bytearray, memoryview], *args: Any, **kwargs: Any\n    ) -> None:\n        if \"content_type\" not in kwargs:\n            kwargs[\"content_type\"] = \"application/octet-stream\"\n\n        super().__init__(value, *args, **kwargs)\n\n        if isinstance(value, memoryview):\n            self._size = value.nbytes\n        elif isinstance(value, (bytes, bytearray)):\n            self._size = len(value)\n        else:\n            raise TypeError(f\"value argument must be byte-ish, not {type(value)!r}\")\n\n        if self._size > TOO_LARGE_BYTES_BODY:\n            warnings.warn(\n                \"Sending a large body directly with raw bytes might\"\n                \" lock the event loop. You should probably pass an \"\n                \"io.BytesIO object instead\",\n                ResourceWarning,\n                source=self,\n            )\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return self._value.decode(encoding, errors)\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        await writer.write(self._value)"
    },
    {
      "chunk_id": 269,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class StringPayload(BytesPayload):\n    def __init__(\n        self,\n        value: str,\n        *args: Any,\n        encoding: Optional[str] = None,\n        content_type: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        if encoding is None:\n            if content_type is None:\n                real_encoding = \"utf-8\"\n                content_type = \"text/plain; charset=utf-8\"\n            else:\n                mimetype = parse_mimetype(content_type)\n                real_encoding = mimetype.parameters.get(\"charset\", \"utf-8\")\n        else:\n            if content_type is None:\n                content_type = \"text/plain; charset=%s\" % encoding\n            real_encoding = encoding\n\n        super().__init__(\n            value.encode(real_encoding),\n            encoding=real_encoding,\n            content_type=content_type,\n            *args,\n            **kwargs,\n        )"
    },
    {
      "chunk_id": 270,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class StringIOPayload(StringPayload):\n    def __init__(self, value: IO[str], *args: Any, **kwargs: Any) -> None:\n        super().__init__(value.read(), *args, **kwargs)"
    },
    {
      "chunk_id": 271,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class IOBasePayload(Payload):\n    _value: io.IOBase\n\n    def __init__(\n        self, value: IO[Any], disposition: str = \"attachment\", *args: Any, **kwargs: Any\n    ) -> None:\n        if \"filename\" not in kwargs:\n            kwargs[\"filename\"] = guess_filename(value)\n\n        super().__init__(value, *args, **kwargs)\n\n        if self._filename is not None and disposition is not None:\n            if hdrs.CONTENT_DISPOSITION not in self.headers:\n                self.set_content_disposition(disposition, filename=self._filename)\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        loop = asyncio.get_event_loop()\n        try:\n            chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n            while chunk:\n                await writer.write(chunk)\n                chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n        finally:\n            await loop.run_in_executor(None, self._value.close)\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return \"\".join(r.decode(encoding, errors) for r in self._value.readlines())"
    },
    {
      "chunk_id": 272,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class TextIOPayload(IOBasePayload):\n    _value: io.TextIOBase\n\n    def __init__(\n        self,\n        value: TextIO,\n        *args: Any,\n        encoding: Optional[str] = None,\n        content_type: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        if encoding is None:\n            if content_type is None:\n                encoding = \"utf-8\"\n                content_type = \"text/plain; charset=utf-8\"\n            else:\n                mimetype = parse_mimetype(content_type)\n                encoding = mimetype.parameters.get(\"charset\", \"utf-8\")\n        else:\n            if content_type is None:\n                content_type = \"text/plain; charset=%s\" % encoding\n\n        super().__init__(\n            value,\n            content_type=content_type,\n            encoding=encoding,\n            *args,\n            **kwargs,\n        )\n\n    @property\n    def size(self) -> Optional[int]:\n        try:\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\n        except OSError:\n            return None\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return self._value.read()\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        loop = asyncio.get_event_loop()\n        try:\n            chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n            while chunk:\n                data = (\n                    chunk.encode(encoding=self._encoding)\n                    if self._encoding\n                    else chunk.encode()\n                )\n                await writer.write(data)\n                chunk = await loop.run_in_executor(None, self._value.read, 2**16)\n        finally:\n            await loop.run_in_executor(None, self._value.close)"
    },
    {
      "chunk_id": 273,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class BytesIOPayload(IOBasePayload):\n    _value: io.BytesIO\n\n    @property\n    def size(self) -> int:\n        position = self._value.tell()\n        end = self._value.seek(0, os.SEEK_END)\n        self._value.seek(position)\n        return end - position\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return self._value.read().decode(encoding, errors)"
    },
    {
      "chunk_id": 274,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class BufferedReaderPayload(IOBasePayload):\n    _value: io.BufferedIOBase\n\n    @property\n    def size(self) -> Optional[int]:\n        try:\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\n        except (OSError, AttributeError):\n            # data.fileno() is not supported, e.g.\n            # io.BufferedReader(io.BytesIO(b'data'))\n            # For some file-like objects (e.g. tarfile), the fileno() attribute may\n            # not exist at all, and will instead raise an AttributeError.\n            return None\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return self._value.read().decode(encoding, errors)"
    },
    {
      "chunk_id": 275,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class JsonPayload(BytesPayload):\n    def __init__(\n        self,\n        value: Any,\n        encoding: str = \"utf-8\",\n        content_type: str = \"application/json\",\n        dumps: JSONEncoder = json.dumps,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(\n            dumps(value).encode(encoding),\n            content_type=content_type,\n            encoding=encoding,\n            *args,\n            **kwargs,\n        )"
    },
    {
      "chunk_id": 276,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class AsyncIterablePayload(Payload):\n    _iter: Optional[_AsyncIterator] = None\n    _value: _AsyncIterable\n\n    def __init__(self, value: _AsyncIterable, *args: Any, **kwargs: Any) -> None:\n        if not isinstance(value, AsyncIterable):\n            raise TypeError(\n                \"value argument must support \"\n                \"collections.abc.AsyncIterable interface, \"\n                \"got {!r}\".format(type(value))\n            )\n\n        if \"content_type\" not in kwargs:\n            kwargs[\"content_type\"] = \"application/octet-stream\"\n\n        super().__init__(value, *args, **kwargs)\n\n        self._iter = value.__aiter__()\n\n    async def write(self, writer: AbstractStreamWriter) -> None:\n        if self._iter:\n            try:\n                # iter is not None check prevents rare cases\n                # when the case iterable is used twice\n                while True:\n                    chunk = await self._iter.__anext__()\n                    await writer.write(chunk)\n            except StopAsyncIteration:\n                self._iter = None\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        raise TypeError(\"Unable to decode.\")"
    },
    {
      "chunk_id": 277,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "class StreamReaderPayload(AsyncIterablePayload):\n    def __init__(self, value: StreamReader, *args: Any, **kwargs: Any) -> None:\n        super().__init__(value.iter_any(), *args, **kwargs)"
    },
    {
      "chunk_id": 278,
      "source": "__internal__/data_repo/aiohttp/aiohttp/payload.py",
      "content": "PAYLOAD_REGISTRY = PayloadRegistry()\nPAYLOAD_REGISTRY.register(BytesPayload, (bytes, bytearray, memoryview))\nPAYLOAD_REGISTRY.register(StringPayload, str)\nPAYLOAD_REGISTRY.register(StringIOPayload, io.StringIO)\nPAYLOAD_REGISTRY.register(TextIOPayload, io.TextIOBase)\nPAYLOAD_REGISTRY.register(BytesIOPayload, io.BytesIO)\nPAYLOAD_REGISTRY.register(BufferedReaderPayload, (io.BufferedReader, io.BufferedRandom))\nPAYLOAD_REGISTRY.register(IOBasePayload, io.IOBase)\nPAYLOAD_REGISTRY.register(StreamReaderPayload, StreamReader)\n# try_last for giving a chance to more specialized async interables like\n# multidict.BodyPartReaderPayload override the default\nPAYLOAD_REGISTRY.register(AsyncIterablePayload, AsyncIterable, order=Order.try_last)"
    },
    {
      "chunk_id": 279,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "import asyncio\nimport logging\nimport os\nimport socket\nimport sys\nimport warnings\nfrom argparse import ArgumentParser\nfrom collections.abc import Iterable\nfrom contextlib import suppress\nfrom importlib import import_module\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Iterable as TypingIterable,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    cast,\n)\n\nfrom .abc import AbstractAccessLogger\nfrom .helpers import AppKey\nfrom .log import access_logger\nfrom .typedefs import PathLike\nfrom .web_app import Application, CleanupError\nfrom .web_exceptions import (\n    HTTPAccepted,\n    HTTPBadGateway,\n    HTTPBadRequest,\n    HTTPClientError,\n    HTTPConflict,\n    HTTPCreated,\n    HTTPError,\n    HTTPException,\n    HTTPExpectationFailed,\n    HTTPFailedDependency,\n    HTTPForbidden,\n    HTTPFound,\n    HTTPGatewayTimeout,\n    HTTPGone,\n    HTTPInsufficientStorage,\n    HTTPInternalServerError,\n    HTTPLengthRequired,\n    HTTPMethodNotAllowed,\n    HTTPMisdirectedRequest,\n    HTTPMove,\n    HTTPMovedPermanently,\n    HTTPMultipleChoices,\n    HTTPNetworkAuthenticationRequired,\n    HTTPNoContent,\n    HTTPNonAuthoritativeInformation,\n    HTTPNotAcceptable,\n    HTTPNotExtended,\n    HTTPNotFound,\n    HTTPNotImplemented,\n    HTTPNotModified,\n    HTTPOk,\n    HTTPPartialContent,\n    HTTPPaymentRequired,\n    HTTPPermanentRedirect,\n    HTTPPreconditionFailed,\n    HTTPPreconditionRequired,\n    HTTPProxyAuthenticationRequired,\n    HTTPRedirection,\n    HTTPRequestEntityTooLarge,\n    HTTPRequestHeaderFieldsTooLarge,\n    HTTPRequestRangeNotSatisfiable,\n    HTTPRequestTimeout,\n    HTTPRequestURITooLong,\n    HTTPResetContent,\n    HTTPSeeOther,\n    HTTPServerError,\n    HTTPServiceUnavailable,\n    HTTPSuccessful,\n    HTTPTemporaryRedirect,\n    HTTPTooManyRequests,\n    HTTPUnauthorized,\n    HTTPUnavailableForLegalReasons,\n    HTTPUnprocessableEntity,\n    HTTPUnsupportedMediaType,\n    HTTPUpgradeRequired,\n    HTTPUseProxy,\n    HTTPVariantAlsoNegotiates,\n    HTTPVersionNotSupported,\n    NotAppKeyWarning,\n)\nfrom .web_fileresponse import FileResponse\nfrom .web_log import AccessLogger\nfrom .web_middlewares import middleware, normalize_path_middleware\nfrom .web_protocol import PayloadAccessError, RequestHandler, RequestPayloadError\nfrom .web_request import BaseRequest, FileField, Request\nfrom .web_response import ContentCoding, Response, StreamResponse, json_response\nfrom .web_routedef import (\n    AbstractRouteDef,\n    RouteDef,\n    RouteTableDef,\n    StaticDef,\n    delete,\n    get,\n    head,\n    options,\n    patch,\n    post,\n    put,\n    route,\n    static,\n    view,\n)\nfrom .web_runner import (\n    AppRunner,\n    BaseRunner,\n    BaseSite,\n    GracefulExit,\n    NamedPipeSite,\n    ServerRunner,\n    SockSite,\n    TCPSite,\n    UnixSite,\n)\nfrom .web_server import Server\nfrom .web_urldispatcher import (\n    AbstractResource,\n    AbstractRoute,\n    DynamicResource,\n    PlainResource,\n    PrefixedSubAppResource,\n    Resource,\n    ResourceRoute,\n    StaticResource,\n    UrlDispatcher,\n    UrlMappingMatchInfo,\n    View,\n)\nfrom .web_ws import WebSocketReady, WebSocketResponse, WSMsgType"
    },
    {
      "chunk_id": 280,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "if TYPE_CHECKING:\n    from ssl import SSLContext\nelse:\n    try:\n        from ssl import SSLContext\n    except ImportError:  # pragma: no cover\n        SSLContext = object  # type: ignore[misc,assignment]\n\n# Only display warning when using -Wdefault, -We, -X dev or similar.\nwarnings.filterwarnings(\"ignore\", category=NotAppKeyWarning, append=True)"
    },
    {
      "chunk_id": 281,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "HostSequence = TypingIterable[str]"
    },
    {
      "chunk_id": 282,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "async def _run_app(\n    app: Union[Application, Awaitable[Application]],\n    *,\n    host: Optional[Union[str, HostSequence]] = None,\n    port: Optional[int] = None,\n    path: Union[PathLike, TypingIterable[PathLike], None] = None,\n    sock: Optional[Union[socket.socket, TypingIterable[socket.socket]]] = None,\n    shutdown_timeout: float = 60.0,\n    keepalive_timeout: float = 75.0,\n    ssl_context: Optional[SSLContext] = None,\n    print: Optional[Callable[..., None]] = print,\n    backlog: int = 128,\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n    access_log_format: str = AccessLogger.LOG_FORMAT,\n    access_log: Optional[logging.Logger] = access_logger,\n    handle_signals: bool = True,\n    reuse_address: Optional[bool] = None,\n    reuse_port: Optional[bool] = None,\n    handler_cancellation: bool = False,\n) -> None:\n    # An internal function to actually do all dirty job for application running\n    if asyncio.iscoroutine(app):\n        app = await app\n\n    app = cast(Application, app)\n\n    runner = AppRunner(\n        app,\n        handle_signals=handle_signals,\n        access_log_class=access_log_class,\n        access_log_format=access_log_format,\n        access_log=access_log,\n        keepalive_timeout=keepalive_timeout,\n        shutdown_timeout=shutdown_timeout,\n        handler_cancellation=handler_cancellation,\n    )\n\n    await runner.setup()\n\n    sites: List[BaseSite] = []\n\n    try:\n        if host is not None:\n            if isinstance(host, str):\n                sites.append(\n                    TCPSite(\n                        runner,\n                        host,\n                        port,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                        reuse_address=reuse_address,\n                        reuse_port=reuse_port,\n                    )\n                )\n            else:\n                for h in host:\n                    sites.append(\n                        TCPSite(\n                            runner,\n                            h,\n                            port,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                            reuse_address=reuse_address,\n                            reuse_port=reuse_port,\n                        )\n                    )\n        elif path is None and sock is None or port is not None:\n            sites.append(\n                TCPSite(\n                    runner,\n                    port=port,\n                    ssl_context=ssl_context,\n                    backlog=backlog,\n                    reuse_address=reuse_address,\n                    reuse_port=reuse_port,\n                )\n            )\n\n        if path is not None:\n            if isinstance(path, (str, os.PathLike)):\n                sites.append(\n                    UnixSite(\n                        runner,\n                        path,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                    )\n                )\n            else:\n                for p in path:\n                    sites.append(\n                        UnixSite(\n                            runner,\n                            p,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                        )\n                    )\n\n        if sock is not None:\n            if not isinstance(sock, Iterable):\n                sites.append(\n                    SockSite(\n                        runner,\n                        sock,\n                        ssl_context=ssl_context,\n                        backlog=backlog,\n                    )\n                )\n            else:\n                for s in sock:\n                    sites.append(\n                        SockSite(\n                            runner,\n                            s,\n                            ssl_context=ssl_context,\n                            backlog=backlog,\n                        )\n                    )\n        for site in sites:\n            await site.start()\n\n        if print:  # pragma: no branch\n            names = sorted(str(s.name) for s in runner.sites)\n            print(\n                \"======== Running on {} ========\\n\"\n                \"(Press CTRL+C to quit)\".format(\", \".join(names))\n            )\n\n        # sleep forever by 1 hour intervals,\n        while True:\n            await asyncio.sleep(3600)\n    finally:\n        await runner.cleanup()"
    },
    {
      "chunk_id": 283,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "def _cancel_tasks(\n    to_cancel: Set[\"asyncio.Task[Any]\"], loop: asyncio.AbstractEventLoop\n) -> None:\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(asyncio.gather(*to_cancel, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler(\n                {\n                    \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                    \"exception\": task.exception(),\n                    \"task\": task,\n                }\n            )"
    },
    {
      "chunk_id": 284,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "def run_app(\n    app: Union[Application, Awaitable[Application]],\n    *,\n    debug: bool = False,\n    host: Optional[Union[str, HostSequence]] = None,\n    port: Optional[int] = None,\n    path: Union[PathLike, TypingIterable[PathLike], None] = None,\n    sock: Optional[Union[socket.socket, TypingIterable[socket.socket]]] = None,\n    shutdown_timeout: float = 60.0,\n    keepalive_timeout: float = 75.0,\n    ssl_context: Optional[SSLContext] = None,\n    print: Optional[Callable[..., None]] = print,\n    backlog: int = 128,\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n    access_log_format: str = AccessLogger.LOG_FORMAT,\n    access_log: Optional[logging.Logger] = access_logger,\n    handle_signals: bool = True,\n    reuse_address: Optional[bool] = None,\n    reuse_port: Optional[bool] = None,\n    handler_cancellation: bool = False,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n) -> None:\n    \"\"\"Run an app locally\"\"\"\n    if loop is None:\n        loop = asyncio.new_event_loop()\n    loop.set_debug(debug)\n\n    # Configure if and only if in debugging mode and using the default logger\n    if loop.get_debug() and access_log and access_log.name == \"aiohttp.access\":\n        if access_log.level == logging.NOTSET:\n            access_log.setLevel(logging.DEBUG)\n        if not access_log.hasHandlers():\n            access_log.addHandler(logging.StreamHandler())\n\n    main_task = loop.create_task(\n        _run_app(\n            app,\n            host=host,\n            port=port,\n            path=path,\n            sock=sock,\n            shutdown_timeout=shutdown_timeout,\n            keepalive_timeout=keepalive_timeout,\n            ssl_context=ssl_context,\n            print=print,\n            backlog=backlog,\n            access_log_class=access_log_class,\n            access_log_format=access_log_format,\n            access_log=access_log,\n            handle_signals=handle_signals,\n            reuse_address=reuse_address,\n            reuse_port=reuse_port,\n            handler_cancellation=handler_cancellation,\n        )\n    )\n\n    try:\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(main_task)\n    except (GracefulExit, KeyboardInterrupt):  # pragma: no cover\n        pass\n    finally:\n        try:\n            main_task.cancel()\n            with suppress(asyncio.CancelledError):\n                loop.run_until_complete(main_task)\n        finally:\n            _cancel_tasks(asyncio.all_tasks(loop), loop)\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.close()\n            asyncio.set_event_loop(None)"
    },
    {
      "chunk_id": 285,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "def main(argv: List[str]) -> None:\n    arg_parser = ArgumentParser(\n        description=\"aiohttp.web Application server\", prog=\"aiohttp.web\"\n    )\n    arg_parser.add_argument(\n        \"entry_func\",\n        help=(\n            \"Callable returning the `aiohttp.web.Application` instance to \"\n            \"run. Should be specified in the 'module:function' syntax.\"\n        ),\n        metavar=\"entry-func\",\n    )\n    arg_parser.add_argument(\n        \"-H\",\n        \"--hostname\",\n        help=\"TCP/IP hostname to serve on (default: localhost)\",\n        default=None,\n    )\n    arg_parser.add_argument(\n        \"-P\",\n        \"--port\",\n        help=\"TCP/IP port to serve on (default: %(default)r)\",\n        type=int,\n        default=8080,\n    )\n    arg_parser.add_argument(\n        \"-U\",\n        \"--path\",\n        help=\"Unix file system path to serve on. Can be combined with hostname \"\n        \"to serve on both Unix and TCP.\",\n    )\n    args, extra_argv = arg_parser.parse_known_args(argv)\n\n    # Import logic\n    mod_str, _, func_str = args.entry_func.partition(\":\")\n    if not func_str or not mod_str:\n        arg_parser.error(\"'entry-func' not in 'module:function' syntax\")\n    if mod_str.startswith(\".\"):\n        arg_parser.error(\"relative module names not supported\")\n    try:\n        module = import_module(mod_str)\n    except ImportError as ex:\n        arg_parser.error(f\"unable to import {mod_str}: {ex}\")\n    try:\n        func = getattr(module, func_str)\n    except AttributeError:\n        arg_parser.error(f\"module {mod_str!r} has no attribute {func_str!r}\")\n\n    # Compatibility logic\n    if args.path is not None and not hasattr(socket, \"AF_UNIX\"):\n        arg_parser.error(\n            \"file system paths not supported by your operating environment\"\n        )\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    if args.path and args.hostname is None:\n        host = port = None\n    else:\n        host = args.hostname or \"localhost\"\n        port = args.port\n\n    app = func(extra_argv)\n    run_app(app, host=host, port=port, path=args.path)\n    arg_parser.exit(message=\"Stopped\\n\")"
    },
    {
      "chunk_id": 286,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web.py",
      "content": "if __name__ == \"__main__\":  # pragma: no branch\n    main(sys.argv[1:])  # pragma: no cover"
    },
    {
      "chunk_id": 287,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "```python"
    },
    {
      "chunk_id": 288,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "import asyncio\nimport asyncio.streams\nimport sys\nimport traceback\nfrom collections import deque\nfrom contextlib import suppress\nfrom html import escape as html_escape\nfrom http import HTTPStatus\nfrom logging import Logger\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Deque,\n    Generic,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport yarl\n\nfrom .abc import AbstractAccessLogger, AbstractAsyncAccessLogger, AbstractStreamWriter\nfrom .base_protocol import BaseProtocol\nfrom .helpers import ceil_timeout, frozen_dataclass_decorator\nfrom .http import (\n    HttpProcessingError,\n    HttpRequestParser,\n    HttpVersion10,\n    RawRequestMessage,\n    StreamWriter,\n)\nfrom .http_exceptions import BadHttpMethod\nfrom .log import access_logger, server_logger\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .tcp_helpers import tcp_keepalive\nfrom .web_exceptions import HTTPException, HTTPInternalServerError\nfrom .web_log import AccessLogger\nfrom .web_request import BaseRequest\nfrom .web_response import Response, StreamResponse\n\n__all__ = (\"RequestHandler\", \"RequestPayloadError\", \"PayloadAccessError\")"
    },
    {
      "chunk_id": 289,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "class RequestPayloadError(Exception):\n    \"\"\"Payload parsing error.\"\"\""
    },
    {
      "chunk_id": 290,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "class PayloadAccessError(Exception):\n    \"\"\"Payload was accessed after response was sent.\"\"\""
    },
    {
      "chunk_id": 291,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "class AccessLoggerWrapper(AbstractAsyncAccessLogger):\n    \"\"\"Wrap an AbstractAccessLogger so it behaves like an AbstractAsyncAccessLogger.\"\"\"\n\n    __slots__ = (\"access_logger\", \"_loop\")\n\n    def __init__(\n        self, access_logger: AbstractAccessLogger, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self.access_logger = access_logger\n        self._loop = loop\n        super().__init__()\n\n    async def log(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        self.access_logger.log(request, response, self._loop.time() - request_start)\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        return self.access_logger.enabled"
    },
    {
      "chunk_id": 292,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "@frozen_dataclass_decorator\nclass _ErrInfo:\n    status: int\n    exc: BaseException\n    message: str"
    },
    {
      "chunk_id": 293,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_protocol.py",
      "content": "class RequestHandler(BaseProtocol, Generic[_Request]):\n    \"\"\"HTTP protocol implementation.\n\n    RequestHandler handles incoming HTTP request. It reads request line,\n    request headers and request payload and calls handle_request() method.\n    By default it always returns with 404 response.\n\n    RequestHandler handles errors in incoming request, like bad\n    status line, bad headers or incomplete payload. If any error occurs,\n    connection gets closed.\n\n    keepalive_timeout -- number of seconds before closing\n                         keep-alive connection\n\n    tcp_keepalive -- TCP keep-alive is on, default is on\n\n    logger -- custom logger object\n\n    access_log_class -- custom class for access_logger\n\n    access_log -- custom logging object\n\n    access_log_format -- access log format string\n\n    loop -- Optional event loop\n\n    max_line_size -- Optional maximum header line size\n\n    max_field_size -- Optional maximum header field size\n\n    timeout_ceil_threshold -- Optional value to specify\n                              threshold to ceil() timeout\n                              values\n\n    \"\"\"\n\n    __slots__ = (\n        \"_request_count\",\n        \"_keepalive\",\n        \"_manager\",\n        \"_request_handler\",\n        \"_request_factory\",\n        \"_tcp_keepalive\",\n        \"_next_keepalive_close_time\",\n        \"_keepalive_handle\",\n        \"_keepalive_timeout\",\n        \"_lingering_time\",\n        \"_messages\",\n        \"_message_tail\",\n        \"_handler_waiter\",\n        \"_waiter\",\n        \"_task_handler\",\n        \"_upgrade\",\n        \"_payload_parser\",\n        \"_request_parser\",\n        \"logger\",\n        \"access_log\",\n        \"access_logger\",\n        \"_close\",\n        \"_force_close\",\n        \"_current_request\",\n        \"_timeout_ceil_threshold\",\n        \"_request_in_progress\",\n    )\n\n    def __init__(\n        self,\n        manager: \"Server[_Request]\",\n        *,\n        loop: asyncio.AbstractEventLoop,\n        # Default should be high enough that it's likely longer than a reverse proxy.\n        keepalive_timeout: float = 3630,\n        tcp_keepalive: bool = True,\n        logger: Logger = server_logger,\n        access_log_class: _AnyAbstractAccessLogger = AccessLogger,\n        access_log: Optional[Logger] = access_logger,\n        access_log_format: str = AccessLogger.LOG_FORMAT,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        lingering_time: float = 10.0,\n        read_bufsize: int = 2**16,\n        auto_decompress: bool = True,\n        timeout_ceil_threshold: float = 5,\n    ):\n        super().__init__(loop)\n\n        # _request_count is the number of requests processed with the same connection.\n        self._request_count = 0\n        self._keepalive = False\n        self._current_request: Optional[_Request] = None\n        self._manager: Optional[Server[_Request]] = manager\n        self._request_handler: Optional[_RequestHandler[_Request]] = (\n            manager.request_handler\n        )\n        self._request_factory: Optional[_RequestFactory[_Request]] = (\n            manager.request_factory\n        )\n\n        self._tcp_keepalive = tcp_keepalive\n        # placeholder to be replaced on keepalive timeout setup\n        self._next_keepalive_close_time = 0.0\n        self._keepalive_handle: Optional[asyncio.Handle] = None\n        self._keepalive_timeout = keepalive_timeout\n        self._lingering_time = float(lingering_time)\n\n        self._messages: Deque[_MsgType] = deque()\n        self._message_tail = b\"\"\n\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._handler_waiter: Optional[asyncio.Future[None]] = None\n        self._task_handler: Optional[asyncio.Task[None]] = None\n\n        self._upgrade = False\n        self._payload_parser: Any = None\n        self._request_parser: Optional[HttpRequestParser] = HttpRequestParser(\n            self,\n            loop,\n            read_bufsize,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n            payload_exception=RequestPayloadError,\n            auto_decompress=auto_decompress,\n        )\n\n        self._timeout_ceil_threshold: float = 5\n        try:\n            self._timeout_ceil_threshold = float(timeout_ceil_threshold)\n        except (TypeError, ValueError):\n            pass\n\n        self.logger = logger\n        self.access_log = access_log\n        if access_log:\n            if issubclass(access_log_class, AbstractAsyncAccessLogger):\n                self.access_logger: Optional[AbstractAsyncAccessLogger] = (\n                    access_log_class()\n                )\n            else:\n                access_logger = access_log_class(access_log, access_log_format)\n                self.access_logger = AccessLoggerWrapper(\n                    access_logger,\n                    self._loop,\n                )\n        else:\n            self.access_logger = None\n\n        self._close = False\n        self._force_close = False\n        self._request_in_progress = False\n\n    def __repr__(self) -> str:\n        return \"<{} {}>\".format(\n            self.__class__.__name__,\n            \"connected\" if self.transport is not None else \"disconnected\",\n        )\n\n    @property\n    def keepalive_timeout(self) -> float:\n        return self._keepalive_timeout\n\n    async def shutdown(self, timeout: Optional[float] = 15.0) -> None:\n        \"\"\"Do worker process exit preparations.\n\n        We need to clean up everything and stop accepting requests.\n        It is especially important for keep-alive connections.\n        \"\"\"\n        self._force_close = True\n\n        if self._keepalive_handle is not None:\n            self._keepalive_handle.cancel()\n\n        # Wait for graceful handler completion\n        if self._request_in_progress:\n            # The future is only created when we are shutting\n            # down while the handler is still processing a request\n            # to avoid creating a future for every request.\n            self._handler_waiter = self._loop.create_future()\n            try:\n                async with ceil_timeout(timeout):\n                    await self._handler_waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._handler_waiter = None\n                if (\n                    sys.version_info >= (3, 11)\n                    and (task := asyncio.current_task())\n                    and task.cancelling()\n                ):\n                    raise\n        # Then cancel handler and wait\n        try:\n            async with ceil_timeout(timeout):\n                if self._current_request is not None:\n                    self._current_request._cancel(asyncio.CancelledError())\n\n                if self._task_handler is not None and not self._task_handler.done():\n                    await asyncio.shield(self._task_handler)\n        except (asyncio.CancelledError, asyncio.TimeoutError):\n            if (\n                sys.version_info >= (3, 11)\n                and (task := asyncio.current_task())\n                and task.cancelling()\n            ):\n                raise\n\n        # force-close non-idle handler\n        if self._task_handler is not None:\n            self._task_handler.cancel()\n\n        self.force_close()\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        super().connection_made(transport)\n\n        real_transport = cast(asyncio.Transport, transport)\n        if self._tcp_keepalive:\n            tcp_keepalive(real_transport)\n\n        assert self._manager is not None\n        self._manager.connection_made(self, real_transport)\n\n        loop = self._loop\n        if sys.version_info >= (3, 12):\n            task = asyncio.Task(self.start(), loop=loop, eager_start=True)\n        else:\n            task = loop.create_task(self.start())\n        self._task_handler = task\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        if self._manager is None:\n            return\n        self._manager.connection_lost(self, exc)\n\n        # Grab value before setting _manager to None.\n        handler_cancellation = self._manager.handler_cancellation\n\n        self.force_close()\n        super().connection_lost(exc)\n        self._manager = None\n        self._request_factory = None\n        self._request_handler = None\n        self._request_parser = None\n\n        if self._keepalive_handle is not None:\n            self._keepalive_handle.cancel()\n\n        if self._current_request is not None:\n            if exc is None:\n                exc = ConnectionResetError(\"Connection lost\")\n            self._current_request._cancel(exc)\n\n        if handler_cancellation and self._task_handler is not None:\n            self._task_handler.cancel()\n\n        self._task_handler = None\n\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n\n    def set_parser(self, parser: Any) -> None:\n        # Actual type is WebReader\n        assert self._payload_parser is None\n\n        self._payload_parser = parser\n\n        if self._message_tail:\n            self._payload_parser.feed_data(self._message_tail)\n            self._message_tail = b\"\"\n\n    def eof_received(self) -> None:\n        pass\n\n    def data_received(self, data: bytes) -> None:\n        if self._force_close or self._close:\n            return\n        # parse http messages\n        messages: Sequence[_MsgType]\n        if self._payload_parser is None and not self._upgrade:\n            assert self._request_parser is not None\n            try:\n                messages, upgraded, tail = self._request_parser.feed_data(data)\n            except HttpProcessingError as exc:\n                messages = [\n                    (_ErrInfo(status=400, exc=exc, message=exc.message), EMPTY_PAYLOAD)\n                ]\n                upgraded = False\n                tail = b\"\"\n\n            for msg, payload in messages or ():\n                self._request_count += 1\n                self._messages.append((msg, payload))\n\n            waiter = self._waiter\n            if messages and waiter is not None and not waiter.done():\n                # don't set result twice\n                waiter.set_result(None)\n\n            self._upgrade = upgraded\n            if upgraded and tail:\n                self._message_tail = tail\n\n        # no parser, just store\n        elif self._payload_parser is None and self._upgrade and data:\n            self._message_tail += data\n\n        # feed payload\n        elif data:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self.close()\n\n    def keep_alive(self, val: bool) -> None:\n        \"\"\"Set keep-alive connection mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._keepalive = val\n        if self._keepalive_handle:\n            self._keepalive_handle.cancel()\n            self._keepalive_handle = None\n\n    def close(self) -> None:\n        \"\"\"Close connection.\n\n        Stop accepting new pipelining messages and close\n        connection when handlers done processing messages.\n        \"\"\"\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()\n\n    def force_close(self) -> None:\n        \"\"\"Forcefully close connection.\"\"\"\n        self._force_close = True\n        if self._waiter:\n            self._waiter.cancel()\n        if self.transport is not None:\n            self.transport.close()\n            self.transport = None\n\n    async def log_access(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        if self.access_logger is not None and self.access_logger.enabled:\n            await self.access_logger.log(request, response, request_start)\n\n    def log_debug(self, *args: Any, **kw: Any) -> None:\n        if self._loop.get_debug():\n            self.logger.debug(*args, **kw)\n\n    def log_exception(self, *args: Any, **kw: Any) -> None:\n        self.logger.exception(*args, **kw)\n\n    def _process_keepalive(self) -> None:\n        self._keepalive_handle = None\n        if self._force_close or not self._keepalive:\n            return\n\n        loop = self._loop\n        now = loop.time()\n        close_time = self._next_keepalive_close_time\n        if now < close_time:\n            # Keep alive close check fired too early, reschedule\n            self._keepalive_handle = loop.call_at(close_time, self._process_keepalive)\n            return\n\n        # handler in idle state\n        if self._waiter and not self._waiter.done():\n            self.force_close()\n\n    async def _handle_request(\n        self,\n        request: _Request,\n        start_time: float,\n        request_handler: Callable[[_Request], Awaitable[StreamResponse]],\n    ) -> Tuple[StreamResponse, bool]:\n        self._request_in_progress = True\n        try:\n            try:\n                self._current_request = request\n                resp = await request_handler(request)\n            finally:\n                self._current_request = None\n        except HTTPException as exc:\n            resp = Response(\n                status=exc.status, reason=exc.reason, text=exc.text, headers=exc.headers\n            )\n            resp._cookies = exc._cookies\n            resp, reset = await self.finish_response(request, resp, start_time)\n        except asyncio.CancelledError:\n            raise\n        except asyncio.TimeoutError as exc:\n            self.log_debug(\"Request handler timed out.\", exc_info=exc)\n            resp = self.handle_error(request, 504)\n            resp, reset = await self.finish_response(request, resp, start_time)\n        except Exception as exc:\n            resp = self.handle_error(request, 500, exc)\n            resp, reset = await self.finish_response(request, resp, start_time)\n        else:\n            resp, reset = await self.finish_response(request, resp, start_time)\n        finally:\n            self._request_in_progress = False\n            if self._handler_waiter is not None:\n                self._handler_waiter.set_result(None)\n\n        return resp, reset\n\n    async def start(self) -> None:\n        \"\"\"Process incoming request.\n\n        It reads request line, request headers and request payload, then\n        calls handle_request() method. Subclass has to override\n        handle_request(). start() handles various exceptions in request\n        or response handling. Connection is being closed always unless\n        keep_alive(True) specified.\n        \"\"\"\n        loop = self._loop\n        handler = asyncio.current_task(loop)\n        assert handler is not None\n        manager = self._manager\n        assert manager is not None\n        keepalive_timeout = self._keepalive_timeout\n        resp = None\n        assert self._request_factory is not None\n        assert self._request_handler is not None\n\n        while not self._force_close:\n            if not self._messages:\n                try:\n                    # wait for next request\n                    self._waiter = loop.create_future()\n                    await self._waiter\n                finally:\n                    self._waiter = None\n\n            message, payload = self._messages.popleft()\n\n            start = loop.time()\n\n            manager.requests_count += 1\n            writer = StreamWriter(self, loop)\n            if not isinstance(message, _ErrInfo):\n                request_handler = self._request_handler\n            else:\n                # make request_factory work\n                request_handler = self._make_error_handler(message)\n                message = ERROR\n\n            request = self._request_factory(message, payload, self, writer, handler)\n            try:\n                # a new task is used for copy context vars (#3406)\n                coro = self._handle_request(request, start, request_handler)\n                if sys.version_info >= (3, 12):\n                    task = asyncio.Task(coro, loop=loop, eager_start=True)\n                else:\n                    task = loop.create_task(coro)\n                try:\n                    resp, reset = await task  # type: ignore[possibly-undefined]\n                except ConnectionError:\n                    self.log_debug(\"Ignored premature client disconnection\")\n                    break\n\n                # Drop the processed task from asyncio.Task.all_tasks() early\n                del task  # type: ignore[possibly-undefined]\n                # https://github.com/python/mypy/issues/14309\n                if reset:  # type: ignore[possibly-undefined]\n                    self.log_debug(\"Ignored premature client disconnection 2\")\n                    break\n\n                # notify server about keep-alive\n                self._keepalive = bool(resp.keep_alive)\n\n                # check payload\n                if not payload.is_eof():\n                    lingering_time = self._lingering_time\n                    # Could be force closed while awaiting above tasks.\n                    if not self._force_close and lingering_time:  # type: ignore[redundant-expr]\n                        self.log_debug(\n                            \"Start lingering close timer for %s sec.\", lingering_time\n                        )\n\n                        now = loop.time()\n                        end_t = now + lingering_time\n\n                        try:\n                            while not payload.is_eof() and now < end_t:\n                                async with ceil_timeout(end_t - now):\n                                    # read and ignore\n                                    await payload.readany()\n                                now = loop.time()\n                        except (asyncio.CancelledError, asyncio.TimeoutError):\n                            if (\n                                sys.version_info >= (3, 11)\n                                and (t := asyncio.current_task())\n                                and t.cancelling()\n                            ):\n                                raise\n\n                    # if payload still uncompleted\n                    if not payload.is_eof() and not self._force_close:\n                        self.log_debug(\"Uncompleted request.\")\n                        self.close()\n\n                payload.set_exception(_PAYLOAD_ACCESS_ERROR)\n\n            except asyncio.CancelledError:\n                self.log_debug(\"Ignored premature client disconnection\")\n                raise\n            except Exception as exc:\n                self.log_exception(\"Unhandled exception\", exc_info=exc)\n                self.force_close()\n            finally:\n                if self.transport is None and resp is not None:\n                    self.log_debug(\"Ignored premature client disconnection.\")\n                elif not self._force_close:\n                    if self._keepalive and not self._close:\n                        # start keep-alive timer\n                        if keepalive_timeout is not None:\n                            now = loop.time()\n                            close_time = now + keepalive_timeout\n                            self._next_keepalive_close_time = close_time\n                            if self._keepalive_handle is None:\n                                self._keepalive_handle = loop.call_at(\n                                    close_time, self._process_keepalive\n                                )\n                    else:\n                        break\n\n        # remove handler, close transport if no handlers left\n        if not self._force_close:\n            self._task_handler = None\n            if self.transport is not None:\n                self.transport.close()\n\n    async def finish_response(\n        self, request: BaseRequest, resp: StreamResponse, start_time: float\n    ) -> Tuple[StreamResponse, bool]:\n        \"\"\"Prepare the response and write_eof, then log access.\n\n        This has to\n        be called within the context of any exception so the access logger\n        can get exception information. Returns True if the client disconnects\n        prematurely.\n        \"\"\"\n        request._finish()\n        if self._request_parser is not None:\n            self._request_parser.set_upgraded(False)\n            self._upgrade = False\n            if self._message_tail:\n                self._request_parser.feed_data(self._message_tail)\n                self._message_tail = b\"\"\n        try:\n            prepare_meth = resp.prepare\n        except AttributeError:\n            if resp is None:\n                self.log_exception(\"Missing return statement on request handler\")  # type: ignore[unreachable]\n            else:\n                self.log_exception(\n                    \"Web-handler should return a response instance, \"\n                    \"got {!r}\".format(resp)\n                )\n            exc = HTTPInternalServerError()\n            resp = Response(\n                status=exc.status, reason=exc.reason, text=exc.text, headers=exc.headers\n            )\n            prepare_meth = resp.prepare\n        try:\n            await prepare_meth(request)\n            await resp.write_eof()\n        except ConnectionError:\n            await self.log_access(request, resp, start_time)\n            return resp, True\n\n        await self.log_access(request, resp, start_time)\n        return resp, False\n\n    def handle_error(\n        self,\n        request: BaseRequest,\n        status: int = 500,\n        exc: Optional[BaseException] = None,\n        message: Optional[str] = None,\n    ) -> StreamResponse:\n        \"\"\"Handle errors.\n\n        Returns HTTP response with specific status code. Logs additional\n        information. It always closes current connection.\n        \"\"\"\n        if self._request_count == 1 and isinstance(exc, BadHttpMethod):\n            # BadHttpMethod is common when a client sends non-HTTP\n            # or encrypted traffic to an HTTP port. This is expected\n            # to happen when connected to the public internet so we log\n            # it at the debug level as to not fill logs with noise.\n            self.logger.debug(\n                \"Error handling request from %s\", request.remote, exc_info=exc\n            )\n        else:\n            self.log_exception(\n                \"Error handling request from %s\", request.remote, exc_info=exc\n            )\n\n        # some data already got sent, connection is broken\n        if request.writer.output_size > 0:\n            raise ConnectionError(\n                \"Response is sent already, cannot send another response \"\n                \"with the error message\"\n            )\n\n        ct = \"text/plain\"\n        if status == HTTPStatus.INTERNAL_SERVER_ERROR:\n            title = \"{0.value} {0.phrase}\".format(HTTPStatus.INTERNAL_SERVER_ERROR)\n            msg = HTTPStatus.INTERNAL_SERVER_ERROR.description\n            tb = None\n            if self._loop.get_debug():\n                with suppress(Exception):\n                    tb = traceback.format_exc()\n\n            if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n                if tb:\n                    tb = html_escape(tb)\n                    msg = f\"<h2>Traceback:</h2>\\n<pre>{tb}</pre>\"\n                message = (\n                    \"<html><head>\"\n                    \"<title>{title}</title>\"\n                    \"</head><body>\\n<h1>{title}</h1>\"\n                    \"\\n{msg}\\n</body></html>\\n\"\n                ).format(title=title, msg=msg)\n                ct = \"text/html\"\n            else:\n                if tb:\n                    msg = tb\n                message = title + \"\\n\\n\" + msg\n\n        resp = Response(status=status, text=message, content_type=ct)\n        resp.force_close()\n\n        return resp\n\n    def _make_error_handler(\n        self, err_info: _ErrInfo\n    ) -> Callable[[BaseRequest], Awaitable[StreamResponse]]:\n        async def handler(request: BaseRequest) -> StreamResponse:\n            return self.handle_error(\n                request, err_info.status, err_info.exc, err_info.message\n            )\n\n        return handler\n```"
    },
    {
      "chunk_id": 294,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_log.py",
      "content": "import datetime\nimport functools\nimport logging\nimport os\nimport re\nimport time as time_mod\nfrom collections import namedtuple\nfrom typing import Any, Callable, Dict, Iterable, List, Tuple  # noqa\n\nfrom .abc import AbstractAccessLogger\nfrom .web_request import BaseRequest\nfrom .web_response import StreamResponse"
    },
    {
      "chunk_id": 295,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_log.py",
      "content": "KeyMethod = namedtuple(\"KeyMethod\", \"key method\")"
    },
    {
      "chunk_id": 296,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_log.py",
      "content": "class AccessLogger(AbstractAccessLogger):\n    \"\"\"Helper object to log access.\n\n    Usage:\n        log = logging.getLogger(\"spam\")\n        log_format = \"%a %{User-Agent}i\"\n        access_logger = AccessLogger(log, log_format)\n        access_logger.log(request, response, time)\n\n    Format:\n        %%  The percent sign\n        %a  Remote IP-address (IP-address of proxy if using reverse proxy)\n        %t  Time when the request was started to process\n        %P  The process ID of the child that serviced the request\n        %r  First line of request\n        %s  Response status code\n        %b  Size of response in bytes, including HTTP headers\n        %T  Time taken to serve the request, in seconds\n        %Tf Time taken to serve the request, in seconds with floating fraction\n            in .06f format\n        %D  Time taken to serve the request, in microseconds\n        %{FOO}i  request.headers['FOO']\n        %{FOO}o  response.headers['FOO']\n        %{FOO}e  os.environ['FOO']\n\n    \"\"\"\n\n    LOG_FORMAT_MAP = {\n        \"a\": \"remote_address\",\n        \"t\": \"request_start_time\",\n        \"P\": \"process_id\",\n        \"r\": \"first_request_line\",\n        \"s\": \"response_status\",\n        \"b\": \"response_size\",\n        \"T\": \"request_time\",\n        \"Tf\": \"request_time_frac\",\n        \"D\": \"request_time_micro\",\n        \"i\": \"request_header\",\n        \"o\": \"response_header\",\n    }\n\n    LOG_FORMAT = '%a %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\"'\n    FORMAT_RE = re.compile(r\"%(\\{([A-Za-z0-9\\-_]+)\\}([ioe])|[atPrsbOD]|Tf?)\")\n    CLEANUP_RE = re.compile(r\"(%[^s])\")\n    _FORMAT_CACHE: Dict[str, Tuple[str, List[KeyMethod]]] = {}\n\n    def __init__(self, logger: logging.Logger, log_format: str = LOG_FORMAT) -> None:\n        \"\"\"Initialise the logger.\n\n        logger is a logger object to be used for logging.\n        log_format is a string with apache compatible log format description.\n\n        \"\"\"\n        super().__init__(logger, log_format=log_format)\n\n        _compiled_format = AccessLogger._FORMAT_CACHE.get(log_format)\n        if not _compiled_format:\n            _compiled_format = self.compile_format(log_format)\n            AccessLogger._FORMAT_CACHE[log_format] = _compiled_format\n\n        self._log_format, self._methods = _compiled_format\n\n    def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\n        \"\"\"Translate log_format into form usable by modulo formatting\n\n        All known atoms will be replaced with %s\n        Also methods for formatting of those atoms will be added to\n        _methods in appropriate order\n\n        For example we have log_format = \"%a %t\"\n        This format will be translated to \"%s %s\"\n        Also contents of _methods will be\n        [self._format_a, self._format_t]\n        These method will be called and results will be passed\n        to translated string format.\n\n        Each _format_* method receive 'args' which is list of arguments\n        given to self.log\n\n        Exceptions are _format_e, _format_i and _format_o methods which\n        also receive key name (by functools.partial)\n\n        \"\"\"\n        # list of (key, method) tuples, we don't use an OrderedDict as users\n        # can repeat the same key more than once\n        methods = list()\n\n        for atom in self.FORMAT_RE.findall(log_format):\n            if atom[1] == \"\":\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\n                m = getattr(AccessLogger, \"_format_%s\" % atom[0])\n                key_method = KeyMethod(format_key1, m)\n            else:\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\n                m = getattr(AccessLogger, \"_format_%s\" % atom[2])\n                key_method = KeyMethod(format_key2, functools.partial(m, atom[1]))\n\n            methods.append(key_method)\n\n        log_format = self.FORMAT_RE.sub(r\"%s\", log_format)\n        log_format = self.CLEANUP_RE.sub(r\"%\\1\", log_format)\n        return log_format, methods\n\n    @staticmethod\n    def _format_i(\n        key: str, request: BaseRequest, response: StreamResponse, time: float\n    ) -> str:\n        # suboptimal, make istr(key) once\n        return request.headers.get(key, \"-\")\n\n    @staticmethod\n    def _format_o(\n        key: str, request: BaseRequest, response: StreamResponse, time: float\n    ) -> str:\n        # suboptimal, make istr(key) once\n        return response.headers.get(key, \"-\")\n\n    @staticmethod\n    def _format_a(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        ip = request.remote\n        return ip if ip is not None else \"-\"\n\n    @staticmethod\n    def _format_t(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        tz = datetime.timezone(datetime.timedelta(seconds=-time_mod.timezone))\n        now = datetime.datetime.now(tz)\n        start_time = now - datetime.timedelta(seconds=time)\n        return start_time.strftime(\"[%d/%b/%Y:%H:%M:%S %z]\")\n\n    @staticmethod\n    def _format_P(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return \"<%s>\" % os.getpid()\n\n    @staticmethod\n    def _format_r(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return \"{} {} HTTP/{}.{}\".format(\n            request.method,\n            request.path_qs,\n            request.version.major,\n            request.version.minor,\n        )\n\n    @staticmethod\n    def _format_s(request: BaseRequest, response: StreamResponse, time: float) -> int:\n        return response.status\n\n    @staticmethod\n    def _format_b(request: BaseRequest, response: StreamResponse, time: float) -> int:\n        return response.body_length\n\n    @staticmethod\n    def _format_T(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return str(round(time))\n\n    @staticmethod\n    def _format_Tf(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return \"%06f\" % time\n\n    @staticmethod\n    def _format_D(request: BaseRequest, response: StreamResponse, time: float) -> str:\n        return str(round(time * 1000000))\n\n    def _format_line(\n        self, request: BaseRequest, response: StreamResponse, time: float\n    ) -> Iterable[Tuple[str, Callable[[BaseRequest, StreamResponse, float], str]]]:\n        return [(key, method(request, response, time)) for key, method in self._methods]\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        # Avoid formatting the log line if it will not be emitted.\n        return self.logger.isEnabledFor(logging.INFO)\n\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\n        try:\n            fmt_info = self._format_line(request, response, time)\n\n            values = list()\n            extra = dict()\n            for key, value in fmt_info:\n                values.append(value)\n\n                if key.__class__ is str:\n                    extra[key] = value\n                else:\n                    k1, k2 = key  # type: ignore[misc]\n                    dct = extra.get(k1, {})  # type: ignore[var-annotated,has-type]\n                    dct[k2] = value  # type: ignore[index,has-type]\n                    extra[k1] = dct  # type: ignore[has-type,assignment]\n\n            self.logger.info(self._log_format % tuple(values), extra=extra)\n        except Exception:\n            self.logger.exception(\"Error in logging\")"
    },
    {
      "chunk_id": 297,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "import asyncio\nimport zlib\nfrom concurrent.futures import Executor\nfrom typing import Optional, cast\n\ntry:\n    try:\n        import brotlicffi as brotli\n    except ImportError:\n        import brotli\n\n    HAS_BROTLI = True\nexcept ImportError:  # pragma: no cover\n    HAS_BROTLI = False\n\nMAX_SYNC_CHUNK_SIZE = 1024"
    },
    {
      "chunk_id": 298,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "def encoding_to_mode(\n    encoding: Optional[str] = None,\n    suppress_deflate_header: bool = False,\n) -> int:\n    if encoding == \"gzip\":\n        return 16 + zlib.MAX_WBITS\n\n    return -zlib.MAX_WBITS if suppress_deflate_header else zlib.MAX_WBITS"
    },
    {
      "chunk_id": 299,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "class ZlibBaseHandler:\n    def __init__(\n        self,\n        mode: int,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        self._mode = mode\n        self._executor = executor\n        self._max_sync_chunk_size = max_sync_chunk_size"
    },
    {
      "chunk_id": 300,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "class ZLibCompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        level: Optional[int] = None,\n        wbits: Optional[int] = None,\n        strategy: int = zlib.Z_DEFAULT_STRATEGY,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=(\n                encoding_to_mode(encoding, suppress_deflate_header)\n                if wbits is None\n                else wbits\n            ),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        if level is None:\n            self._compressor = zlib.compressobj(wbits=self._mode, strategy=strategy)\n        else:\n            self._compressor = zlib.compressobj(\n                wbits=self._mode, strategy=strategy, level=level\n            )\n        self._compress_lock = asyncio.Lock()\n\n    def compress_sync(self, data: bytes) -> bytes:\n        return self._compressor.compress(data)\n\n    async def compress(self, data: bytes) -> bytes:\n        \"\"\"Compress the data and returned the compressed bytes.\n\n        Note that flush() must be called after the last call to compress()\n\n        If the data size is large than the max_sync_chunk_size, the compression\n        will be done in the executor. Otherwise, the compression will be done\n        in the event loop.\n        \"\"\"\n        async with self._compress_lock:\n            # To ensure the stream is consistent in the event\n            # there are multiple writers, we need to lock\n            # the compressor so that only one writer can\n            # compress at a time.\n            if (\n                self._max_sync_chunk_size is not None\n                and len(data) > self._max_sync_chunk_size\n            ):\n                return await asyncio.get_running_loop().run_in_executor(\n                    self._executor, self._compressor.compress, data\n                )\n            return self.compress_sync(data)\n\n    def flush(self, mode: int = zlib.Z_FINISH) -> bytes:\n        return self._compressor.flush(mode)"
    },
    {
      "chunk_id": 301,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "class ZLibDecompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=encoding_to_mode(encoding, suppress_deflate_header),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        self._decompressor = zlib.decompressobj(wbits=self._mode)\n\n    def decompress_sync(self, data: bytes, max_length: int = 0) -> bytes:\n        return self._decompressor.decompress(data, max_length)\n\n    async def decompress(self, data: bytes, max_length: int = 0) -> bytes:\n        \"\"\"Decompress the data and return the decompressed bytes.\n\n        If the data size is large than the max_sync_chunk_size, the decompression\n        will be done in the executor. Otherwise, the decompression will be done\n        in the event loop.\n        \"\"\"\n        if (\n            self._max_sync_chunk_size is not None\n            and len(data) > self._max_sync_chunk_size\n        ):\n            return await asyncio.get_running_loop().run_in_executor(\n                self._executor, self._decompressor.decompress, data, max_length\n            )\n        return self.decompress_sync(data, max_length)\n\n    def flush(self, length: int = 0) -> bytes:\n        return (\n            self._decompressor.flush(length)\n            if length > 0\n            else self._decompressor.flush()\n        )\n\n    @property\n    def eof(self) -> bool:\n        return self._decompressor.eof\n\n    @property\n    def unconsumed_tail(self) -> bytes:\n        return self._decompressor.unconsumed_tail\n\n    @property\n    def unused_data(self) -> bytes:\n        return self._decompressor.unused_data"
    },
    {
      "chunk_id": 302,
      "source": "__internal__/data_repo/aiohttp/aiohttp/compression_utils.py",
      "content": "class BrotliDecompressor:\n    # Supports both 'brotlipy' and 'Brotli' packages\n    # since they share an import name. The top branches\n    # are for 'brotlipy' and bottom branches for 'Brotli'\n    def __init__(self) -> None:\n        if not HAS_BROTLI:\n            raise RuntimeError(\n                \"The brotli decompression is not available. \"\n                \"Please install `Brotli` module\"\n            )\n        self._obj = brotli.Decompressor()\n\n    def decompress_sync(self, data: bytes) -> bytes:\n        if hasattr(self._obj, \"decompress\"):\n            return cast(bytes, self._obj.decompress(data))\n        return cast(bytes, self._obj.process(data))\n\n    def flush(self) -> bytes:\n        if hasattr(self._obj, \"flush\"):\n            return cast(bytes, self._obj.flush())\n        return b\"\""
    },
    {
      "chunk_id": 303,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "```python"
    },
    {
      "chunk_id": 304,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "import asyncio\nimport io\nimport os\nimport pathlib\nimport sys\nfrom contextlib import suppress\nfrom enum import Enum, auto\nfrom mimetypes import MimeTypes\nfrom stat import S_ISREG\nfrom types import MappingProxyType\nfrom typing import (\n    IO,\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Final,\n    Optional,\n    Set,\n    Tuple,\n)\n\nfrom . import hdrs\nfrom .abc import AbstractStreamWriter\nfrom .helpers import ETAG_ANY, ETag, must_be_empty_body\nfrom .typedefs import LooseHeaders, PathLike\nfrom .web_exceptions import (\n    HTTPForbidden,\n    HTTPNotFound,\n    HTTPNotModified,\n    HTTPPartialContent,\n    HTTPPreconditionFailed,\n    HTTPRequestRangeNotSatisfiable,\n)\nfrom .web_response import StreamResponse"
    },
    {
      "chunk_id": 305,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\n\nNOSENDFILE: Final[bool] = bool(os.environ.get(\"AIOHTTP_NOSENDFILE\"))\n\nCONTENT_TYPES: Final[MimeTypes] = MimeTypes()\n\n# File extension to IANA encodings map that will be checked in the order defined.\nENCODING_EXTENSIONS = MappingProxyType(\n    {ext: CONTENT_TYPES.encodings_map[ext] for ext in (\".br\", \".gz\")}\n)\n\nFALLBACK_CONTENT_TYPE = \"application/octet-stream\"\n\n# Provide additional MIME type/extension pairs to be recognized.\n# https://en.wikipedia.org/wiki/List_of_archive_formats#Compression_only\nADDITIONAL_CONTENT_TYPES = MappingProxyType(\n    {\n        \"application/gzip\": \".gz\",\n        \"application/x-brotli\": \".br\",\n        \"application/x-bzip2\": \".bz2\",\n        \"application/x-compress\": \".Z\",\n        \"application/x-xz\": \".xz\",\n    }\n)"
    },
    {
      "chunk_id": 306,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "class _FileResponseResult(Enum):\n    \"\"\"The result of the file response.\"\"\"\n\n    SEND_FILE = auto()  # Ie a regular file to send\n    NOT_ACCEPTABLE = auto()  # Ie a socket, or non-regular file\n    PRE_CONDITION_FAILED = auto()  # Ie If-Match or If-None-Match failed\n    NOT_MODIFIED = auto()  # 304 Not Modified"
    },
    {
      "chunk_id": 307,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "# Add custom pairs and clear the encodings map so guess_type ignores them.\nCONTENT_TYPES.encodings_map.clear()\nfor content_type, extension in ADDITIONAL_CONTENT_TYPES.items():\n    CONTENT_TYPES.add_type(content_type, extension)\n\n_CLOSE_FUTURES: Set[asyncio.Future[None]] = set()"
    },
    {
      "chunk_id": 308,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_fileresponse.py",
      "content": "class FileResponse(StreamResponse):\n    \"\"\"A response object can be used to send files.\"\"\"\n\n    def __init__(\n        self,\n        path: PathLike,\n        chunk_size: int = 256 * 1024,\n        status: int = 200,\n        reason: Optional[str] = None,\n        headers: Optional[LooseHeaders] = None,\n    ) -> None:\n        super().__init__(status=status, reason=reason, headers=headers)\n\n        self._path = pathlib.Path(path)\n        self._chunk_size = chunk_size\n\n    def _seek_and_read(self, fobj: IO[Any], offset: int, chunk_size: int) -> bytes:\n        fobj.seek(offset)\n        return fobj.read(chunk_size)  # type: ignore[no-any-return]\n\n    async def _sendfile_fallback(\n        self, writer: AbstractStreamWriter, fobj: IO[Any], offset: int, count: int\n    ) -> AbstractStreamWriter:\n        # To keep memory usage low,fobj is transferred in chunks\n        # controlled by the constructor's chunk_size argument.\n\n        chunk_size = self._chunk_size\n        loop = asyncio.get_event_loop()\n        chunk = await loop.run_in_executor(\n            None, self._seek_and_read, fobj, offset, chunk_size\n        )\n        while chunk:\n            await writer.write(chunk)\n            count = count - chunk_size\n            if count <= 0:\n                break\n            chunk = await loop.run_in_executor(None, fobj.read, min(chunk_size, count))\n\n        await writer.drain()\n        return writer\n\n    async def _sendfile(\n        self, request: \"BaseRequest\", fobj: IO[Any], offset: int, count: int\n    ) -> AbstractStreamWriter:\n        writer = await super().prepare(request)\n        assert writer is not None\n\n        if NOSENDFILE or self.compression:\n            return await self._sendfile_fallback(writer, fobj, offset, count)\n\n        loop = request._loop\n        transport = request.transport\n        assert transport is not None\n\n        try:\n            await loop.sendfile(transport, fobj, offset, count)\n        except NotImplementedError:\n            return await self._sendfile_fallback(writer, fobj, offset, count)\n\n        await super().write_eof()\n        return writer\n\n    @staticmethod\n    def _etag_match(etag_value: str, etags: Tuple[ETag, ...], *, weak: bool) -> bool:\n        if len(etags) == 1 and etags[0].value == ETAG_ANY:\n            return True\n        return any(\n            etag.value == etag_value for etag in etags if weak or not etag.is_weak\n        )\n\n    async def _not_modified(\n        self, request: \"BaseRequest\", etag_value: str, last_modified: float\n    ) -> Optional[AbstractStreamWriter]:\n        self.set_status(HTTPNotModified.status_code)\n        self._length_check = False\n        self.etag = etag_value  # type: ignore[assignment]\n        self.last_modified = last_modified  # type: ignore[assignment]\n        # Delete any Content-Length headers provided by user. HTTP 304\n        # should always have empty response body\n        return await super().prepare(request)\n\n    async def _precondition_failed(\n        self, request: \"BaseRequest\"\n    ) -> Optional[AbstractStreamWriter]:\n        self.set_status(HTTPPreconditionFailed.status_code)\n        self.content_length = 0\n        return await super().prepare(request)\n\n    def _make_response(\n        self, request: \"BaseRequest\", accept_encoding: str\n    ) -> Tuple[\n        _FileResponseResult, Optional[io.BufferedReader], os.stat_result, Optional[str]\n    ]:\n        \"\"\"Return the response result, io object, stat result, and encoding.\n\n        If an uncompressed file is returned, the encoding is set to\n        :py:data:`None`.\n\n        This method should be called from a thread executor\n        since it calls os.stat which may block.\n        \"\"\"\n        file_path, st, file_encoding = self._get_file_path_stat_encoding(\n            accept_encoding\n        )\n        if not file_path:\n            return _FileResponseResult.NOT_ACCEPTABLE, None, st, None\n\n        etag_value = f\"{st.st_mtime_ns:x}-{st.st_size:x}\"\n\n        # https://www.rfc-editor.org/rfc/rfc9110#section-13.1.1-2\n        if (ifmatch := request.if_match) is not None and not self._etag_match(\n            etag_value, ifmatch, weak=False\n        ):\n            return _FileResponseResult.PRE_CONDITION_FAILED, None, st, file_encoding\n\n        if (\n            (unmodsince := request.if_unmodified_since) is not None\n            and ifmatch is None\n            and st.st_mtime > unmodsince.timestamp()\n        ):\n            return _FileResponseResult.PRE_CONDITION_FAILED, None, st, file_encoding\n\n        # https://www.rfc-editor.org/rfc/rfc9110#section-13.1.2-2\n        if (ifnonematch := request.if_none_match) is not None and self._etag_match(\n            etag_value, ifnonematch, weak=True\n        ):\n            return _FileResponseResult.NOT_MODIFIED, None, st, file_encoding\n\n        if (\n            (modsince := request.if_modified_since) is not None\n            and ifnonematch is None\n            and st.st_mtime <= modsince.timestamp()\n        ):\n            return _FileResponseResult.NOT_MODIFIED, None, st, file_encoding\n\n        fobj = file_path.open(\"rb\")\n        with suppress(OSError):\n            # fstat() may not be available on all platforms\n            # Once we open the file, we want the fstat() to ensure\n            # the file has not changed between the first stat()\n            # and the open().\n            st = os.stat(fobj.fileno())\n        return _FileResponseResult.SEND_FILE, fobj, st, file_encoding\n\n    def _get_file_path_stat_encoding(\n        self, accept_encoding: str\n    ) -> Tuple[Optional[pathlib.Path], os.stat_result, Optional[str]]:\n        file_path = self._path\n        for file_extension, file_encoding in ENCODING_EXTENSIONS.items():\n            if file_encoding not in accept_encoding:\n                continue\n\n            compressed_path = file_path.with_suffix(file_path.suffix + file_extension)\n            with suppress(OSError):\n                # Do not follow symlinks and ignore any non-regular files.\n                st = compressed_path.lstat()\n                if S_ISREG(st.st_mode):\n                    return compressed_path, st, file_encoding\n\n        # Fallback to the uncompressed file\n        st = file_path.stat()\n        return file_path if S_ISREG(st.st_mode) else None, st, None\n\n    async def prepare(self, request: \"BaseRequest\") -> Optional[AbstractStreamWriter]:\n        loop = asyncio.get_running_loop()\n        # Encoding comparisons should be case-insensitive\n        # https://www.rfc-editor.org/rfc/rfc9110#section-8.4.1\n        accept_encoding = request.headers.get(hdrs.ACCEPT_ENCODING, \"\").lower()\n        try:\n            response_result, fobj, st, file_encoding = await loop.run_in_executor(\n                None, self._make_response, request, accept_encoding\n            )\n        except PermissionError:\n            self.set_status(HTTPForbidden.status_code)\n            return await super().prepare(request)\n        except OSError:\n            # Most likely to be FileNotFoundError or OSError for circular\n            # symlinks in python >= 3.13, so respond with 404.\n            self.set_status(HTTPNotFound.status_code)\n            return await super().prepare(request)\n\n        # Forbid special files like sockets, pipes, devices, etc.\n        if response_result is _FileResponseResult.NOT_ACCEPTABLE:\n            self.set_status(HTTPForbidden.status_code)\n            return await super().prepare(request)\n\n        if response_result is _FileResponseResult.PRE_CONDITION_FAILED:\n            return await self._precondition_failed(request)\n\n        if response_result is _FileResponseResult.NOT_MODIFIED:\n            etag_value = f\"{st.st_mtime_ns:x}-{st.st_size:x}\"\n            last_modified = st.st_mtime\n            return await self._not_modified(request, etag_value, last_modified)\n\n        assert fobj is not None\n        try:\n            return await self._prepare_open_file(request, fobj, st, file_encoding)\n        finally:\n            # We do not await here because we do not want to wait\n            # for the executor to finish before returning the response\n            # so the connection can begin servicing another request\n            # as soon as possible.\n            close_future = loop.run_in_executor(None, fobj.close)\n            # Hold a strong reference to the future to prevent it from being\n            # garbage collected before it completes.\n            _CLOSE_FUTURES.add(close_future)\n            close_future.add_done_callback(_CLOSE_FUTURES.remove)\n\n    async def _prepare_open_file(\n        self,\n        request: \"BaseRequest\",\n        fobj: io.BufferedReader,\n        st: os.stat_result,\n        file_encoding: Optional[str],\n    ) -> Optional[AbstractStreamWriter]:\n        status = self._status\n        file_size: int = st.st_size\n        file_mtime: float = st.st_mtime\n        count: int = file_size\n        start: Optional[int] = None\n\n        if (ifrange := request.if_range) is None or file_mtime <= ifrange.timestamp():\n            # If-Range header check:\n            # condition = cached date >= last modification date\n            # return 206 if True else 200.\n            # if False:\n            #   Range header would not be processed, return 200\n            # if True but Range header missing\n            #   return 200\n            try:\n                rng = request.http_range\n                start = rng.start\n                end: Optional[int] = rng.stop\n            except ValueError:\n                # https://tools.ietf.org/html/rfc7233:\n                # A server generating a 416 (Range Not Satisfiable) response to\n                # a byte-range request SHOULD send a Content-Range header field\n                # with an unsatisfied-range value.\n                # The complete-length in a 416 response indicates the current\n                # length of the selected representation.\n                #\n                # Will do the same below. Many servers ignore this and do not\n                # send a Content-Range header with HTTP 416\n                self._headers[hdrs.CONTENT_RANGE] = f\"bytes */{file_size}\"\n                self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\n                return await super().prepare(request)\n\n            # If a range request has been made, convert start, end slice\n            # notation into file pointer offset and count\n            if start is not None:\n                if start < 0 and end is None:  # return tail of file\n                    start += file_size\n                    if start < 0:\n                        # if Range:bytes=-1000 in request header but file size\n                        # is only 200, there would be trouble without this\n                        start = 0\n                else:\n                    # rfc7233:If the last-byte-pos value is\n                    # absent, or if the value is greater than or equal to\n                    # the current length of the representation data,\n                    # the byte range is interpreted as the remainder\n                    # of the representation (i.e., the server replaces the\n                    # value of last-byte-pos with a value that is one less than\n                    # the current length of the selected representation).\n                    count = (\n                        min(end if end is not None else file_size, file_size) - start\n                    )\n\n                if start >= file_size:\n                    # HTTP 416 should be returned in this case.\n                    #\n                    # According to https://tools.ietf.org/html/rfc7233:\n                    # If a valid byte-range-set includes at least one\n                    # byte-range-spec with a first-byte-pos that is less than\n                    # the current length of the representation, or at least one\n                    # suffix-byte-range-spec with a non-zero suffix-length,\n                    # then the byte-range-set is satisfiable. Otherwise, the\n                    # byte-range-set is unsatisfiable.\n                    self._headers[hdrs.CONTENT_RANGE] = f\"bytes */{file_size}\"\n                    self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\n                    return await super().prepare(request)\n\n                status = HTTPPartialContent.status_code\n                # Even though you are sending the whole file, you should still\n                # return a HTTP 206 for a Range request.\n                self.set_status(status)\n\n        # If the Content-Type header is not already set, guess it based on the\n        # extension of the request path. The encoding returned by guess_type\n        #  can be ignored since the map was cleared above.\n        if hdrs.CONTENT_TYPE not in self._headers:\n            if sys.version_info >= (3, 13):\n                guesser = CONTENT_TYPES.guess_file_type\n            else:\n                guesser = CONTENT_TYPES.guess_type\n            self.content_type = guesser(self._path)[0] or FALLBACK_CONTENT_TYPE\n\n        if file_encoding:\n            self._headers[hdrs.CONTENT_ENCODING] = file_encoding\n            self._headers[hdrs.VARY] = hdrs.ACCEPT_ENCODING\n            # Disable compression if we are already sending\n            # a compressed file since we don't want to double\n            # compress.\n            self._compression = False\n\n        self.etag = f\"{st.st_mtime_ns:x}-{st.st_size:x}\"  # type: ignore[assignment]\n        self.last_modified = file_mtime  # type: ignore[assignment]\n        self.content_length = count\n\n        self._headers[hdrs.ACCEPT_RANGES] = \"bytes\"\n\n        if status == HTTPPartialContent.status_code:\n            real_start = start\n            assert real_start is not None\n            self._headers[hdrs.CONTENT_RANGE] = \"bytes {}-{}/{}\".format(\n                real_start, real_start + count - 1, file_size\n            )\n\n        # If we are sending 0 bytes calling sendfile() will throw a ValueError\n        if count == 0 or must_be_empty_body(request.method, status):\n            return await super().prepare(request)\n\n        # be aware that start could be None or int=0 here.\n        offset = start or 0\n\n        return await self._sendfile(request, fobj, offset, count)\n```"
    },
    {
      "chunk_id": 309,
      "source": "__internal__/data_repo/aiohttp/aiohttp/typedefs.py",
      "content": "import json\nimport os\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Iterable,\n    Mapping,\n    Protocol,\n    Tuple,\n    Union,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy, istr\nfrom yarl import URL, Query as _Query\n\nQuery = _Query\n\nDEFAULT_JSON_ENCODER = json.dumps\nDEFAULT_JSON_DECODER = json.loads\n\nif TYPE_CHECKING:\n    _CIMultiDict = CIMultiDict[str]\n    _CIMultiDictProxy = CIMultiDictProxy[str]\n    _MultiDict = MultiDict[str]\n    _MultiDictProxy = MultiDictProxy[str]\n    from http.cookies import BaseCookie, Morsel\n\n    from .web import Request, StreamResponse\nelse:\n    _CIMultiDict = CIMultiDict\n    _CIMultiDictProxy = CIMultiDictProxy\n    _MultiDict = MultiDict\n    _MultiDictProxy = MultiDictProxy\n\nByteish = Union[bytes, bytearray, memoryview]\nJSONEncoder = Callable[[Any], str]\nJSONDecoder = Callable[[str], Any]\nLooseHeaders = Union[\n    Mapping[str, str],\n    Mapping[istr, str],\n    _CIMultiDict,\n    _CIMultiDictProxy,\n    Iterable[Tuple[Union[str, istr], str]],\n]\nRawHeaders = Tuple[Tuple[bytes, bytes], ...]\nStrOrURL = Union[str, URL]\n\nLooseCookiesMappings = Mapping[str, Union[str, \"BaseCookie[str]\", \"Morsel[Any]\"]]\nLooseCookiesIterables = Iterable[\n    Tuple[str, Union[str, \"BaseCookie[str]\", \"Morsel[Any]\"]]\n]\nLooseCookies = Union[\n    LooseCookiesMappings,\n    LooseCookiesIterables,\n    \"BaseCookie[str]\",\n]\n\nHandler = Callable[[\"Request\"], Awaitable[\"StreamResponse\"]]"
    },
    {
      "chunk_id": 310,
      "source": "__internal__/data_repo/aiohttp/aiohttp/typedefs.py",
      "content": "class Middleware(Protocol):\n    def __call__(\n        self, request: \"Request\", handler: Handler\n    ) -> Awaitable[\"StreamResponse\"]: ..."
    },
    {
      "chunk_id": 311,
      "source": "__internal__/data_repo/aiohttp/aiohttp/typedefs.py",
      "content": "PathLike = Union[str, \"os.PathLike[str]\"]"
    },
    {
      "chunk_id": 312,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "```python"
    },
    {
      "chunk_id": 313,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "import asyncio\nimport logging\nimport warnings\nfrom functools import lru_cache, partial, update_wrapper\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    final,\n    overload,\n)\n\nfrom aiosignal import Signal\nfrom frozenlist import FrozenList\n\nfrom . import hdrs\nfrom .helpers import AppKey\nfrom .log import web_logger\nfrom .typedefs import Handler, Middleware\nfrom .web_exceptions import NotAppKeyWarning\nfrom .web_middlewares import _fix_request_current_app\nfrom .web_request import Request\nfrom .web_response import StreamResponse\nfrom .web_routedef import AbstractRouteDef\nfrom .web_urldispatcher import (\n    AbstractResource,\n    AbstractRoute,\n    Domain,\n    MaskDomain,\n    MatchedSubAppResource,\n    PrefixedSubAppResource,\n    SystemRoute,\n    UrlDispatcher,\n)\n\n__all__ = (\"Application\", \"CleanupError\")"
    },
    {
      "chunk_id": 314,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "def _build_middlewares(\n    handler: Handler, apps: Tuple[\"Application\", ...]\n) -> Callable[[Request], Awaitable[StreamResponse]]:\n    \"\"\"Apply middlewares to handler.\"\"\"\n    # The slice is to reverse the order of the apps\n    # so they are applied in the order they were added\n    for app in apps[::-1]:\n        assert app.pre_frozen, \"middleware handlers are not ready\"\n        for m in app._middlewares_handlers:\n            handler = update_wrapper(partial(m, handler=handler), handler)\n    return handler\n\n\n_cached_build_middleware = lru_cache(maxsize=1024)(_build_middlewares)"
    },
    {
      "chunk_id": 315,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "@final\nclass Application(MutableMapping[Union[str, AppKey[Any]], Any]):\n    __slots__ = (\n        \"logger\",\n        \"_router\",\n        \"_loop\",\n        \"_handler_args\",\n        \"_middlewares\",\n        \"_middlewares_handlers\",\n        \"_run_middlewares\",\n        \"_state\",\n        \"_frozen\",\n        \"_pre_frozen\",\n        \"_subapps\",\n        \"_on_response_prepare\",\n        \"_on_startup\",\n        \"_on_shutdown\",\n        \"_on_cleanup\",\n        \"_client_max_size\",\n        \"_cleanup_ctx\",\n    )\n\n    def __init__(\n        self,\n        *,\n        logger: logging.Logger = web_logger,\n        middlewares: Iterable[Middleware] = (),\n        handler_args: Optional[Mapping[str, Any]] = None,\n        client_max_size: int = 1024**2,\n        debug: Any = ...,  # mypy doesn't support ellipsis\n    ) -> None:\n        if debug is not ...:\n            warnings.warn(\n                \"debug argument is no-op since 4.0 and scheduled for removal in 5.0\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self._router = UrlDispatcher()\n        self._handler_args = handler_args\n        self.logger = logger\n\n        self._middlewares: _Middlewares = FrozenList(middlewares)\n\n        # initialized on freezing\n        self._middlewares_handlers: _MiddlewaresHandlers = tuple()\n        # initialized on freezing\n        self._run_middlewares: Optional[bool] = None\n\n        self._state: Dict[Union[AppKey[Any], str], object] = {}\n        self._frozen = False\n        self._pre_frozen = False\n        self._subapps: _Subapps = []\n\n        self._on_response_prepare: _RespPrepareSignal = Signal(self)\n        self._on_startup: _AppSignal = Signal(self)\n        self._on_shutdown: _AppSignal = Signal(self)\n        self._on_cleanup: _AppSignal = Signal(self)\n        self._cleanup_ctx = CleanupContext()\n        self._on_startup.append(self._cleanup_ctx._on_startup)\n        self._on_cleanup.append(self._cleanup_ctx._on_cleanup)\n        self._client_max_size = client_max_size\n\n    def __init_subclass__(cls: Type[\"Application\"]) -> None:\n        raise TypeError(\n            \"Inheritance class {} from web.Application \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    # MutableMapping API\n\n    def __eq__(self, other: object) -> bool:\n        return self is other\n\n    @overload  # type: ignore[override]\n    def __getitem__(self, key: AppKey[_T]) -> _T: ...\n\n    @overload\n    def __getitem__(self, key: str) -> Any: ...\n\n    def __getitem__(self, key: Union[str, AppKey[_T]]) -> Any:\n        return self._state[key]\n\n    def _check_frozen(self) -> None:\n        if self._frozen:\n            raise RuntimeError(\n                \"Changing state of started or joined application is forbidden\"\n            )\n\n    @overload  # type: ignore[override]\n    def __setitem__(self, key: AppKey[_T], value: _T) -> None: ...\n\n    @overload\n    def __setitem__(self, key: str, value: Any) -> None: ...\n\n    def __setitem__(self, key: Union[str, AppKey[_T]], value: Any) -> None:\n        self._check_frozen()\n        if not isinstance(key, AppKey):\n            warnings.warn(\n                \"It is recommended to use web.AppKey instances for keys.\\n\"\n                + \"https://docs.aiohttp.org/en/stable/web_advanced.html\"\n                + \"#application-s-config\",\n                category=NotAppKeyWarning,\n                stacklevel=2,\n            )\n        self._state[key] = value\n\n    def __delitem__(self, key: Union[str, AppKey[_T]]) -> None:\n        self._check_frozen()\n        del self._state[key]\n\n    def __len__(self) -> int:\n        return len(self._state)\n\n    def __iter__(self) -> Iterator[Union[str, AppKey[Any]]]:\n        return iter(self._state)\n\n    def __hash__(self) -> int:\n        return id(self)\n\n    @overload  # type: ignore[override]\n    def get(self, key: AppKey[_T], default: None = ...) -> Optional[_T]: ...\n\n    @overload\n    def get(self, key: AppKey[_T], default: _U) -> Union[_T, _U]: ...\n\n    @overload\n    def get(self, key: str, default: Any = ...) -> Any: ...\n\n    def get(self, key: Union[str, AppKey[_T]], default: Any = None) -> Any:\n        return self._state.get(key, default)\n\n    ########\n    def _set_loop(self, loop: Optional[asyncio.AbstractEventLoop]) -> None:\n        warnings.warn(\n            \"_set_loop() is no-op since 4.0 and scheduled for removal in 5.0\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    @property\n    def pre_frozen(self) -> bool:\n        return self._pre_frozen\n\n    def pre_freeze(self) -> None:\n        if self._pre_frozen:\n            return\n\n        self._pre_frozen = True\n        self._middlewares.freeze()\n        self._router.freeze()\n        self._on_response_prepare.freeze()\n        self._cleanup_ctx.freeze()\n        self._on_startup.freeze()\n        self._on_shutdown.freeze()\n        self._on_cleanup.freeze()\n        self._middlewares_handlers = tuple(self._prepare_middleware())\n\n        # If current app and any subapp do not have middlewares avoid run all\n        # of the code footprint that it implies, which have a middleware\n        # hardcoded per app that sets up the current_app attribute. If no\n        # middlewares are configured the handler will receive the proper\n        # current_app without needing all of this code.\n        self._run_middlewares = True if self.middlewares else False\n\n        for subapp in self._subapps:\n            subapp.pre_freeze()\n            self._run_middlewares = self._run_middlewares or subapp._run_middlewares\n\n    @property\n    def frozen(self) -> bool:\n        return self._frozen\n\n    def freeze(self) -> None:\n        if self._frozen:\n            return\n\n        self.pre_freeze()\n        self._frozen = True\n        for subapp in self._subapps:\n            subapp.freeze()\n\n    @property\n    def debug(self) -> bool:\n        warnings.warn(\n            \"debug property is deprecated since 4.0 and scheduled for removal in 5.0\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return asyncio.get_event_loop().get_debug()\n\n    def _reg_subapp_signals(self, subapp: \"Application\") -> None:\n        def reg_handler(signame: str) -> None:\n            subsig = getattr(subapp, signame)\n\n            async def handler(app: \"Application\") -> None:\n                await subsig.send(subapp)\n\n            appsig = getattr(self, signame)\n            appsig.append(handler)\n\n        reg_handler(\"on_startup\")\n        reg_handler(\"on_shutdown\")\n        reg_handler(\"on_cleanup\")\n\n    def add_subapp(self, prefix: str, subapp: \"Application\") -> PrefixedSubAppResource:\n        if not isinstance(prefix, str):\n            raise TypeError(\"Prefix must be str\")\n        prefix = prefix.rstrip(\"/\")\n        if not prefix:\n            raise ValueError(\"Prefix cannot be empty\")\n        factory = partial(PrefixedSubAppResource, prefix, subapp)\n        return self._add_subapp(factory, subapp)\n\n    def _add_subapp(\n        self, resource_factory: Callable[[], _Resource], subapp: \"Application\"\n    ) -> _Resource:\n        if self.frozen:\n            raise RuntimeError(\"Cannot add sub application to frozen application\")\n        if subapp.frozen:\n            raise RuntimeError(\"Cannot add frozen application\")\n        resource = resource_factory()\n        self.router.register_resource(resource)\n        self._reg_subapp_signals(subapp)\n        self._subapps.append(subapp)\n        subapp.pre_freeze()\n        return resource\n\n    def add_domain(self, domain: str, subapp: \"Application\") -> MatchedSubAppResource:\n        if not isinstance(domain, str):\n            raise TypeError(\"Domain must be str\")\n        elif \"*\" in domain:\n            rule: Domain = MaskDomain(domain)\n        else:\n            rule = Domain(domain)\n        factory = partial(MatchedSubAppResource, rule, subapp)\n        return self._add_subapp(factory, subapp)\n\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\n        return self.router.add_routes(routes)\n\n    @property\n    def on_response_prepare(self) -> _RespPrepareSignal:\n        return self._on_response_prepare\n\n    @property\n    def on_startup(self) -> _AppSignal:\n        return self._on_startup\n\n    @property\n    def on_shutdown(self) -> _AppSignal:\n        return self._on_shutdown\n\n    @property\n    def on_cleanup(self) -> _AppSignal:\n        return self._on_cleanup\n\n    @property\n    def cleanup_ctx(self) -> \"CleanupContext\":\n        return self._cleanup_ctx\n\n    @property\n    def router(self) -> UrlDispatcher:\n        return self._router\n\n    @property\n    def middlewares(self) -> _Middlewares:\n        return self._middlewares\n\n    async def startup(self) -> None:\n        \"\"\"Causes on_startup signal\n\n        Should be called in the event loop along with the request handler.\n        \"\"\"\n        await self.on_startup.send(self)\n\n    async def shutdown(self) -> None:\n        \"\"\"Causes on_shutdown signal\n\n        Should be called before cleanup()\n        \"\"\"\n        await self.on_shutdown.send(self)\n\n    async def cleanup(self) -> None:\n        \"\"\"Causes on_cleanup signal\n\n        Should be called after shutdown()\n        \"\"\"\n        if self.on_cleanup.frozen:\n            await self.on_cleanup.send(self)\n        else:\n            # If an exception occurs in startup, ensure cleanup contexts are completed.\n            await self._cleanup_ctx._on_cleanup(self)\n\n    def _prepare_middleware(self) -> Iterator[Middleware]:\n        yield from reversed(self._middlewares)\n        yield _fix_request_current_app(self)\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        match_info = await self._router.resolve(request)\n        match_info.add_app(self)\n        match_info.freeze()\n\n        request._match_info = match_info\n\n        if request.headers.get(hdrs.EXPECT):\n            resp = await match_info.expect_handler(request)\n            await request.writer.drain()\n            if resp is not None:\n                return resp\n\n        handler = match_info.handler\n\n        if self._run_middlewares:\n            # If its a SystemRoute, don't cache building the middlewares since\n            # they are constructed for every MatchInfoError as a new handler\n            # is made each time.\n            if isinstance(match_info.route, SystemRoute):\n                handler = _build_middlewares(handler, match_info.apps)\n            else:\n                handler = _cached_build_middleware(handler, match_info.apps)\n\n        return await handler(request)\n\n    def __call__(self) -> \"Application\":\n        \"\"\"gunicorn compatibility\"\"\"\n        return self\n\n    def __repr__(self) -> str:\n        return f\"<Application 0x{id(self):x}>\"\n\n    def __bool__(self) -> bool:\n        return True"
    },
    {
      "chunk_id": 316,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "class CleanupError(RuntimeError):\n    @property\n    def exceptions(self) -> List[BaseException]:\n        return cast(List[BaseException], self.args[1])"
    },
    {
      "chunk_id": 317,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "if TYPE_CHECKING:\n    _CleanupContextBase = FrozenList[Callable[[Application], AsyncIterator[None]]]\nelse:\n    _CleanupContextBase = FrozenList"
    },
    {
      "chunk_id": 318,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_app.py",
      "content": "class CleanupContext(_CleanupContextBase):\n    def __init__(self) -> None:\n        super().__init__()\n        self._exits: List[AsyncIterator[None]] = []\n\n    async def _on_startup(self, app: Application) -> None:\n        for cb in self:\n            it = cb(app).__aiter__()\n            await it.__anext__()\n            self._exits.append(it)\n\n    async def _on_cleanup(self, app: Application) -> None:\n        errors = []\n        for it in reversed(self._exits):\n            try:\n                await it.__anext__()\n            except StopAsyncIteration:\n                pass\n            except (Exception, asyncio.CancelledError) as exc:\n                errors.append(exc)\n            else:\n                errors.append(RuntimeError(f\"{it!r} has more than one 'yield'\"))\n        if errors:\n            if len(errors) == 1:\n                raise errors[0]\n            else:\n                raise CleanupError(\"Multiple errors on cleanup stage\", errors)\n```"
    },
    {
      "chunk_id": 319,
      "source": "__internal__/data_repo/aiohttp/aiohttp/base_protocol.py",
      "content": "import asyncio\nfrom typing import Optional, cast\n\nfrom .client_exceptions import ClientConnectionResetError\nfrom .helpers import set_exception\nfrom .tcp_helpers import tcp_nodelay"
    },
    {
      "chunk_id": 320,
      "source": "__internal__/data_repo/aiohttp/aiohttp/base_protocol.py",
      "content": "class BaseProtocol(asyncio.Protocol):\n    __slots__ = (\n        \"_loop\",\n        \"_paused\",\n        \"_drain_waiter\",\n        \"_connection_lost\",\n        \"_reading_paused\",\n        \"transport\",\n    )\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop: asyncio.AbstractEventLoop = loop\n        self._paused = False\n        self._drain_waiter: Optional[asyncio.Future[None]] = None\n        self._reading_paused = False\n\n        self.transport: Optional[asyncio.Transport] = None\n\n    @property\n    def connected(self) -> bool:\n        \"\"\"Return True if the connection is open.\"\"\"\n        return self.transport is not None\n\n    @property\n    def writing_paused(self) -> bool:\n        return self._paused\n\n    def pause_writing(self) -> None:\n        assert not self._paused\n        self._paused = True\n\n    def resume_writing(self) -> None:\n        assert self._paused\n        self._paused = False\n\n        waiter = self._drain_waiter\n        if waiter is not None:\n            self._drain_waiter = None\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def pause_reading(self) -> None:\n        if not self._reading_paused and self.transport is not None:\n            try:\n                self.transport.pause_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = True\n\n    def resume_reading(self) -> None:\n        if self._reading_paused and self.transport is not None:\n            try:\n                self.transport.resume_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = False\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        tr = cast(asyncio.Transport, transport)\n        tcp_nodelay(tr, True)\n        self.transport = tr\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        # Wake up the writer if currently paused.\n        self.transport = None\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            return\n        self._drain_waiter = None\n        if waiter.done():\n            return\n        if exc is None:\n            waiter.set_result(None)\n        else:\n            set_exception(\n                waiter,\n                ConnectionError(\"Connection lost\"),\n                exc,\n            )\n\n    async def _drain_helper(self) -> None:\n        if self.transport is None:\n            raise ClientConnectionResetError(\"Connection lost\")\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            waiter = self._loop.create_future()\n            self._drain_waiter = waiter\n        await asyncio.shield(waiter)"
    },
    {
      "chunk_id": 321,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "```python"
    },
    {
      "chunk_id": 322,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "import asyncio\nimport codecs\nimport contextlib\nimport functools\nimport io\nimport re\nimport sys\nimport traceback\nimport warnings\nfrom hashlib import md5, sha1, sha256\nfrom http.cookies import CookieError, Morsel, SimpleCookie\nfrom types import MappingProxyType, TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    NamedTuple,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\nfrom yarl import URL\n\nfrom . import hdrs, helpers, http, multipart, payload\nfrom .abc import AbstractStreamWriter\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientOSError,\n    ClientResponseError,\n    ContentTypeError,\n    InvalidURL,\n    ServerFingerprintMismatch,\n)\nfrom .compression_utils import HAS_BROTLI\nfrom .formdata import FormData\nfrom .hdrs import CONTENT_TYPE\nfrom .helpers import (\n    _SENTINEL,\n    BaseTimerContext,\n    BasicAuth,\n    HeadersMixin,\n    TimerNoop,\n    basicauth_from_netrc,\n    frozen_dataclass_decorator,\n    is_expected_content_type,\n    netrc_from_env,\n    parse_mimetype,\n    reify,\n    set_exception,\n    set_result,\n)\nfrom .http import (\n    SERVER_SOFTWARE,\n    HttpVersion,\n    HttpVersion10,\n    HttpVersion11,\n    StreamWriter,\n)\nfrom .log import client_logger\nfrom .streams import StreamReader\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    JSONDecoder,\n    LooseCookies,\n    LooseHeaders,\n    Query,\n    RawHeaders,\n)\n\nif TYPE_CHECKING:\n    import ssl\n    from ssl import SSLContext\nelse:\n    try:\n        import ssl\n        from ssl import SSLContext\n    except ImportError:  # pragma: no cover\n        ssl = None  # type: ignore[assignment]\n        SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\"ClientRequest\", \"ClientResponse\", \"RequestInfo\", \"Fingerprint\")"
    },
    {
      "chunk_id": 323,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "_CONTAINS_CONTROL_CHAR_RE = re.compile(r\"[^-!#$%&'*+.^_`|~0-9a-zA-Z]\")\n\n\ndef _gen_default_accept_encoding() -> str:\n    return \"gzip, deflate, br\" if HAS_BROTLI else \"gzip, deflate\""
    },
    {
      "chunk_id": 324,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "@frozen_dataclass_decorator\nclass ContentDisposition:\n    type: Optional[str]\n    parameters: \"MappingProxyType[str, str]\"\n    filename: Optional[str]"
    },
    {
      "chunk_id": 325,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class _RequestInfo(NamedTuple):\n    url: URL\n    method: str\n    headers: \"CIMultiDictProxy[str]\"\n    real_url: URL"
    },
    {
      "chunk_id": 326,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class RequestInfo(_RequestInfo):\n\n    def __new__(\n        cls,\n        url: URL,\n        method: str,\n        headers: \"CIMultiDictProxy[str]\",\n        real_url: URL = _SENTINEL,  # type: ignore[assignment]\n    ) -> \"RequestInfo\":\n        \"\"\"Create a new RequestInfo instance.\n\n        For backwards compatibility, the real_url parameter is optional.\n        \"\"\"\n        return tuple.__new__(\n            cls, (url, method, headers, url if real_url is _SENTINEL else real_url)\n        )"
    },
    {
      "chunk_id": 327,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class Fingerprint:\n    HASHFUNC_BY_DIGESTLEN = {\n        16: md5,\n        20: sha1,\n        32: sha256,\n    }\n\n    def __init__(self, fingerprint: bytes) -> None:\n        digestlen = len(fingerprint)\n        hashfunc = self.HASHFUNC_BY_DIGESTLEN.get(digestlen)\n        if not hashfunc:\n            raise ValueError(\"fingerprint has invalid length\")\n        elif hashfunc is md5 or hashfunc is sha1:\n            raise ValueError(\"md5 and sha1 are insecure and not supported. Use sha256.\")\n        self._hashfunc = hashfunc\n        self._fingerprint = fingerprint\n\n    @property\n    def fingerprint(self) -> bytes:\n        return self._fingerprint\n\n    def check(self, transport: asyncio.Transport) -> None:\n        if not transport.get_extra_info(\"sslcontext\"):\n            return\n        sslobj = transport.get_extra_info(\"ssl_object\")\n        cert = sslobj.getpeercert(binary_form=True)\n        got = self._hashfunc(cert).digest()\n        if got != self._fingerprint:\n            host, port, *_ = transport.get_extra_info(\"peername\")\n            raise ServerFingerprintMismatch(self._fingerprint, got, host, port)"
    },
    {
      "chunk_id": 328,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "if ssl is not None:\n    SSL_ALLOWED_TYPES = (ssl.SSLContext, bool, Fingerprint)\nelse:  # pragma: no cover\n    SSL_ALLOWED_TYPES = (bool,)  # type: ignore[unreachable]"
    },
    {
      "chunk_id": 329,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "_SSL_SCHEMES = frozenset((\"https\", \"wss\"))"
    },
    {
      "chunk_id": 330,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class ConnectionKey(NamedTuple):\n    # the key should contain an information about used proxy / TLS\n    # to prevent reusing wrong connections from a pool\n    host: str\n    port: Optional[int]\n    is_ssl: bool\n    ssl: Union[SSLContext, bool, Fingerprint]\n    proxy: Optional[URL]\n    proxy_auth: Optional[BasicAuth]\n    proxy_headers_hash: Optional[int]  # hash(CIMultiDict)"
    },
    {
      "chunk_id": 331,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class ClientRequest:\n    GET_METHODS = {\n        hdrs.METH_GET,\n        hdrs.METH_HEAD,\n        hdrs.METH_OPTIONS,\n        hdrs.METH_TRACE,\n    }\n    POST_METHODS = {hdrs.METH_PATCH, hdrs.METH_POST, hdrs.METH_PUT}\n    ALL_METHODS = GET_METHODS.union(POST_METHODS).union({hdrs.METH_DELETE})\n\n    DEFAULT_HEADERS = {\n        hdrs.ACCEPT: \"*/*\",\n        hdrs.ACCEPT_ENCODING: _gen_default_accept_encoding(),\n    }\n\n    # Type of body depends on PAYLOAD_REGISTRY, which is dynamic.\n    body: Any = b\"\"\n    auth = None\n    response = None\n\n    __writer: Optional[\"asyncio.Task[None]\"] = None  # async task for streaming data\n    _continue = None  # waiter future for '100 Continue' response\n\n    _skip_auto_headers: Optional[\"CIMultiDict[None]\"] = None\n\n    # N.B.\n    # Adding __del__ method with self._writer closing doesn't make sense\n    # because _writer is instance method, thus it keeps a reference to self.\n    # Until writer has finished finalizer will not be called.\n\n    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        params: Query = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Optional[Iterable[str]] = None,\n        data: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        auth: Optional[BasicAuth] = None,\n        version: http.HttpVersion = http.HttpVersion11,\n        compress: Union[str, bool] = False,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        loop: asyncio.AbstractEventLoop,\n        response_class: Optional[Type[\"ClientResponse\"]] = None,\n        proxy: Optional[URL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timer: Optional[BaseTimerContext] = None,\n        session: Optional[\"ClientSession\"] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        proxy_headers: Optional[LooseHeaders] = None,\n        traces: Optional[List[\"Trace\"]] = None,\n        trust_env: bool = False,\n        server_hostname: Optional[str] = None,\n    ):\n        if match := _CONTAINS_CONTROL_CHAR_RE.search(method):\n            raise ValueError(\n                f\"Method cannot contain non-token characters {method!r} \"\n                f\"(found at least {match.group()!r})\"\n            )\n        # URL forbids subclasses, so a simple type check is enough.\n        assert type(url) is URL, url\n        if proxy is not None:\n            assert type(proxy) is URL, proxy\n        # FIXME: session is None in tests only, need to fix tests\n        # assert session is not None\n        if TYPE_CHECKING:\n            assert session is not None\n        self._session = session\n        if params:\n            url = url.extend_query(params)\n        self.original_url = url\n        self.url = url.with_fragment(None) if url.raw_fragment else url\n        self.method = method.upper()\n        self.chunked = chunked\n        self.loop = loop\n        self.length = None\n        if response_class is None:\n            real_response_class = ClientResponse\n        else:\n            real_response_class = response_class\n        self.response_class: Type[ClientResponse] = real_response_class\n        self._timer = timer if timer is not None else TimerNoop()\n        self._ssl = ssl\n        self.server_hostname = server_hostname\n\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n        self.update_version(version)\n        self.update_host(url)\n        self.update_headers(headers)\n        self.update_auto_headers(skip_auto_headers)\n        self.update_cookies(cookies)\n        self.update_content_encoding(data, compress)\n        self.update_auth(auth, trust_env)\n        self.update_proxy(proxy, proxy_auth, proxy_headers)\n\n        self.update_body_from_data(data)\n        if data is not None or self.method not in self.GET_METHODS:\n            self.update_transfer_encoding()\n        self.update_expect_continue(expect100)\n        self._traces = [] if traces is None else traces\n\n    def __reset_writer(self, _: object = None) -> None:\n        self.__writer = None\n\n    @property\n    def skip_auto_headers(self) -> CIMultiDict[None]:\n        return self._skip_auto_headers or CIMultiDict()\n\n    @property\n    def _writer(self) -> Optional[\"asyncio.Task[None]\"]:\n        return self.__writer\n\n    @_writer.setter\n    def _writer(self, writer: \"asyncio.Task[None]\") -> None:\n        if self.__writer is not None:\n            self.__writer.remove_done_callback(self.__reset_writer)\n        self.__writer = writer\n        writer.add_done_callback(self.__reset_writer)\n\n    def is_ssl(self) -> bool:\n        return self.url.scheme in _SSL_SCHEMES\n\n    @property\n    def ssl(self) -> Union[\"SSLContext\", bool, Fingerprint]:\n        return self._ssl\n\n    @property\n    def connection_key(self) -> ConnectionKey:\n        if proxy_headers := self.proxy_headers:\n            h: Optional[int] = hash(tuple(proxy_headers.items()))\n        else:\n            h = None\n        url = self.url\n        return tuple.__new__(\n            ConnectionKey,\n            (\n                url.raw_host or \"\",\n                url.port,\n                url.scheme in _SSL_SCHEMES,\n                self._ssl,\n                self.proxy,\n                self.proxy_auth,\n                h,\n            ),\n        )\n\n    @property\n    def host(self) -> str:\n        ret = self.url.raw_host\n        assert ret is not None\n        return ret\n\n    @property\n    def port(self) -> Optional[int]:\n        return self.url.port\n\n    @property\n    def request_info(self) -> RequestInfo:\n        headers: CIMultiDictProxy[str] = CIMultiDictProxy(self.headers)\n        # These are created on every request, so we use a NamedTuple\n        # for performance reasons. We don't use the RequestInfo.__new__\n        # method because it has a different signature which is provided\n        # for backwards compatibility only.\n        return tuple.__new__(\n            RequestInfo, (self.url, self.method, headers, self.original_url)\n        )\n\n    def update_host(self, url: URL) -> None:\n        \"\"\"Update destination host, port and connection type (ssl).\"\"\"\n        # get host/port\n        if not url.raw_host:\n            raise InvalidURL(url)\n\n        # basic auth info\n        if url.raw_user or url.raw_password:\n            self.auth = helpers.BasicAuth(url.user or \"\", url.password or \"\")\n\n    def update_version(self, version: Union[http.HttpVersion, str]) -> None:\n        \"\"\"Convert request version to two elements tuple.\n\n        parser HTTP version '1.1' => (1, 1)\n        \"\"\"\n        if isinstance(version, str):\n            v = [part.strip() for part in version.split(\".\", 1)]\n            try:\n                version = http.HttpVersion(int(v[0]), int(v[1]))\n            except ValueError:\n                raise ValueError(\n                    f\"Can not parse http version number: {version}\"\n                ) from None\n        self.version = version\n\n    def update_headers(self, headers: Optional[LooseHeaders]) -> None:\n        \"\"\"Update request headers.\"\"\"\n        self.headers: CIMultiDict[str] = CIMultiDict()\n\n        # Build the host header\n        host = self.url.host_port_subcomponent\n\n        # host_port_subcomponent is None when the URL is a relative URL.\n        # but we know we do not have a relative URL here.\n        assert host is not None\n        self.headers[hdrs.HOST] = host\n\n        if not headers:\n            return\n\n        if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\n            headers = headers.items()\n\n        for key, value in headers:  # type: ignore[misc]\n            # A special case for Host header\n            if key in hdrs.HOST_ALL:\n                self.headers[key] = value\n            else:\n                self.headers.add(key, value)\n\n    def update_auto_headers(self, skip_auto_headers: Optional[Iterable[str]]) -> None:\n        if skip_auto_headers is not None:\n            self._skip_auto_headers = CIMultiDict(\n                (hdr, None) for hdr in sorted(skip_auto_headers)\n            )\n            used_headers = self.headers.copy()\n            used_headers.extend(self._skip_auto_headers)  # type: ignore[arg-type]\n        else:\n            # Fast path when there are no headers to skip\n            # which is the most common case.\n            used_headers = self.headers\n\n        for hdr, val in self.DEFAULT_HEADERS.items():\n            if hdr not in used_headers:\n                self.headers[hdr] = val\n\n        if hdrs.USER_AGENT not in used_headers:\n            self.headers[hdrs.USER_AGENT] = SERVER_SOFTWARE\n\n    def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\n        \"\"\"Update request cookies header.\"\"\"\n        if not cookies:\n            return\n\n        c = SimpleCookie()\n        if hdrs.COOKIE in self.headers:\n            c.load(self.headers.get(hdrs.COOKIE, \"\"))\n            del self.headers[hdrs.COOKIE]\n\n        if isinstance(cookies, Mapping):\n            iter_cookies = cookies.items()\n        else:\n            iter_cookies = cookies  # type: ignore[assignment]\n        for name, value in iter_cookies:\n            if isinstance(value, Morsel):\n                # Preserve coded_value\n                mrsl_val = value.get(value.key, Morsel())\n                mrsl_val.set(value.key, value.value, value.coded_value)\n                c[name] = mrsl_val\n            else:\n                c[name] = value  # type: ignore[assignment]\n\n        self.headers[hdrs.COOKIE] = c.output(header=\"\", sep=\";\").strip()\n\n    def update_content_encoding(self, data: Any, compress: Union[bool, str]) -> None:\n        \"\"\"Set request content encoding.\"\"\"\n        self.compress = None\n        if not data:\n            return\n\n        if self.headers.get(hdrs.CONTENT_ENCODING):\n            if compress:\n                raise ValueError(\n                    \"compress can not be set if Content-Encoding header is set\"\n                )\n        elif compress:\n            self.compress = compress if isinstance(compress, str) else \"deflate\"\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\n            self.chunked = True  # enable chunked, no need to deal with length\n\n    def update_transfer_encoding(self) -> None:\n        \"\"\"Analyze transfer-encoding header.\"\"\"\n        te = self.headers.get(hdrs.TRANSFER_ENCODING, \"\").lower()\n\n        if \"chunked\" in te:\n            if self.chunked:\n                raise ValueError(\n                    \"chunked can not be set \"\n                    'if \"Transfer-Encoding: chunked\" header is set'\n                )\n\n        elif self.chunked:\n            if hdrs.CONTENT_LENGTH in self.headers:\n                raise ValueError(\n                    \"chunked can not be set if Content-Length header is set\"\n                )\n\n            self.headers[hdrs.TRANSFER_ENCODING] = \"chunked\"\n        else:\n            if hdrs.CONTENT_LENGTH not in self.headers:\n                self.headers[hdrs.CONTENT_LENGTH] = str(len(self.body))\n\n    def update_auth(self, auth: Optional[BasicAuth], trust_env: bool = False) -> None:\n        \"\"\"Set basic auth.\"\"\"\n        if auth is None:\n            auth = self.auth\n        if auth is None and trust_env and self.url.host is not None:\n            netrc_obj = netrc_from_env()\n            with contextlib.suppress(LookupError):\n                auth = basicauth_from_netrc(netrc_obj, self.url.host)\n        if auth is None:\n            return\n\n        if not isinstance(auth, helpers.BasicAuth):\n            raise TypeError(\"BasicAuth() tuple is required instead\")\n\n        self.headers[hdrs.AUTHORIZATION] = auth.encode()\n\n    def update_body_from_data(self, body: Any) -> None:\n        if body is None:\n            return\n\n        # FormData\n        if isinstance(body, FormData):\n            body = body()\n\n        try:\n            body = payload.PAYLOAD_REGISTRY.get(body, disposition=None)\n        except payload.LookupError:\n            boundary = None\n            if CONTENT_TYPE in self.headers:\n                boundary = parse_mimetype(self.headers[CONTENT_TYPE]).parameters.get(\n                    \"boundary\"\n                )\n            body = FormData(body, boundary=boundary)()\n\n        self.body = body\n\n        # enable chunked encoding if needed\n        if not self.chunked and hdrs.CONTENT_LENGTH not in self.headers:\n            if (size := body.size) is not None:\n                self.headers[hdrs.CONTENT_LENGTH] = str(size)\n            else:\n                self.chunked = True\n\n        # copy payload headers\n        assert body.headers\n        headers = self.headers\n        skip_headers = self._skip_auto_headers\n        for key, value in body.headers.items():\n            if key in headers or (skip_headers is not None and key in skip_headers):\n                continue\n            headers[key] = value\n\n    def update_expect_continue(self, expect: bool = False) -> None:\n        if expect:\n            self.headers[hdrs.EXPECT] = \"100-continue\"\n        elif (\n            hdrs.EXPECT in self.headers\n            and self.headers[hdrs.EXPECT].lower() == \"100-continue\"\n        ):\n            expect = True\n\n        if expect:\n            self._continue = self.loop.create_future()\n\n    def update_proxy(\n        self,\n        proxy: Optional[URL],\n        proxy_auth: Optional[BasicAuth],\n        proxy_headers: Optional[LooseHeaders],\n    ) -> None:\n        self.proxy = proxy\n        if proxy is None:\n            self.proxy_auth = None\n            self.proxy_headers = None\n            return\n\n        if proxy_auth and not isinstance(proxy_auth, helpers.BasicAuth):\n            raise ValueError(\"proxy_auth must be None or BasicAuth() tuple\")\n        self.proxy_auth = proxy_auth\n\n        if proxy_headers is not None and not isinstance(\n            proxy_headers, (MultiDict, MultiDictProxy)\n        ):\n            proxy_headers = CIMultiDict(proxy_headers)\n        self.proxy_headers = proxy_headers\n\n    async def write_bytes(\n        self, writer: AbstractStreamWriter, conn: \"Connection\"\n    ) -> None:\n        \"\"\"Support coroutines that yields bytes objects.\"\"\"\n        # 100 response\n        if self._continue is not None:\n            await writer.drain()\n            await self._continue\n\n        protocol = conn.protocol\n        assert protocol is not None\n        try:\n            if isinstance(self.body, payload.Payload):\n                await self.body.write(writer)\n            else:\n                if isinstance(self.body, (bytes, bytearray)):\n                    self.body = (self.body,)\n\n                for chunk in self.body:\n                    await writer.write(chunk)\n        except OSError as underlying_exc:\n            reraised_exc = underlying_exc\n\n            exc_is_not_timeout = underlying_exc.errno is not None or not isinstance(\n                underlying_exc, asyncio.TimeoutError\n            )\n            if exc_is_not_timeout:\n                reraised_exc = ClientOSError(\n                    underlying_exc.errno,\n                    f\"Can not write request body for {self.url !s}\",\n                )\n\n            set_exception(protocol, reraised_exc, underlying_exc)\n        except asyncio.CancelledError:\n            # Body hasn't been fully sent, so connection can't be reused.\n            conn.close()\n            raise\n        except Exception as underlying_exc:\n            set_exception(\n                protocol,\n                ClientConnectionError(\n                    f\"Failed to send bytes into the underlying connection {conn !s}\",\n                ),\n                underlying_exc,\n            )\n        else:\n            await writer.write_eof()\n            protocol.start_timeout()\n\n    async def send(self, conn: \"Connection\") -> \"ClientResponse\":\n        # Specify request target:\n        # - CONNECT request must send authority form URI\n        # - not CONNECT proxy must send absolute form URI\n        # - most common is origin form URI\n        if self.method == hdrs.METH_CONNECT:\n            connect_host = self.url.host_subcomponent\n            assert connect_host is not None\n            path = f\"{connect_host}:{self.url.port}\"\n        elif self.proxy and not self.is_ssl():\n            path = str(self.url)\n        else:\n            path = self.url.raw_path_qs\n\n        protocol = conn.protocol\n        assert protocol is not None\n        writer = StreamWriter(\n            protocol,\n            self.loop,\n            on_chunk_sent=(\n                functools.partial(self._on_chunk_request_sent, self.method, self.url)\n                if self._traces\n                else None\n            ),\n            on_headers_sent=(\n                functools.partial(self._on_headers_request_sent, self.method, self.url)\n                if self._traces\n                else None\n            ),\n        )\n\n        if self.compress:\n            writer.enable_compression(self.compress)\n\n        if self.chunked is not None:\n            writer.enable_chunking()\n\n        # set default content-type\n        if (\n            self.method in self.POST_METHODS\n            and (\n                self._skip_auto_headers is None\n                or hdrs.CONTENT_TYPE not in self._skip_auto_headers\n            )\n            and hdrs.CONTENT_TYPE not in self.headers\n        ):\n            self.headers[hdrs.CONTENT_TYPE] = \"application/octet-stream\"\n\n        v = self.version\n        if hdrs.CONNECTION not in self.headers:\n            if conn._connector.force_close:\n                if v == HttpVersion11:\n                    self.headers[hdrs.CONNECTION] = \"close\"\n            elif v == HttpVersion10:\n                self.headers[hdrs.CONNECTION] = \"keep-alive\"\n\n        # status + headers\n        status_line = f\"{self.method} {path} HTTP/{v.major}.{v.minor}\"\n        await writer.write_headers(status_line, self.headers)\n        task: Optional[\"asyncio.Task[None]\"]\n        if self.body or self._continue is not None or protocol.writing_paused:\n            coro = self.write_bytes(writer, conn)\n            if sys.version_info >= (3, 12):\n                # Optimization for Python 3.12, try to write\n                # bytes immediately to avoid having to schedule\n                # the task on the event loop.\n                task = asyncio.Task(coro, loop=self.loop, eager_start=True)\n            else:\n                task = self.loop.create_task(coro)\n            if task.done():\n                task = None\n            else:\n                self._writer = task\n        else:\n            # We have nothing to write because\n            # - there is no body\n            # - the protocol does not have writing paused\n            # - we are not waiting for a 100-continue response\n            protocol.start_timeout()\n            writer.set_eof()\n            task = None\n        response_class = self.response_class\n        assert response_class is not None\n        self.response = response_class(\n            self.method,\n            self.original_url,\n            writer=task,\n            continue100=self._continue,\n            timer=self._timer,\n            request_info=self.request_info,\n            traces=self._traces,\n            loop=self.loop,\n            session=self._session,\n        )\n        return self.response\n\n    async def close(self) -> None:\n        if self.__writer is not None:\n            try:\n                await self.__writer\n            except asyncio.CancelledError:\n                if (\n                    sys.version_info >= (3, 11)\n                    and (task := asyncio.current_task())\n                    and task.cancelling()\n                ):\n                    raise\n\n    def terminate(self) -> None:\n        if self.__writer is not None:\n            if not self.loop.is_closed():\n                self.__writer.cancel()\n            self.__writer.remove_done_callback(self.__reset_writer)\n            self.__writer = None\n\n    async def _on_chunk_request_sent(self, method: str, url: URL, chunk: bytes) -> None:\n        for trace in self._traces:\n            await trace.send_request_chunk_sent(method, url, chunk)\n\n    async def _on_headers_request_sent(\n        self, method: str, url: URL, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        for trace in self._traces:\n            await trace.send_request_headers(method, url, headers)"
    },
    {
      "chunk_id": 332,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "_CONNECTION_CLOSED_EXCEPTION = ClientConnectionError(\"Connection closed\")"
    },
    {
      "chunk_id": 333,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_reqrep.py",
      "content": "class ClientResponse(HeadersMixin):\n    # Some of these attributes are None when created,\n    # but will be set by the start() method.\n    # As the end user will likely never see the None values, we cheat the types below.\n    # from the Status-Line of the response\n    version: Optional[HttpVersion] = None  # HTTP-Version\n    status: int = None  # type: ignore[assignment] # Status-Code\n    reason: Optional[str] = None  # Reason-Phrase\n\n    content: StreamReader = None  # type: ignore[assignment] # Payload stream\n    _body: Optional[bytes] = None\n    _headers: CIMultiDictProxy[str] = None  # type: ignore[assignment]\n    _history: Tuple[\"ClientResponse\", ...] = ()\n    _raw_headers: RawHeaders = None  # type: ignore[assignment]\n\n    _connection: Optional[\"Connection\"] = None  # current connection\n    _cookies: Optional[SimpleCookie] = None\n    _continue: Optional[\"asyncio.Future[bool]\"] = None\n    _source_traceback: Optional[traceback.StackSummary] = None\n    _session: Optional[\"ClientSession\"] = None\n    # set up by ClientRequest after ClientResponse object creation\n    # post-init stage allows to not change ctor signature\n    _closed = True  # to allow __del__ for non-initialized properly response\n    _released = False\n    _in_context = False\n\n    _resolve_charset: Callable[[\"ClientResponse\", bytes], str] = lambda *_: \"utf-8\"\n\n    __writer: Optional[\"asyncio.Task[None]\"] = None\n\n    def __init__(\n        self,\n        method: str,\n        url: URL,\n        *,\n        writer: \"Optional[asyncio.Task[None]]\",\n        continue100: Optional[\"asyncio.Future[bool]\"],\n        timer: Optional[BaseTimerContext],\n        request_info: RequestInfo,\n        traces: List[\"Trace\"],\n        loop: asyncio.AbstractEventLoop,\n        session: \"ClientSession\",\n    ) -> None:\n        # URL forbids subclasses, so a simple type check is enough.\n        assert type(url) is URL\n\n        self.method = method\n\n        self._real_url = url\n        self._url = url.with_fragment(None) if url.raw_fragment else url\n        if writer is not None:\n            self._writer = writer\n        if continue100 is not None:\n            self._continue = continue100\n        self._request_info = request_info\n        self._timer = timer if timer is not None else TimerNoop()\n        self._cache: Dict[str, Any] = {}\n        self._traces = traces\n        self._loop = loop\n        # Save reference to _resolve_charset, so that get_encoding() will still\n        # work after the response has finished reading the body.\n        # TODO: Fix session=None in tests (see ClientRequest.__init__).\n        if session is not None:\n            # store a reference to session #1985\n            self._session = session\n            self._resolve_charset = session._resolve_charset\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n    def __reset_writer(self, _: object = None) -> None:\n        self.__writer = None\n\n    @property\n    def _writer(self) -> Optional[\"asyncio.Task[None]\"]:\n        \"\"\"The writer task for streaming data.\n\n        _writer is only provided for backwards compatibility\n        for subclasses that may need to access it.\n        \"\"\"\n        return self.__writer\n\n    @_writer.setter\n    def _writer(self, writer: Optional[\"asyncio.Task[None]\"]) -> None:\n        \"\"\"Set the writer task for streaming data.\"\"\"\n        if self.__writer is not None:\n            self.__writer.remove_done_callback(self.__reset_writer)\n        self.__writer = writer\n        if writer is None:\n            return\n        if writer.done():\n            # The writer is already done, so we can clear it immediately.\n            self.__writer = None\n        else:\n            writer.add_done_callback(self.__reset_writer)\n\n    @property\n    def cookies(self) -> SimpleCookie:\n        if self._cookies is None:\n            self._cookies = SimpleCookie()\n        return self._cookies\n\n    @cookies.setter\n    def cookies(self, cookies: SimpleCookie) -> None:\n        self._cookies = cookies\n\n    @reify\n    def url(self) -> URL:\n        return self._url\n\n    @reify\n    def real_url(self) -> URL:\n        return self._real_url\n\n    @reify\n    def host(self) -> str:\n        assert self._url.host is not None\n        return self._url.host\n\n    @reify\n    def headers(self) -> \"CIMultiDictProxy[str]\":\n        return self._headers\n\n    @reify\n    def raw_headers(self) -> RawHeaders:\n        return self._raw_headers\n\n    @reify\n    def request_info(self) -> RequestInfo:\n        return self._request_info\n\n    @reify\n    def content_disposition(self) -> Optional[ContentDisposition]:\n        raw = self._headers.get(hdrs.CONTENT_DISPOSITION)\n        if raw is None:\n            return None\n        disposition_type, params_dct = multipart.parse_content_disposition(raw)\n        params = MappingProxyType(params_dct)\n        filename = multipart.content_disposition_filename(params)\n        return ContentDisposition(disposition_type, params, filename)\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._closed:\n            return\n\n        if self._connection is not None:\n            self._connection.release()\n            self._cleanup_writer()\n\n            if self._loop.get_debug():\n                _warnings.warn(\n                    f\"Unclosed response {self!r}\", ResourceWarning, source=self\n                )\n                context = {\"client_response\": self, \"message\": \"Unclosed response\"}\n                if self._source_traceback:\n                    context[\"source_traceback\"] = self._source_traceback\n                self._loop.call_exception_handler(context)\n\n    def __repr__(self) -> str:\n        out = io.StringIO()\n        ascii_encodable_url = str(self.url)\n        if self.reason:\n            ascii_encodable_reason = self.reason.encode(\n                \"ascii\", \"backslashreplace\"\n            ).decode(\"ascii\")\n        else:\n            ascii_encodable_reason = \"None\"\n        print(\n            \"<ClientResponse({}) [{} {}]>\".format(\n                ascii_encodable_url, self.status, ascii_encodable_reason\n            ),\n            file=out,\n        )\n        print(self.headers, file=out)\n        return out.getvalue()\n\n    @property\n    def connection(self) -> Optional[\"Connection\"]:\n        return self._connection\n\n    @reify\n    def history(self) -> Tuple[\"ClientResponse\", ...]:\n        \"\"\"A sequence of responses, if redirects occurred.\"\"\"\n        return self._history\n\n    @reify\n    def links(self) -> \"MultiDictProxy[MultiDictProxy[Union[str, URL]]]\":\n        links_str = \", \".join(self.headers.getall(\"link\", []))\n\n        if not links_str:\n            return MultiDictProxy(MultiDict())\n\n        links: MultiDict[MultiDictProxy[Union[str, URL]]] = MultiDict()\n\n        for val in re.split(r\",(?=\\s*<)\", links_str):\n            match = re.match(r\"\\s*<(.*)>(.*)\", val)\n            if match is None:  # pragma: no cover\n                # the check exists to suppress mypy error\n                continue\n            url, params_str = match.groups()\n            params = params_str.split(\";\")[1:]\n\n            link: MultiDict[Union[str, URL]] = MultiDict()\n\n            for param in params:\n                match = re.match(r\"^\\s*(\\S*)\\s*=\\s*(['\\\"]?)(.*?)(\\2)\\s*$\", param, re.M)\n                if match is None:  # pragma: no cover\n                    # the check exists to suppress mypy error\n                    continue\n                key, _, value, _ = match.groups()\n\n                link.add(key, value)\n\n            key = link.get(\"rel\", url)\n\n            link.add(\"url\", self.url.join(URL(url)))\n\n            links.add(str(key), MultiDictProxy(link))\n\n        return MultiDictProxy(links)\n\n    async def start(self, connection: \"Connection\") -> \"ClientResponse\":\n        \"\"\"Start response processing.\"\"\"\n        self._closed = False\n        self._protocol = connection.protocol\n        self._connection = connection\n\n        with self._timer:\n            while True:\n                # read response\n                try:\n                    protocol = self._protocol\n                    message, payload = await protocol.read()  # type: ignore[union-attr]\n                except http.HttpProcessingError as exc:\n                    raise ClientResponseError(\n                        self.request_info,\n                        self.history,\n                        status=exc.code,\n                        message=exc.message,\n                        headers=exc.headers,\n                    ) from exc\n\n                if message.code < 100 or message.code > 199 or message.code == 101:\n                    break\n\n                if self._continue is not None:\n                    set_result(self._continue, True)\n                    self._continue = None\n\n        # payload eof handler\n        payload.on_eof(self._response_eof)\n\n        # response status\n        self.version = message.version\n        self.status = message.code\n        self.reason = message.reason\n\n        # headers\n        self._headers = message.headers  # type is CIMultiDictProxy\n        self._raw_headers = message.raw_headers  # type is Tuple[bytes, bytes]\n\n        # payload\n        self.content = payload\n\n        # cookies\n        if cookie_hdrs := self.headers.getall(hdrs.SET_COOKIE, ()):\n            cookies = SimpleCookie()\n            for hdr in cookie_hdrs:\n                try:\n                    cookies.load(hdr)\n                except CookieError as exc:\n                    client_logger.warning(\"Can not load response cookies: %s\", exc)\n            self._cookies = cookies\n        return self\n\n    def _response_eof(self) -> None:\n        if self._closed:\n            return\n\n        # protocol could be None because connection could be detached\n        protocol = self._connection and self._connection.protocol\n        if protocol is not None and protocol.upgraded:\n            return\n\n        self._closed = True\n        self._cleanup_writer()\n        self._release_connection()\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    def close(self) -> None:\n        if not self._released:\n            self._notify_content()\n\n        self._closed = True\n        if self._loop.is_closed():\n            return\n\n        self._cleanup_writer()\n        if self._connection is not None:\n            self._connection.close()\n            self._connection = None\n\n    def release(self) -> None:\n        if not self._released:\n            self._notify_content()\n\n        self._closed = True\n\n        self._cleanup_writer()\n        self._release_connection()\n\n    @property\n    def ok(self) -> bool:\n        \"\"\"Returns ``True`` if ``status`` is less than ``400``, ``False`` if not.\n\n        This is **not** a check for ``200 OK`` but a check that the response\n        status is under 400.\n        \"\"\"\n        return 400 > self.status\n\n    def raise_for_status(self) -> None:\n        if not self.ok:\n            # reason should always be not None for a started response\n            assert self.reason is not None\n\n            # If we're in a context we can rely on __aexit__() to release as the\n            # exception propagates.\n            if not self._in_context:\n                self.release()\n\n            raise ClientResponseError(\n                self.request_info,\n                self.history,\n                status=self.status,\n                message=self.reason,\n                headers=self.headers,\n            )\n\n    def _release_connection(self) -> None:\n        if self._connection is not None:\n            if self.__writer is None:\n                self._connection.release()\n                self._connection = None\n            else:\n                self.__writer.add_done_callback(lambda f: self._release_connection())\n\n    async def _wait_released(self) -> None:\n        if self.__writer is not None:\n            try:\n                await self.__writer\n            except asyncio.CancelledError:\n                if (\n                    sys.version_info >= (3, 11)\n                    and (task := asyncio.current_task())\n                    and task.cancelling()\n                ):\n                    raise\n        self._release_connection()\n\n    def _cleanup_writer(self) -> None:\n        if self.__writer is not None:\n            self.__writer.cancel()\n        self._session = None\n\n    def _notify_content(self) -> None:\n        content = self.content\n        # content can be None here, but the types are cheated elsewhere.\n        if content and content.exception() is None:  # type: ignore[truthy-bool]\n            set_exception(content, _CONNECTION_CLOSED_EXCEPTION)\n        self._released = True\n\n    async def wait_for_close(self) -> None:\n        if self.__writer is not None:\n            try:\n                await self.__writer\n            except asyncio.CancelledError:\n                if (\n                    sys.version_info >= (3, 11)\n                    and (task := asyncio.current_task())\n                    and task.cancelling()\n                ):\n                    raise\n        self.release()\n\n    async def read(self) -> bytes:\n        \"\"\"Read response payload.\"\"\"\n        if self._body is None:\n            try:\n                self._body = await self.content.read()\n                for trace in self._traces:\n                    await trace.send_response_chunk_received(\n                        self.method, self.url, self._body\n                    )\n            except BaseException:\n                self.close()\n                raise\n        elif self._released:  # Response explicitly released\n            raise ClientConnectionError(\"Connection closed\")\n\n        protocol = self._connection and self._connection.protocol\n        if protocol is None or not protocol.upgraded:\n            await self._wait_released()  # Underlying connection released\n        return self._body\n\n    def get_encoding(self) -> str:\n        ctype = self.headers.get(hdrs.CONTENT_TYPE, \"\").lower()\n        mimetype = helpers.parse_mimetype(ctype)\n\n        encoding = mimetype.parameters.get(\"charset\")\n        if encoding:\n            with contextlib.suppress(LookupError, ValueError):\n                return codecs.lookup(encoding).name\n\n        if mimetype.type == \"application\" and (\n            mimetype.subtype == \"json\" or mimetype.subtype == \"rdap\"\n        ):\n            # RFC 7159 states that the default encoding is UTF-8.\n            # RFC 7483 defines application/rdap+json\n            return \"utf-8\"\n\n        if self._body is None:\n            raise RuntimeError(\n                \"Cannot compute fallback encoding of a not yet read body\"\n            )\n\n        return self._resolve_charset(self, self._body)\n\n    async def text(self, encoding: Optional[str] = None, errors: str = \"strict\") -> str:\n        \"\"\"Read response payload and decode.\"\"\"\n        await self.read()\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return self._body.decode(encoding, errors=errors)  # type: ignore[union-attr]\n\n    async def json(\n        self,\n        *,\n        encoding: Optional[str] = None,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        content_type: Optional[str] = \"application/json\",\n    ) -> Any:\n        \"\"\"Read and decodes JSON response.\"\"\"\n        await self.read()\n\n        if content_type:\n            if not is_expected_content_type(self.content_type, content_type):\n                raise ContentTypeError(\n                    self.request_info,\n                    self.history,\n                    status=self.status,\n                    message=(\n                        \"Attempt to decode JSON with \"\n                        \"unexpected mimetype: %s\" % self.content_type\n                    ),\n                    headers=self.headers,\n                )\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return loads(self._body.decode(encoding))  # type: ignore[union-attr]\n\n    async def __aenter__(self) -> \"ClientResponse\":\n        self._in_context = True\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self._in_context = False\n        # similar to _RequestContextManager, we do not need to check\n        # for exceptions, response object can close connection\n        # if state is broken\n        self.release()\n        await self.wait_for_close()\n```"
    },
    {
      "chunk_id": 334,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_proto.py",
      "content": "```python"
    },
    {
      "chunk_id": 335,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_proto.py",
      "content": "import asyncio\nfrom contextlib import suppress\nfrom typing import Any, Optional, Tuple, Type, Union\n\nfrom .base_protocol import BaseProtocol\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientOSError,\n    ClientPayloadError,\n    ServerDisconnectedError,\n    SocketTimeoutError,\n)\nfrom .helpers import (\n    _EXC_SENTINEL,\n    EMPTY_BODY_STATUS_CODES,\n    BaseTimerContext,\n    set_exception,\n    set_result,\n)\nfrom .http import HttpResponseParser, RawResponseMessage, WebSocketReader\nfrom .http_exceptions import HttpProcessingError\nfrom .streams import EMPTY_PAYLOAD, DataQueue, StreamReader"
    },
    {
      "chunk_id": 336,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_proto.py",
      "content": "class ResponseHandler(BaseProtocol, DataQueue[Tuple[RawResponseMessage, StreamReader]]):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        BaseProtocol.__init__(self, loop=loop)\n        DataQueue.__init__(self, loop)\n\n        self._should_close = False\n\n        self._payload: Optional[StreamReader] = None\n        self._skip_payload = False\n        self._payload_parser: Optional[WebSocketReader] = None\n\n        self._timer = None\n\n        self._tail = b\"\"\n        self._upgraded = False\n        self._parser: Optional[HttpResponseParser] = None\n\n        self._read_timeout: Optional[float] = None\n        self._read_timeout_handle: Optional[asyncio.TimerHandle] = None\n\n        self._timeout_ceil_threshold: Optional[float] = 5\n\n        self.closed: asyncio.Future[None] = self._loop.create_future()\n\n    @property\n    def upgraded(self) -> bool:\n        return self._upgraded\n\n    @property\n    def should_close(self) -> bool:\n        return bool(\n            self._should_close\n            or (self._payload is not None and not self._payload.is_eof())\n            or self._upgraded\n            or self._exception is not None\n            or self._payload_parser is not None\n            or self._buffer\n            or self._tail\n        )\n\n    def force_close(self) -> None:\n        self._should_close = True\n\n    def close(self) -> None:\n        transport = self.transport\n        if transport is not None:\n            transport.close()\n            self.transport = None\n            self._payload = None\n            self._drop_timeout()\n\n    def is_connected(self) -> bool:\n        return self.transport is not None and not self.transport.is_closing()\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        self._drop_timeout()\n\n        original_connection_error = exc\n        reraised_exc = original_connection_error\n\n        connection_closed_cleanly = original_connection_error is None\n\n        if connection_closed_cleanly:\n            set_result(self.closed, None)\n        else:\n            assert original_connection_error is not None\n            set_exception(\n                self.closed,\n                ClientConnectionError(\n                    f\"Connection lost: {original_connection_error !s}\",\n                ),\n                original_connection_error,\n            )\n\n        if self._payload_parser is not None:\n            with suppress(Exception):  # FIXME: log this somehow?\n                self._payload_parser.feed_eof()\n\n        uncompleted = None\n        if self._parser is not None:\n            try:\n                uncompleted = self._parser.feed_eof()\n            except Exception as underlying_exc:\n                if self._payload is not None:\n                    client_payload_exc_msg = (\n                        f\"Response payload is not completed: {underlying_exc !r}\"\n                    )\n                    if not connection_closed_cleanly:\n                        client_payload_exc_msg = (\n                            f\"{client_payload_exc_msg !s}. \"\n                            f\"{original_connection_error !r}\"\n                        )\n                    set_exception(\n                        self._payload,\n                        ClientPayloadError(client_payload_exc_msg),\n                        underlying_exc,\n                    )\n\n        if not self.is_eof():\n            if isinstance(original_connection_error, OSError):\n                reraised_exc = ClientOSError(*original_connection_error.args)\n            if connection_closed_cleanly:\n                reraised_exc = ServerDisconnectedError(uncompleted)\n            # assigns self._should_close to True as side effect,\n            # we do it anyway below\n            underlying_non_eof_exc = (\n                _EXC_SENTINEL\n                if connection_closed_cleanly\n                else original_connection_error\n            )\n            assert underlying_non_eof_exc is not None\n            assert reraised_exc is not None\n            self.set_exception(reraised_exc, underlying_non_eof_exc)\n\n        self._should_close = True\n        self._parser = None\n        self._payload = None\n        self._payload_parser = None\n        self._reading_paused = False\n\n        super().connection_lost(reraised_exc)\n\n    def eof_received(self) -> None:\n        # should call parser.feed_eof() most likely\n        self._drop_timeout()\n\n    def pause_reading(self) -> None:\n        super().pause_reading()\n        self._drop_timeout()\n\n    def resume_reading(self) -> None:\n        super().resume_reading()\n        self._reschedule_timeout()\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._should_close = True\n        self._drop_timeout()\n        super().set_exception(exc, exc_cause)\n\n    def set_parser(self, parser: Any, payload: Any) -> None:\n        # TODO: actual types are:\n        #   parser: WebSocketReader\n        #   payload: WebSocketDataQueue\n        # but they are not generi enough\n        # Need an ABC for both types\n        self._payload = payload\n        self._payload_parser = parser\n\n        self._drop_timeout()\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def set_response_params(\n        self,\n        *,\n        timer: Optional[BaseTimerContext] = None,\n        skip_payload: bool = False,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n        read_timeout: Optional[float] = None,\n        read_bufsize: int = 2**16,\n        timeout_ceil_threshold: float = 5,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n    ) -> None:\n        self._skip_payload = skip_payload\n\n        self._read_timeout = read_timeout\n\n        self._timeout_ceil_threshold = timeout_ceil_threshold\n\n        self._parser = HttpResponseParser(\n            self,\n            self._loop,\n            read_bufsize,\n            timer=timer,\n            payload_exception=ClientPayloadError,\n            response_with_body=not skip_payload,\n            read_until_eof=read_until_eof,\n            auto_decompress=auto_decompress,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n        )\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def _drop_timeout(self) -> None:\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n            self._read_timeout_handle = None\n\n    def _reschedule_timeout(self) -> None:\n        timeout = self._read_timeout\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n\n        if timeout:\n            self._read_timeout_handle = self._loop.call_later(\n                timeout, self._on_read_timeout\n            )\n        else:\n            self._read_timeout_handle = None\n\n    def start_timeout(self) -> None:\n        self._reschedule_timeout()\n\n    @property\n    def read_timeout(self) -> Optional[float]:\n        return self._read_timeout\n\n    @read_timeout.setter\n    def read_timeout(self, read_timeout: Optional[float]) -> None:\n        self._read_timeout = read_timeout\n\n    def _on_read_timeout(self) -> None:\n        exc = SocketTimeoutError(\"Timeout on reading data from socket\")\n        self.set_exception(exc)\n        if self._payload is not None:\n            set_exception(self._payload, exc)\n\n    def data_received(self, data: bytes) -> None:\n        self._reschedule_timeout()\n\n        if not data:\n            return\n\n        # custom payload parser - currently always WebSocketReader\n        if self._payload_parser is not None:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self._payload = None\n                self._payload_parser = None\n\n                if tail:\n                    self.data_received(tail)\n            return\n\n        if self._upgraded or self._parser is None:\n            # i.e. websocket connection, websocket parser is not set yet\n            self._tail += data\n            return\n\n        # parse http messages\n        try:\n            messages, upgraded, tail = self._parser.feed_data(data)\n        except BaseException as underlying_exc:\n            if self.transport is not None:\n                # connection.release() could be called BEFORE\n                # data_received(), the transport is already\n                # closed in this case\n                self.transport.close()\n            # should_close is True after the call\n            if isinstance(underlying_exc, HttpProcessingError):\n                exc = HttpProcessingError(\n                    code=underlying_exc.code,\n                    message=underlying_exc.message,\n                    headers=underlying_exc.headers,\n                )\n            else:\n                exc = HttpProcessingError()\n            self.set_exception(exc, underlying_exc)\n            return\n\n        self._upgraded = upgraded\n\n        payload: Optional[StreamReader] = None\n        for message, payload in messages:\n            if message.should_close:\n                self._should_close = True\n\n            self._payload = payload\n\n            if self._skip_payload or message.code in EMPTY_BODY_STATUS_CODES:\n                self.feed_data((message, EMPTY_PAYLOAD))\n            else:\n                self.feed_data((message, payload))\n\n        if payload is not None:\n            # new message(s) was processed\n            # register timeout handler unsubscribing\n            # either on end-of-stream or immediately for\n            # EMPTY_PAYLOAD\n            if payload is not EMPTY_PAYLOAD:\n                payload.on_eof(self._drop_timeout)\n            else:\n                self._drop_timeout()\n\n        if upgraded and tail:\n            self.data_received(tail)\n```"
    },
    {
      "chunk_id": 337,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "import abc\nimport asyncio\nimport base64\nimport functools\nimport hashlib\nimport html\nimport keyword\nimport os\nimport re\nimport sys\nfrom pathlib import Path\nfrom types import MappingProxyType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Container,\n    Dict,\n    Final,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    NoReturn,\n    Optional,\n    Pattern,\n    Set,\n    Sized,\n    Tuple,\n    Type,\n    TypedDict,\n    Union,\n    cast,\n)\n\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .abc import AbstractMatchInfo, AbstractRouter, AbstractView\nfrom .helpers import DEBUG\nfrom .http import HttpVersion11\nfrom .typedefs import Handler, PathLike\nfrom .web_exceptions import (\n    HTTPException,\n    HTTPExpectationFailed,\n    HTTPForbidden,\n    HTTPMethodNotAllowed,\n    HTTPNotFound,\n)\nfrom .web_fileresponse import FileResponse\nfrom .web_request import Request\nfrom .web_response import Response, StreamResponse\nfrom .web_routedef import AbstractRouteDef\n\n__all__ = (\n    \"UrlDispatcher\",\n    \"UrlMappingMatchInfo\",\n    \"AbstractResource\",\n    \"Resource\",\n    \"PlainResource\",\n    \"DynamicResource\",\n    \"AbstractRoute\",\n    \"ResourceRoute\",\n    \"StaticResource\",\n    \"View\",\n)"
    },
    {
      "chunk_id": 338,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class _InfoDict(TypedDict, total=False):\n    path: str\n\n    formatter: str\n    pattern: Pattern[str]\n\n    directory: Path\n    prefix: str\n    routes: Mapping[str, \"AbstractRoute\"]\n\n    app: \"Application\"\n\n    domain: str\n\n    rule: \"AbstractRuleMatching\"\n\n    http_exception: HTTPException\n\n\nclass AbstractResource(Sized, Iterable[\"AbstractRoute\"]):\n    def __init__(self, *, name: Optional[str] = None) -> None:\n        self._name = name\n\n    @property\n    def name(self) -> Optional[str]:\n        return self._name\n\n    @property\n    @abc.abstractmethod\n    def canonical(self) -> str:\n        \"\"\"Exposes the resource's canonical path.\n\n        For example '/foo/bar/{name}'\n\n        \"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    def url_for(self, **kwargs: str) -> URL:\n        \"\"\"Construct url for resource with additional params.\"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    async def resolve(self, request: Request) -> \"_Resolve\":\n        \"\"\"Resolve resource.\n\n        Return (UrlMappingMatchInfo, allowed_methods) pair.\n        \"\"\"\n\n    @abc.abstractmethod\n    def add_prefix(self, prefix: str) -> None:\n        \"\"\"Add a prefix to processed URLs.\n\n        Required for subapplications support.\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    def freeze(self) -> None:\n        pass\n\n    @abc.abstractmethod\n    def raw_match(self, path: str) -> bool:\n        \"\"\"Perform a raw match against path\"\"\""
    },
    {
      "chunk_id": 339,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class AbstractRoute(abc.ABC):\n    def __init__(\n        self,\n        method: str,\n        handler: Union[Handler, Type[AbstractView]],\n        *,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n        resource: Optional[AbstractResource] = None,\n    ) -> None:\n        if expect_handler is None:\n            expect_handler = _default_expect_handler\n\n        assert asyncio.iscoroutinefunction(\n            expect_handler\n        ), f\"Coroutine is expected, got {expect_handler!r}\"\n\n        method = method.upper()\n        if not HTTP_METHOD_RE.match(method):\n            raise ValueError(f\"{method} is not allowed HTTP method\")\n\n        if asyncio.iscoroutinefunction(handler):\n            pass\n        elif isinstance(handler, type) and issubclass(handler, AbstractView):\n            pass\n        else:\n            raise TypeError(\n                \"Only async functions are allowed as web-handlers \"\n                \", got {!r}\".format(handler)\n            )\n\n        self._method = method\n        self._handler = handler\n        self._expect_handler = expect_handler\n        self._resource = resource\n\n    @property\n    def method(self) -> str:\n        return self._method\n\n    @property\n    def handler(self) -> Handler:\n        return self._handler\n\n    @property\n    @abc.abstractmethod\n    def name(self) -> Optional[str]:\n        \"\"\"Optional route's name, always equals to resource's name.\"\"\"\n\n    @property\n    def resource(self) -> Optional[AbstractResource]:\n        return self._resource\n\n    @abc.abstractmethod\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @abc.abstractmethod\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        \"\"\"Construct url for route with additional params.\"\"\"\n\n    async def handle_expect_header(self, request: Request) -> Optional[StreamResponse]:\n        return await self._expect_handler(request)"
    },
    {
      "chunk_id": 340,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class UrlMappingMatchInfo(BaseDict, AbstractMatchInfo):\n\n    __slots__ = (\"_route\", \"_apps\", \"_current_app\", \"_frozen\")\n\n    def __init__(self, match_dict: Dict[str, str], route: AbstractRoute) -> None:\n        super().__init__(match_dict)\n        self._route = route\n        self._apps: List[Application] = []\n        self._current_app: Optional[Application] = None\n        self._frozen = False\n\n    @property\n    def handler(self) -> Handler:\n        return self._route.handler\n\n    @property\n    def route(self) -> AbstractRoute:\n        return self._route\n\n    @property\n    def expect_handler(self) -> \"_ExpectHandler\":\n        return self._route.handle_expect_header\n\n    @property\n    def http_exception(self) -> Optional[HTTPException]:\n        return None\n\n    def get_info(self) -> _InfoDict:  # type: ignore[override]\n        return self._route.get_info()\n\n    @property\n    def apps(self) -> Tuple[\"Application\", ...]:\n        return tuple(self._apps)\n\n    def add_app(self, app: \"Application\") -> None:\n        if self._frozen:\n            raise RuntimeError(\"Cannot change apps stack after .freeze() call\")\n        if self._current_app is None:\n            self._current_app = app\n        self._apps.insert(0, app)\n\n    @property\n    def current_app(self) -> \"Application\":\n        app = self._current_app\n        assert app is not None\n        return app\n\n    @current_app.setter\n    def current_app(self, app: \"Application\") -> None:\n        if DEBUG:  # pragma: no cover\n            if app not in self._apps:\n                raise RuntimeError(\n                    \"Expected one of the following apps {!r}, got {!r}\".format(\n                        self._apps, app\n                    )\n                )\n        self._current_app = app\n\n    def freeze(self) -> None:\n        self._frozen = True\n\n    def __repr__(self) -> str:\n        return f\"<MatchInfo {super().__repr__()}: {self._route}>\""
    },
    {
      "chunk_id": 341,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class MatchInfoError(UrlMappingMatchInfo):\n\n    __slots__ = (\"_exception\",)\n\n    def __init__(self, http_exception: HTTPException) -> None:\n        self._exception = http_exception\n        super().__init__({}, SystemRoute(self._exception))\n\n    @property\n    def http_exception(self) -> HTTPException:\n        return self._exception\n\n    def __repr__(self) -> str:\n        return \"<MatchInfoError {}: {}>\".format(\n            self._exception.status, self._exception.reason\n        )"
    },
    {
      "chunk_id": 342,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "async def _default_expect_handler(request: Request) -> None:\n    \"\"\"Default handler for Expect header.\n\n    Just send \"100 Continue\" to client.\n    raise HTTPExpectationFailed if value of header is not \"100-continue\"\n    \"\"\"\n    expect = request.headers.get(hdrs.EXPECT, \"\")\n    if request.version == HttpVersion11:\n        if expect.lower() == \"100-continue\":\n            await request.writer.write(b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n            # Reset output_size as we haven't started the main body yet.\n            request.writer.output_size = 0\n        else:\n            raise HTTPExpectationFailed(text=\"Unknown Expect: %s\" % expect)"
    },
    {
      "chunk_id": 343,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class Resource(AbstractResource):\n    def __init__(self, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        self._routes: Dict[str, ResourceRoute] = {}\n        self._any_route: Optional[ResourceRoute] = None\n        self._allowed_methods: Set[str] = set()\n\n    def add_route(\n        self,\n        method: str,\n        handler: Union[Type[AbstractView], Handler],\n        *,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n    ) -> \"ResourceRoute\":\n        if route := self._routes.get(method, self._any_route):\n            raise RuntimeError(\n                \"Added route will never be executed, \"\n                f\"method {route.method} is already \"\n                \"registered\"\n            )\n\n        route_obj = ResourceRoute(method, handler, self, expect_handler=expect_handler)\n        self.register_route(route_obj)\n        return route_obj\n\n    def register_route(self, route: \"ResourceRoute\") -> None:\n        assert isinstance(\n            route, ResourceRoute\n        ), f\"Instance of Route class is required, got {route!r}\"\n        if route.method == hdrs.METH_ANY:\n            self._any_route = route\n        self._allowed_methods.add(route.method)\n        self._routes[route.method] = route\n\n    async def resolve(self, request: Request) -> \"_Resolve\":\n        if (match_dict := self._match(request.rel_url.path_safe)) is None:\n            return None, set()\n        if route := self._routes.get(request.method, self._any_route):\n            return UrlMappingMatchInfo(match_dict, route), self._allowed_methods\n        return None, self._allowed_methods\n\n    @abc.abstractmethod\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        pass  # pragma: no cover\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[\"ResourceRoute\"]:\n        return iter(self._routes.values())\n\n    # TODO: implement all abstract methods"
    },
    {
      "chunk_id": 344,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class PlainResource(Resource):\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        assert not path or path.startswith(\"/\")\n        self._path = path\n\n    @property\n    def canonical(self) -> str:\n        return self._path\n\n    def freeze(self) -> None:\n        if not self._path:\n            self._path = \"/\"\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._path = prefix + self._path\n\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        # string comparison is about 10 times faster than regexp matching\n        if self._path == path:\n            return {}\n        return None\n\n    def raw_match(self, path: str) -> bool:\n        return self._path == path\n\n    def get_info(self) -> _InfoDict:\n        return {\"path\": self._path}\n\n    def url_for(self) -> URL:  # type: ignore[override]\n        return URL.build(path=self._path, encoded=True)\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"' \" if self.name is not None else \"\"\n        return f\"<PlainResource {name} {self._path}>\""
    },
    {
      "chunk_id": 345,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class DynamicResource(Resource):\n    DYN = re.compile(r\"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*)\\}\")\n    DYN_WITH_RE = re.compile(r\"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*):(?P<re>.+)\\}\")\n    GOOD = r\"[^{}/]+\"\n\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\n        super().__init__(name=name)\n        self._orig_path = path\n        pattern = \"\"\n        formatter = \"\"\n        for part in ROUTE_RE.split(path):\n            match = self.DYN.fullmatch(part)\n            if match:\n                pattern += \"(?P<{}>{})\".format(match.group(\"var\"), self.GOOD)\n                formatter += \"{\" + match.group(\"var\") + \"}\"\n                continue\n\n            match = self.DYN_WITH_RE.fullmatch(part)\n            if match:\n                pattern += \"(?P<{var}>{re})\".format(**match.groupdict())\n                formatter += \"{\" + match.group(\"var\") + \"}\"\n                continue\n\n            if \"{\" in part or \"}\" in part:\n                raise ValueError(f\"Invalid path '{path}'['{part}']\")\n\n            part = _requote_path(part)\n            formatter += part\n            pattern += re.escape(part)\n\n        try:\n            compiled = re.compile(pattern)\n        except re.error as exc:\n            raise ValueError(f\"Bad pattern '{pattern}': {exc}\") from None\n        assert compiled.pattern.startswith(PATH_SEP)\n        assert formatter.startswith(\"/\")\n        self._pattern = compiled\n        self._formatter = formatter\n\n    @property\n    def canonical(self) -> str:\n        return self._formatter\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._pattern = re.compile(re.escape(prefix) + self._pattern.pattern)\n        self._formatter = prefix + self._formatter\n\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\n        match = self._pattern.fullmatch(path)\n        if match is None:\n            return None\n        return {\n            key: _unquote_path_safe(value) for key, value in match.groupdict().items()\n        }\n\n    def raw_match(self, path: str) -> bool:\n        return self._orig_path == path\n\n    def get_info(self) -> _InfoDict:\n        return {\"formatter\": self._formatter, \"pattern\": self._pattern}\n\n    def url_for(self, **parts: str) -> URL:\n        url = self._formatter.format_map({k: _quote_path(v) for k, v in parts.items()})\n        return URL.build(path=url, encoded=True)\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"' \" if self.name is not None else \"\"\n        return \"<DynamicResource {name} {formatter}>\".format(\n            name=name, formatter=self._formatter\n        )"
    },
    {
      "chunk_id": 346,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class PrefixResource(AbstractResource):\n    def __init__(self, prefix: str, *, name: Optional[str] = None) -> None:\n        assert not prefix or prefix.startswith(\"/\"), prefix\n        assert prefix in (\"\", \"/\") or not prefix.endswith(\"/\"), prefix\n        super().__init__(name=name)\n        self._prefix = _requote_path(prefix)\n        self._prefix2 = self._prefix + \"/\"\n\n    @property\n    def canonical(self) -> str:\n        return self._prefix\n\n    def add_prefix(self, prefix: str) -> None:\n        assert prefix.startswith(\"/\")\n        assert not prefix.endswith(\"/\")\n        assert len(prefix) > 1\n        self._prefix = prefix + self._prefix\n        self._prefix2 = self._prefix + \"/\"\n\n    def raw_match(self, prefix: str) -> bool:\n        return False\n\n    # TODO: impl missing abstract methods"
    },
    {
      "chunk_id": 347,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class StaticResource(PrefixResource):\n    VERSION_KEY = \"v\"\n\n    def __init__(\n        self,\n        prefix: str,\n        directory: PathLike,\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n        chunk_size: int = 256 * 1024,\n        show_index: bool = False,\n        follow_symlinks: bool = False,\n        append_version: bool = False,\n    ) -> None:\n        super().__init__(prefix, name=name)\n        try:\n            directory = Path(directory).expanduser().resolve(strict=True)\n        except FileNotFoundError as error:\n            raise ValueError(f\"'{directory}' does not exist\") from error\n        if not directory.is_dir():\n            raise ValueError(f\"'{directory}' is not a directory\")\n        self._directory = directory\n        self._show_index = show_index\n        self._chunk_size = chunk_size\n        self._follow_symlinks = follow_symlinks\n        self._expect_handler = expect_handler\n        self._append_version = append_version\n\n        self._routes = {\n            \"GET\": ResourceRoute(\n                \"GET\", self._handle, self, expect_handler=expect_handler\n            ),\n            \"HEAD\": ResourceRoute(\n                \"HEAD\", self._handle, self, expect_handler=expect_handler\n            ),\n        }\n        self._allowed_methods = set(self._routes)\n\n    def url_for(  # type: ignore[override]\n        self,\n        *,\n        filename: PathLike,\n        append_version: Optional[bool] = None,\n    ) -> URL:\n        if append_version is None:\n            append_version = self._append_version\n        filename = str(filename).lstrip(\"/\")\n\n        url = URL.build(path=self._prefix, encoded=True)\n        # filename is not encoded\n        url = url / filename\n\n        if append_version:\n            unresolved_path = self._directory.joinpath(filename)\n            try:\n                if self._follow_symlinks:\n                    normalized_path = Path(os.path.normpath(unresolved_path))\n                    normalized_path.relative_to(self._directory)\n                    filepath = normalized_path.resolve()\n                else:\n                    filepath = unresolved_path.resolve()\n                    filepath.relative_to(self._directory)\n            except (ValueError, FileNotFoundError):\n                # ValueError for case when path point to symlink\n                # with follow_symlinks is False\n                return url  # relatively safe\n            if filepath.is_file():\n                # TODO cache file content\n                # with file watcher for cache invalidation\n                with filepath.open(\"rb\") as f:\n                    file_bytes = f.read()\n                h = self._get_file_hash(file_bytes)\n                url = url.with_query({self.VERSION_KEY: h})\n                return url\n        return url\n\n    @staticmethod\n    def _get_file_hash(byte_array: bytes) -> str:\n        m = hashlib.sha256()  # todo sha256 can be configurable param\n        m.update(byte_array)\n        b64 = base64.urlsafe_b64encode(m.digest())\n        return b64.decode(\"ascii\")\n\n    def get_info(self) -> _InfoDict:\n        return {\n            \"directory\": self._directory,\n            \"prefix\": self._prefix,\n            \"routes\": self._routes,\n        }\n\n    def set_options_route(self, handler: Handler) -> None:\n        if \"OPTIONS\" in self._routes:\n            raise RuntimeError(\"OPTIONS route was set already\")\n        self._routes[\"OPTIONS\"] = ResourceRoute(\n            \"OPTIONS\", handler, self, expect_handler=self._expect_handler\n        )\n        self._allowed_methods.add(\"OPTIONS\")\n\n    async def resolve(self, request: Request) -> \"_Resolve\":\n        path = request.rel_url.path_safe\n        method = request.method\n        if not path.startswith(self._prefix2) and path != self._prefix:\n            return None, set()\n\n        allowed_methods = self._allowed_methods\n        if method not in allowed_methods:\n            return None, allowed_methods\n\n        match_dict = {\"filename\": _unquote_path_safe(path[len(self._prefix) + 1 :])}\n        return (UrlMappingMatchInfo(match_dict, self._routes[method]), allowed_methods)\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        return iter(self._routes.values())\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        rel_url = request.match_info[\"filename\"]\n        filename = Path(rel_url)\n        if filename.anchor:\n            # rel_url is an absolute name like\n            # /static/\\\\machine_name\\c$ or /static/D:\\path\n            # where the static dir is totally different\n            raise HTTPForbidden()\n\n        unresolved_path = self._directory.joinpath(filename)\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(\n            None, self._resolve_path_to_response, unresolved_path\n        )\n\n    def _resolve_path_to_response(self, unresolved_path: Path) -> StreamResponse:\n        \"\"\"Take the unresolved path and query the file system to form a response.\"\"\"\n        # Check for access outside the root directory. For follow symlinks, URI\n        # cannot traverse out, but symlinks can. Otherwise, no access outside\n        # root is permitted.\n        try:\n            if self._follow_symlinks:\n                normalized_path = Path(os.path.normpath(unresolved_path))\n                normalized_path.relative_to(self._directory)\n                file_path = normalized_path.resolve()\n            else:\n                file_path = unresolved_path.resolve()\n                file_path.relative_to(self._directory)\n        except (ValueError, *CIRCULAR_SYMLINK_ERROR) as error:\n            # ValueError is raised for the relative check. Circular symlinks\n            # raise here on resolving for python < 3.13.\n            raise HTTPNotFound() from error\n\n        # if path is a directory, return the contents if permitted. Note the\n        # directory check will raise if a segment is not readable.\n        try:\n            if file_path.is_dir():\n                if self._show_index:\n                    return Response(\n                        text=self._directory_as_html(file_path),\n                        content_type=\"text/html\",\n                    )\n                else:\n                    raise HTTPForbidden()\n        except PermissionError as error:\n            raise HTTPForbidden() from error\n\n        # Return the file response, which handles all other checks.\n        return FileResponse(file_path, chunk_size=self._chunk_size)\n\n    def _directory_as_html(self, dir_path: Path) -> str:\n        \"\"\"returns directory's index as html.\"\"\"\n        assert dir_path.is_dir()\n\n        relative_path_to_dir = dir_path.relative_to(self._directory).as_posix()\n        index_of = f\"Index of /{html_escape(relative_path_to_dir)}\"\n        h1 = f\"<h1>{index_of}</h1>\"\n\n        index_list = []\n        dir_index = dir_path.iterdir()\n        for _file in sorted(dir_index):\n            # show file url as relative to static path\n            rel_path = _file.relative_to(self._directory).as_posix()\n            quoted_file_url = _quote_path(f\"{self._prefix}/{rel_path}\")\n\n            # if file is a directory, add '/' to the end of the name\n            if _file.is_dir():\n                file_name = f\"{_file.name}/\"\n            else:\n                file_name = _file.name\n\n            index_list.append(\n                f'<li><a href=\"{quoted_file_url}\">{html_escape(file_name)}</a></li>'\n            )\n        ul = \"<ul>\\n{}\\n</ul>\".format(\"\\n\".join(index_list))\n        body = f\"<body>\\n{h1}\\n{ul}\\n</body>\"\n\n        head_str = f\"<head>\\n<title>{index_of}</title>\\n</head>\"\n        html = f\"<html>\\n{head_str}\\n{body}\\n</html>\"\n\n        return html\n\n    def __repr__(self) -> str:\n        name = \"'\" + self.name + \"'\" if self.name is not None else \"\"\n        return \"<StaticResource {name} {path} -> {directory!r}>\".format(\n            name=name, path=self._prefix, directory=self._directory\n        )"
    },
    {
      "chunk_id": 348,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class PrefixedSubAppResource(PrefixResource):\n    def __init__(self, prefix: str, app: \"Application\") -> None:\n        super().__init__(prefix)\n        self._app = app\n        self._add_prefix_to_resources(prefix)\n\n    def add_prefix(self, prefix: str) -> None:\n        super().add_prefix(prefix)\n        self._add_prefix_to_resources(prefix)\n\n    def _add_prefix_to_resources(self, prefix: str) -> None:\n        router = self._app.router\n        for resource in router.resources():\n            # Since the canonical path of a resource is about\n            # to change, we need to unindex it and then reindex\n            router.unindex_resource(resource)\n            resource.add_prefix(prefix)\n            router.index_resource(resource)\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        raise RuntimeError(\".url_for() is not supported by sub-application root\")\n\n    def get_info(self) -> _InfoDict:\n        return {\"app\": self._app, \"prefix\": self._prefix}\n\n    async def resolve(self, request: Request) -> \"_Resolve\":\n        match_info = await self._app.router.resolve(request)\n        match_info.add_app(self._app)\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\n            methods = match_info.http_exception.allowed_methods\n        else:\n            methods = set()\n        return match_info, methods\n\n    def __len__(self) -> int:\n        return len(self._app.router.routes())\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        return iter(self._app.router.routes())\n\n    def __repr__(self) -> str:\n        return \"<PrefixedSubAppResource {prefix} -> {app!r}>\".format(\n            prefix=self._prefix, app=self._app\n        )"
    },
    {
      "chunk_id": 349,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class AbstractRuleMatching(abc.ABC):\n    @abc.abstractmethod  # pragma: no branch\n    async def match(self, request: Request) -> bool:\n        \"\"\"Return bool if the request satisfies the criteria\"\"\"\n\n    @abc.abstractmethod  # pragma: no branch\n    def get_info(self) -> _InfoDict:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @property\n    @abc.abstractmethod  # pragma: no branch\n    def canonical(self) -> str:\n        \"\"\"Return a str\"\"\""
    },
    {
      "chunk_id": 350,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class Domain(AbstractRuleMatching):\n    re_part = re.compile(r\"(?!-)[a-z\\d-]{1,63}(?<!-)\")\n\n    def __init__(self, domain: str) -> None:\n        super().__init__()\n        self._domain = self.validation(domain)\n\n    @property\n    def canonical(self) -> str:\n        return self._domain\n\n    def validation(self, domain: str) -> str:\n        if not isinstance(domain, str):\n            raise TypeError(\"Domain must be str\")\n        domain = domain.rstrip(\".\").lower()\n        if not domain:\n            raise ValueError(\"Domain cannot be empty\")\n        elif \"://\" in domain:\n            raise ValueError(\"Scheme not supported\")\n        url = URL(\"http://\" + domain)\n        assert url.raw_host is not None\n        if not all(self.re_part.fullmatch(x) for x in url.raw_host.split(\".\")):\n            raise ValueError(\"Domain not valid\")\n        if url.port == 80:\n            return url.raw_host\n        return f\"{url.raw_host}:{url.port}\"\n\n    async def match(self, request: Request) -> bool:\n        host = request.headers.get(hdrs.HOST)\n        if not host:\n            return False\n        return self.match_domain(host)\n\n    def match_domain(self, host: str) -> bool:\n        return host.lower() == self._domain\n\n    def get_info(self) -> _InfoDict:\n        return {\"domain\": self._domain}"
    },
    {
      "chunk_id": 351,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class MaskDomain(Domain):\n    re_part = re.compile(r\"(?!-)[a-z\\d\\*-]{1,63}(?<!-)\")\n\n    def __init__(self, domain: str) -> None:\n        super().__init__(domain)\n        mask = self._domain.replace(\".\", r\"\\.\").replace(\"*\", \".*\")\n        self._mask = re.compile(mask)\n\n    @property\n    def canonical(self) -> str:\n        return self._mask.pattern\n\n    def match_domain(self, host: str) -> bool:\n        return self._mask.fullmatch(host) is not None"
    },
    {
      "chunk_id": 352,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class MatchedSubAppResource(PrefixedSubAppResource):\n    def __init__(self, rule: AbstractRuleMatching, app: \"Application\") -> None:\n        AbstractResource.__init__(self)\n        self._prefix = \"\"\n        self._app = app\n        self._rule = rule\n\n    @property\n    def canonical(self) -> str:\n        return self._rule.canonical\n\n    def get_info(self) -> _InfoDict:\n        return {\"app\": self._app, \"rule\": self._rule}\n\n    async def resolve(self, request: Request) -> \"_Resolve\":\n        if not await self._rule.match(request):\n            return None, set()\n        match_info = await self._app.router.resolve(request)\n        match_info.add_app(self._app)\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\n            methods = match_info.http_exception.allowed_methods\n        else:\n            methods = set()\n        return match_info, methods\n\n    def __repr__(self) -> str:\n        return f\"<MatchedSubAppResource -> {self._app!r}>\""
    },
    {
      "chunk_id": 353,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class ResourceRoute(AbstractRoute):\n    \"\"\"A route with resource\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        handler: Union[Handler, Type[AbstractView]],\n        resource: AbstractResource,\n        *,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n    ) -> None:\n        super().__init__(\n            method, handler, expect_handler=expect_handler, resource=resource\n        )\n\n    def __repr__(self) -> str:\n        return \"<ResourceRoute [{method}] {resource} -> {handler!r}\".format(\n            method=self.method, resource=self._resource, handler=self.handler\n        )\n\n    @property\n    def name(self) -> Optional[str]:\n        if self._resource is None:\n            return None\n        return self._resource.name\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        \"\"\"Construct url for route with additional params.\"\"\"\n        assert self._resource is not None\n        return self._resource.url_for(*args, **kwargs)\n\n    def get_info(self) -> _InfoDict:\n        assert self._resource is not None\n        return self._resource.get_info()"
    },
    {
      "chunk_id": 354,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class SystemRoute(AbstractRoute):\n    def __init__(self, http_exception: HTTPException) -> None:\n        super().__init__(hdrs.METH_ANY, self._handle)\n        self._http_exception = http_exception\n\n    def url_for(self, *args: str, **kwargs: str) -> URL:\n        raise RuntimeError(\".url_for() is not allowed for SystemRoute\")\n\n    @property\n    def name(self) -> Optional[str]:\n        return None\n\n    def get_info(self) -> _InfoDict:\n        return {\"http_exception\": self._http_exception}\n\n    async def _handle(self, request: Request) -> StreamResponse:\n        raise self._http_exception\n\n    @property\n    def status(self) -> int:\n        return self._http_exception.status\n\n    @property\n    def reason(self) -> str:\n        return self._http_exception.reason\n\n    def __repr__(self) -> str:\n        return \"<SystemRoute {self.status}: {self.reason}>\".format(self=self)"
    },
    {
      "chunk_id": 355,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class View(AbstractView):\n    async def _iter(self) -> StreamResponse:\n        if self.request.method not in hdrs.METH_ALL:\n            self._raise_allowed_methods()\n        method: Optional[Callable[[], Awaitable[StreamResponse]]] = getattr(\n            self, self.request.method.lower(), None\n        )\n        if method is None:\n            self._raise_allowed_methods()\n        return await method()\n\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\n        return self._iter().__await__()\n\n    def _raise_allowed_methods(self) -> NoReturn:\n        allowed_methods = {m for m in hdrs.METH_ALL if hasattr(self, m.lower())}\n        raise HTTPMethodNotAllowed(self.request.method, allowed_methods)"
    },
    {
      "chunk_id": 356,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class ResourcesView(Sized, Iterable[AbstractResource], Container[AbstractResource]):\n    def __init__(self, resources: List[AbstractResource]) -> None:\n        self._resources = resources\n\n    def __len__(self) -> int:\n        return len(self._resources)\n\n    def __iter__(self) -> Iterator[AbstractResource]:\n        yield from self._resources\n\n    def __contains__(self, resource: object) -> bool:\n        return resource in self._resources"
    },
    {
      "chunk_id": 357,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class RoutesView(Sized, Iterable[AbstractRoute], Container[AbstractRoute]):\n    def __init__(self, resources: List[AbstractResource]):\n        self._routes: List[AbstractRoute] = []\n        for resource in resources:\n            for route in resource:\n                self._routes.append(route)\n\n    def __len__(self) -> int:\n        return len(self._routes)\n\n    def __iter__(self) -> Iterator[AbstractRoute]:\n        yield from self._routes\n\n    def __contains__(self, route: object) -> bool:\n        return route in self._routes"
    },
    {
      "chunk_id": 358,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "class UrlDispatcher(AbstractRouter, Mapping[str, AbstractResource]):\n    NAME_SPLIT_RE = re.compile(r\"[.:-]\")\n    HTTP_NOT_FOUND = HTTPNotFound()\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._resources: List[AbstractResource] = []\n        self._named_resources: Dict[str, AbstractResource] = {}\n        self._resource_index: dict[str, list[AbstractResource]] = {}\n        self._matched_sub_app_resources: List[MatchedSubAppResource] = []\n\n    async def resolve(self, request: Request) -> UrlMappingMatchInfo:\n        resource_index = self._resource_index\n        allowed_methods: Set[str] = set()\n\n        # Walk the url parts looking for candidates. We walk the url backwards\n        # to ensure the most explicit match is found first. If there are multiple\n        # candidates for a given url part because there are multiple resources\n        # registered for the same canonical path, we resolve them in a linear\n        # fashion to ensure registration order is respected.\n        url_part = request.rel_url.path_safe\n        while url_part:\n            for candidate in resource_index.get(url_part, ()):\n                match_dict, allowed = await candidate.resolve(request)\n                if match_dict is not None:\n                    return match_dict\n                else:\n                    allowed_methods |= allowed\n            if url_part == \"/\":\n                break\n            url_part = url_part.rpartition(\"/\")[0] or \"/\"\n\n        #\n        # We didn't find any candidates, so we'll try the matched sub-app\n        # resources which we have to walk in a linear fashion because they\n        # have regex/wildcard match rules and we cannot index them.\n        #\n        # For most cases we do not expect there to be many of these since\n        # currently they are only added by `add_domain`\n        #\n        for resource in self._matched_sub_app_resources:\n            match_dict, allowed = await resource.resolve(request)\n            if match_dict is not None:\n                return match_dict\n            else:\n                allowed_methods |= allowed\n\n        if allowed_methods:\n            return MatchInfoError(HTTPMethodNotAllowed(request.method, allowed_methods))\n\n        return MatchInfoError(self.HTTP_NOT_FOUND)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._named_resources)\n\n    def __len__(self) -> int:\n        return len(self._named_resources)\n\n    def __contains__(self, resource: object) -> bool:\n        return resource in self._named_resources\n\n    def __getitem__(self, name: str) -> AbstractResource:\n        return self._named_resources[name]\n\n    def resources(self) -> ResourcesView:\n        return ResourcesView(self._resources)\n\n    def routes(self) -> RoutesView:\n        return RoutesView(self._resources)\n\n    def named_resources(self) -> Mapping[str, AbstractResource]:\n        return MappingProxyType(self._named_resources)\n\n    def register_resource(self, resource: AbstractResource) -> None:\n        assert isinstance(\n            resource, AbstractResource\n        ), f\"Instance of AbstractResource class is required, got {resource!r}\"\n        if self.frozen:\n            raise RuntimeError(\"Cannot register a resource into frozen router.\")\n\n        name = resource.name\n\n        if name is not None:\n            parts = self.NAME_SPLIT_RE.split(name)\n            for part in parts:\n                if keyword.iskeyword(part):\n                    raise ValueError(\n                        f\"Incorrect route name {name!r}, \"\n                        \"python keywords cannot be used \"\n                        \"for route name\"\n                    )\n                if not part.isidentifier():\n                    raise ValueError(\n                        \"Incorrect route name {!r}, \"\n                        \"the name should be a sequence of \"\n                        \"python identifiers separated \"\n                        \"by dash, dot or column\".format(name)\n                    )\n            if name in self._named_resources:\n                raise ValueError(\n                    \"Duplicate {!r}, \"\n                    \"already handled by {!r}\".format(name, self._named_resources[name])\n                )\n            self._named_resources[name] = resource\n        self._resources.append(resource)\n\n        if isinstance(resource, MatchedSubAppResource):\n            # We cannot index match sub-app resources because they have match rules\n            self._matched_sub_app_resources.append(resource)\n        else:\n            self.index_resource(resource)\n\n    def _get_resource_index_key(self, resource: AbstractResource) -> str:\n        \"\"\"Return a key to index the resource in the resource index.\"\"\"\n        if \"{\" in (index_key := resource.canonical):\n            # strip at the first { to allow for variables, and than\n            # rpartition at / to allow for variable parts in the path\n            # For example if the canonical path is `/core/locations{tail:.*}`\n            # the index key will be `/core` since index is based on the\n            # url parts split by `/`\n            index_key = index_key.partition(\"{\")[0].rpartition(\"/\")[0]\n        return index_key.rstrip(\"/\") or \"/\"\n\n    def index_resource(self, resource: AbstractResource) -> None:\n        \"\"\"Add a resource to the resource index.\"\"\"\n        resource_key = self._get_resource_index_key(resource)\n        # There may be multiple resources for a canonical path\n        # so we keep them in a list to ensure that registration\n        # order is respected.\n        self._resource_index.setdefault(resource_key, []).append(resource)\n\n    def unindex_resource(self, resource: AbstractResource) -> None:\n        \"\"\"Remove a resource from the resource index.\"\"\"\n        resource_key = self._get_resource_index_key(resource)\n        self._resource_index[resource_key].remove(resource)\n\n    def add_resource(self, path: str, *, name: Optional[str] = None) -> Resource:\n        if path and not path.startswith(\"/\"):\n            raise ValueError(\"path should be started with / or be empty\")\n        # Reuse last added resource if path and name are the same\n        if self._resources:\n            resource = self._resources[-1]\n            if resource.name == name and resource.raw_match(path):\n                return cast(Resource, resource)\n        if not (\"{\" in path or \"}\" in path or ROUTE_RE.search(path)):\n            resource = PlainResource(path, name=name)\n            self.register_resource(resource)\n            return resource\n        resource = DynamicResource(path, name=name)\n        self.register_resource(resource)\n        return resource\n\n    def add_route(\n        self,\n        method: str,\n        path: str,\n        handler: Union[Handler, Type[AbstractView]],\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n    ) -> AbstractRoute:\n        resource = self.add_resource(path, name=name)\n        return resource.add_route(method, handler, expect_handler=expect_handler)\n\n    def add_static(\n        self,\n        prefix: str,\n        path: PathLike,\n        *,\n        name: Optional[str] = None,\n        expect_handler: Optional[\"_ExpectHandler\"] = None,\n        chunk_size: int = 256 * 1024,\n        show_index: bool = False,\n        follow_symlinks: bool = False,\n        append_version: bool = False,\n    ) -> StaticResource:\n        \"\"\"Add static files view.\n\n        prefix - url prefix\n        path - folder with files\n\n        \"\"\"\n        assert prefix.startswith(\"/\")\n        if prefix.endswith(\"/\"):\n            prefix = prefix[:-1]\n        resource = StaticResource(\n            prefix,\n            path,\n            name=name,\n            expect_handler=expect_handler,\n            chunk_size=chunk_size,\n            show_index=show_index,\n            follow_symlinks=follow_symlinks,\n            append_version=append_version,\n        )\n        self.register_resource(resource)\n        return resource\n\n    def add_head(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method HEAD.\"\"\"\n        return self.add_route(hdrs.METH_HEAD, path, handler, **kwargs)\n\n    def add_options(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method OPTIONS.\"\"\"\n        return self.add_route(hdrs.METH_OPTIONS, path, handler, **kwargs)\n\n    def add_get(\n        self,\n        path: str,\n        handler: Handler,\n        *,\n        name: Optional[str] = None,\n        allow_head: bool = True,\n        **kwargs: Any,\n    ) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method GET.\n\n        If allow_head is true, another\n        route is added allowing head requests to the same endpoint.\n        \"\"\"\n        resource = self.add_resource(path, name=name)\n        if allow_head:\n            resource.add_route(hdrs.METH_HEAD, handler, **kwargs)\n        return resource.add_route(hdrs.METH_GET, handler, **kwargs)\n\n    def add_post(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method POST.\"\"\"\n        return self.add_route(hdrs.METH_POST, path, handler, **kwargs)\n\n    def add_put(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method PUT.\"\"\"\n        return self.add_route(hdrs.METH_PUT, path, handler, **kwargs)\n\n    def add_patch(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method PATCH.\"\"\"\n        return self.add_route(hdrs.METH_PATCH, path, handler, **kwargs)\n\n    def add_delete(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with method DELETE.\"\"\"\n        return self.add_route(hdrs.METH_DELETE, path, handler, **kwargs)\n\n    def add_view(\n        self, path: str, handler: Type[AbstractView], **kwargs: Any\n    ) -> AbstractRoute:\n        \"\"\"Shortcut for add_route with ANY methods for a class-based view.\"\"\"\n        return self.add_route(hdrs.METH_ANY, path, handler, **kwargs)\n\n    def freeze(self) -> None:\n        super().freeze()\n        for resource in self._resources:\n            resource.freeze()\n\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\n        \"\"\"Append routes to route table.\n\n        Parameter should be a sequence of RouteDef objects.\n\n        Returns a list of registered AbstractRoute instances.\n        \"\"\"\n        registered_routes = []\n        for route_def in routes:\n            registered_routes.extend(route_def.register(self))\n        return registered_routes"
    },
    {
      "chunk_id": 359,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "def _quote_path(value: str) -> str:\n    return URL.build(path=value, encoded=False).raw_path"
    },
    {
      "chunk_id": 360,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "def _unquote_path_safe(value: str) -> str:\n    if \"%\" not in value:\n        return value\n    return value.replace(\"%2F\", \"/\").replace(\"%25\", \"%\")"
    },
    {
      "chunk_id": 361,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_urldispatcher.py",
      "content": "def _requote_path(value: str) -> str:\n    # Quote non-ascii characters and other characters which must be quoted,\n    # but preserve existing %-sequences.\n    result = _quote_path(value)\n    if \"%\" in value:\n        result = result.replace(\"%25\", \"%\")\n    return result"
    },
    {
      "chunk_id": 362,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "```python"
    },
    {
      "chunk_id": 363,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "\"\"\"Utilities shared by tests.\"\"\"\n\nimport asyncio\nimport contextlib\nimport gc\nimport inspect\nimport ipaddress\nimport os\nimport socket\nimport sys\nfrom abc import ABC, abstractmethod\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Generic,\n    Iterator,\n    List,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nfrom unittest import IsolatedAsyncioTestCase, mock\n\nfrom aiosignal import Signal\nfrom multidict import CIMultiDict, CIMultiDictProxy\nfrom yarl import URL\n\nimport aiohttp\nfrom aiohttp.client import (\n    _RequestContextManager,\n    _RequestOptions,\n    _WSRequestContextManager,\n)\n\nfrom . import ClientSession, hdrs\nfrom .abc import AbstractCookieJar, AbstractStreamWriter\nfrom .client_reqrep import ClientResponse\nfrom .client_ws import ClientWebSocketResponse\nfrom .helpers import sentinel\nfrom .http import HttpVersion, RawRequestMessage\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .typedefs import LooseHeaders, StrOrURL\nfrom .web import (\n    Application,\n    AppRunner,\n    BaseRequest,\n    BaseRunner,\n    Request,\n    RequestHandler,\n    Server,\n    ServerRunner,\n    SockSite,\n    UrlMappingMatchInfo,\n)\nfrom .web_protocol import _RequestHandler\n\nif TYPE_CHECKING:\n    from ssl import SSLContext\nelse:\n    SSLContext = None\n\nif sys.version_info >= (3, 11) and TYPE_CHECKING:\n    from typing import Unpack\n\nif sys.version_info >= (3, 11):\n    from typing import Self\nelse:\n    Self = Any\n\n_ApplicationNone = TypeVar(\"_ApplicationNone\", Application, None)\n_Request = TypeVar(\"_Request\", bound=BaseRequest)\n\nREUSE_ADDRESS = os.name == \"posix\" and sys.platform != \"cygwin\""
    },
    {
      "chunk_id": 364,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def get_unused_port_socket(\n    host: str, family: socket.AddressFamily = socket.AF_INET\n) -> socket.socket:\n    return get_port_socket(host, 0, family)"
    },
    {
      "chunk_id": 365,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def get_port_socket(\n    host: str, port: int, family: socket.AddressFamily = socket.AF_INET\n) -> socket.socket:\n    s = socket.socket(family, socket.SOCK_STREAM)\n    if REUSE_ADDRESS:\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    s.bind((host, port))\n    return s"
    },
    {
      "chunk_id": 366,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def unused_port() -> int:\n    \"\"\"Return a port that is unused on the current host.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"127.0.0.1\", 0))\n        return cast(int, s.getsockname()[1])"
    },
    {
      "chunk_id": 367,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "class BaseTestServer(ABC, Generic[_Request]):\n    __test__ = False\n\n    def __init__(\n        self,\n        *,\n        scheme: str = \"\",\n        host: str = \"127.0.0.1\",\n        port: Optional[int] = None,\n        skip_url_asserts: bool = False,\n        socket_factory: Callable[\n            [str, int, socket.AddressFamily], socket.socket\n        ] = get_port_socket,\n        **kwargs: Any,\n    ) -> None:\n        self.runner: Optional[BaseRunner[_Request]] = None\n        self._root: Optional[URL] = None\n        self.host = host\n        self.port = port or 0\n        self._closed = False\n        self.scheme = scheme\n        self.skip_url_asserts = skip_url_asserts\n        self.socket_factory = socket_factory\n\n    async def start_server(self, **kwargs: Any) -> None:\n        if self.runner:\n            return\n        self._ssl = kwargs.pop(\"ssl\", None)\n        self.runner = await self._make_runner(handler_cancellation=True, **kwargs)\n        await self.runner.setup()\n        absolute_host = self.host\n        try:\n            version = ipaddress.ip_address(self.host).version\n        except ValueError:\n            version = 4\n        if version == 6:\n            absolute_host = f\"[{self.host}]\"\n        family = socket.AF_INET6 if version == 6 else socket.AF_INET\n        _sock = self.socket_factory(self.host, self.port, family)\n        self.host, self.port = _sock.getsockname()[:2]\n        site = SockSite(self.runner, sock=_sock, ssl_context=self._ssl)\n        await site.start()\n        server = site._server\n        assert server is not None\n        sockets = server.sockets  # type: ignore[attr-defined]\n        assert sockets is not None\n        self.port = sockets[0].getsockname()[1]\n        if not self.scheme:\n            self.scheme = \"https\" if self._ssl else \"http\"\n        self._root = URL(f\"{self.scheme}://{absolute_host}:{self.port}\")\n\n    @abstractmethod  # pragma: no cover\n    async def _make_runner(self, **kwargs: Any) -> BaseRunner[_Request]:\n        pass\n\n    def make_url(self, path: StrOrURL) -> URL:\n        assert self._root is not None\n        url = URL(path)\n        if not self.skip_url_asserts:\n            assert not url.absolute\n            return self._root.join(url)\n        else:\n            return URL(str(self._root) + str(path))\n\n    @property\n    def started(self) -> bool:\n        return self.runner is not None\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def handler(self) -> Server[_Request]:\n        runner = self.runner\n        assert runner is not None\n        assert runner.server is not None\n        return runner.server\n\n    async def close(self) -> None:\n        \"\"\"Close all fixtures created by the test client.\n\n        After that point, the TestClient is no longer usable.\n\n        This is an idempotent function: running close multiple times\n        will not have any additional effects.\n\n        close is also run when the object is garbage collected, and on\n        exit when used as a context manager.\n\n        \"\"\"\n        if self.started and not self.closed:\n            assert self.runner is not None\n            await self.runner.cleanup()\n            self._root = None\n            self.port = 0\n            self._closed = True\n\n    async def __aenter__(self) -> Self:\n        await self.start_server()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_value: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -> None:\n        await self.close()"
    },
    {
      "chunk_id": 368,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "class TestServer(BaseTestServer[Request]):\n    def __init__(\n        self,\n        app: Application,\n        *,\n        scheme: str = \"\",\n        host: str = \"127.0.0.1\",\n        port: Optional[int] = None,\n        **kwargs: Any,\n    ):\n        self.app = app\n        super().__init__(scheme=scheme, host=host, port=port, **kwargs)\n\n    async def _make_runner(self, **kwargs: Any) -> AppRunner:\n        return AppRunner(self.app, **kwargs)"
    },
    {
      "chunk_id": 369,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "class RawTestServer(BaseTestServer[BaseRequest]):\n    def __init__(\n        self,\n        handler: _RequestHandler[BaseRequest],\n        *,\n        scheme: str = \"\",\n        host: str = \"127.0.0.1\",\n        port: Optional[int] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._handler = handler\n        super().__init__(scheme=scheme, host=host, port=port, **kwargs)\n\n    async def _make_runner(self, **kwargs: Any) -> ServerRunner:\n        srv = Server(self._handler, **kwargs)\n        return ServerRunner(srv, **kwargs)"
    },
    {
      "chunk_id": 370,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "class TestClient(Generic[_Request, _ApplicationNone]):\n    \"\"\"\n    A test client implementation.\n\n    To write functional tests for aiohttp based servers.\n\n    \"\"\"\n\n    __test__ = False\n\n    @overload\n    def __init__(\n        self: \"TestClient[Request, Application]\",\n        server: TestServer,\n        *,\n        cookie_jar: Optional[AbstractCookieJar] = None,\n        **kwargs: Any,\n    ) -> None: ...\n    @overload\n    def __init__(\n        self: \"TestClient[_Request, None]\",\n        server: BaseTestServer[_Request],\n        *,\n        cookie_jar: Optional[AbstractCookieJar] = None,\n        **kwargs: Any,\n    ) -> None: ...\n    def __init__(  # type: ignore[misc]\n        self,\n        server: BaseTestServer[_Request],\n        *,\n        cookie_jar: Optional[AbstractCookieJar] = None,\n        **kwargs: Any,\n    ) -> None:\n        if not isinstance(server, BaseTestServer):\n            raise TypeError(\n                \"server must be TestServer instance, found type: %r\" % type(server)\n            )\n        self._server = server\n        if cookie_jar is None:\n            cookie_jar = aiohttp.CookieJar(unsafe=True)\n        self._session = ClientSession(cookie_jar=cookie_jar, **kwargs)\n        self._session._retry_connection = False\n        self._closed = False\n        self._responses: List[ClientResponse] = []\n        self._websockets: List[ClientWebSocketResponse] = []\n\n    async def start_server(self) -> None:\n        await self._server.start_server()\n\n    @property\n    def scheme(self) -> Union[str, object]:\n        return self._server.scheme\n\n    @property\n    def host(self) -> str:\n        return self._server.host\n\n    @property\n    def port(self) -> int:\n        return self._server.port\n\n    @property\n    def server(self) -> BaseTestServer[_Request]:\n        return self._server\n\n    @property\n    def app(self) -> _ApplicationNone:\n        return getattr(self._server, \"app\", None)  # type: ignore[return-value]\n\n    @property\n    def session(self) -> ClientSession:\n        \"\"\"An internal aiohttp.ClientSession.\n\n        Unlike the methods on the TestClient, client session requests\n        do not automatically include the host in the url queried, and\n        will require an absolute path to the resource.\n\n        \"\"\"\n        return self._session\n\n    def make_url(self, path: StrOrURL) -> URL:\n        return self._server.make_url(path)\n\n    async def _request(\n        self, method: str, path: StrOrURL, **kwargs: Any\n    ) -> ClientResponse:\n        resp = await self._session.request(method, self.make_url(path), **kwargs)\n        self._responses.append(resp)\n        return resp\n\n    if sys.version_info >= (3, 11) and TYPE_CHECKING:\n\n        def request(\n            self, method: str, path: StrOrURL, **kwargs: Unpack[_RequestOptions]\n        ) -> _RequestContextManager: ...\n\n        def get(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def options(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def head(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def post(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def put(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def patch(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n        def delete(\n            self,\n            path: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> _RequestContextManager: ...\n\n    else:\n\n        def request(\n            self, method: str, path: StrOrURL, **kwargs: Any\n        ) -> _RequestContextManager:\n            \"\"\"Routes a request to tested http server.\n\n            The interface is identical to aiohttp.ClientSession.request,\n            except the loop kwarg is overridden by the instance used by the\n            test server.\n\n            \"\"\"\n            return _RequestContextManager(self._request(method, path, **kwargs))\n\n        def get(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP GET request.\"\"\"\n            return _RequestContextManager(self._request(hdrs.METH_GET, path, **kwargs))\n\n        def post(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP POST request.\"\"\"\n            return _RequestContextManager(self._request(hdrs.METH_POST, path, **kwargs))\n\n        def options(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP OPTIONS request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_OPTIONS, path, **kwargs)\n            )\n\n        def head(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP HEAD request.\"\"\"\n            return _RequestContextManager(self._request(hdrs.METH_HEAD, path, **kwargs))\n\n        def put(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP PUT request.\"\"\"\n            return _RequestContextManager(self._request(hdrs.METH_PUT, path, **kwargs))\n\n        def patch(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP PATCH request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_PATCH, path, **kwargs)\n            )\n\n        def delete(self, path: StrOrURL, **kwargs: Any) -> _RequestContextManager:\n            \"\"\"Perform an HTTP PATCH request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_DELETE, path, **kwargs)\n            )\n\n    def ws_connect(self, path: StrOrURL, **kwargs: Any) -> _WSRequestContextManager:\n        \"\"\"Initiate websocket connection.\n\n        The api corresponds to aiohttp.ClientSession.ws_connect.\n\n        \"\"\"\n        return _WSRequestContextManager(self._ws_connect(path, **kwargs))\n\n    async def _ws_connect(\n        self, path: StrOrURL, **kwargs: Any\n    ) -> ClientWebSocketResponse:\n        ws = await self._session.ws_connect(self.make_url(path), **kwargs)\n        self._websockets.append(ws)\n        return ws\n\n    async def close(self) -> None:\n        \"\"\"Close all fixtures created by the test client.\n\n        After that point, the TestClient is no longer usable.\n\n        This is an idempotent function: running close multiple times\n        will not have any additional effects.\n\n        close is also run on exit when used as a(n) (asynchronous)\n        context manager.\n\n        \"\"\"\n        if not self._closed:\n            for resp in self._responses:\n                resp.close()\n            for ws in self._websockets:\n                await ws.close()\n            await self._session.close()\n            await self._server.close()\n            self._closed = True\n\n    async def __aenter__(self) -> Self:\n        await self.start_server()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        await self.close()"
    },
    {
      "chunk_id": 371,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "class AioHTTPTestCase(IsolatedAsyncioTestCase, ABC):\n    \"\"\"A base class to allow for unittest web applications using aiohttp.\n\n    Provides the following:\n\n    * self.client (aiohttp.test_utils.TestClient): an aiohttp test client.\n    * self.app (aiohttp.web.Application): the application returned by\n        self.get_application()\n\n    Note that the TestClient's methods are asynchronous: you have to\n    execute function on the test client using asynchronous methods.\n    \"\"\"\n\n    @abstractmethod\n    async def get_application(self) -> Application:\n        \"\"\"Get application.\n\n        This method should be overridden to return the aiohttp.web.Application\n        object to test.\n        \"\"\"\n\n    async def asyncSetUp(self) -> None:\n        self.app = await self.get_application()\n        self.server = await self.get_server(self.app)\n        self.client = await self.get_client(self.server)\n\n        await self.client.start_server()\n\n    async def asyncTearDown(self) -> None:\n        await self.client.close()"
    },
    {
      "chunk_id": 372,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "_LOOP_FACTORY = Callable[[], asyncio.AbstractEventLoop]"
    },
    {
      "chunk_id": 373,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "@contextlib.contextmanager\ndef loop_context(\n    loop_factory: _LOOP_FACTORY = asyncio.new_event_loop, fast: bool = False\n) -> Iterator[asyncio.AbstractEventLoop]:\n    \"\"\"A contextmanager that creates an event_loop, for test purposes.\n\n    Handles the creation and cleanup of a test loop.\n    \"\"\"\n    loop = setup_test_loop(loop_factory)\n    yield loop\n    teardown_test_loop(loop, fast=fast)"
    },
    {
      "chunk_id": 374,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def setup_test_loop(\n    loop_factory: _LOOP_FACTORY = asyncio.new_event_loop,\n) -> asyncio.AbstractEventLoop:\n    \"\"\"Create and return an asyncio.BaseEventLoop instance.\n\n    The caller should also call teardown_test_loop,\n    once they are done with the loop.\n    \"\"\"\n    loop = loop_factory()\n    asyncio.set_event_loop(loop)\n    return loop"
    },
    {
      "chunk_id": 375,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def teardown_test_loop(loop: asyncio.AbstractEventLoop, fast: bool = False) -> None:\n    \"\"\"Teardown and cleanup an event_loop created by setup_test_loop.\"\"\"\n    closed = loop.is_closed()\n    if not closed:\n        loop.call_soon(loop.stop)\n        loop.run_forever()\n        loop.close()\n\n    if not fast:\n        gc.collect()\n\n    asyncio.set_event_loop(None)"
    },
    {
      "chunk_id": 376,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def _create_app_mock() -> mock.MagicMock:\n    def get_dict(app: Any, key: str) -> Any:\n        return app.__app_dict[key]\n\n    def set_dict(app: Any, key: str, value: Any) -> None:\n        app.__app_dict[key] = value\n\n    app = mock.MagicMock(spec=Application)\n    app.__app_dict = {}\n    app.__getitem__ = get_dict\n    app.__setitem__ = set_dict\n\n    app.on_response_prepare = Signal(app)\n    app.on_response_prepare.freeze()\n    return app"
    },
    {
      "chunk_id": 377,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def _create_transport(sslcontext: Optional[SSLContext] = None) -> mock.Mock:\n    transport = mock.Mock()\n\n    def get_extra_info(key: str) -> Optional[SSLContext]:\n        if key == \"sslcontext\":\n            return sslcontext\n        else:\n            return None\n\n    transport.get_extra_info.side_effect = get_extra_info\n    return transport"
    },
    {
      "chunk_id": 378,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def make_mocked_request(\n    method: str,\n    path: str,\n    headers: Optional[LooseHeaders] = None,\n    *,\n    match_info: Optional[Dict[str, str]] = None,\n    version: HttpVersion = HttpVersion(1, 1),\n    closing: bool = False,\n    app: Optional[Application] = None,\n    writer: Optional[AbstractStreamWriter] = None,\n    protocol: Optional[RequestHandler[Request]] = None,\n    transport: Optional[asyncio.Transport] = None,\n    payload: StreamReader = EMPTY_PAYLOAD,\n    sslcontext: Optional[SSLContext] = None,\n    client_max_size: int = 1024**2,\n    loop: Any = ...,\n) -> Request:\n    \"\"\"Creates mocked web.Request testing purposes.\n\n    Useful in unit tests, when spinning full web server is overkill or\n    specific conditions and errors are hard to trigger.\n    \"\"\"\n    task = mock.Mock()\n    if loop is ...:\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            loop = mock.Mock()\n            loop.create_future.return_value = ()\n\n    if version < HttpVersion(1, 1):\n        closing = True\n\n    if headers:\n        headers = CIMultiDictProxy(CIMultiDict(headers))\n        raw_hdrs = tuple(\n            (k.encode(\"utf-8\"), v.encode(\"utf-8\")) for k, v in headers.items()\n        )\n    else:\n        headers = CIMultiDictProxy(CIMultiDict())\n        raw_hdrs = ()\n\n    chunked = \"chunked\" in headers.get(hdrs.TRANSFER_ENCODING, \"\").lower()\n\n    message = RawRequestMessage(\n        method,\n        path,\n        version,\n        headers,\n        raw_hdrs,\n        closing,\n        None,\n        False,\n        chunked,\n        URL(path),\n    )\n    if app is None:\n        app = _create_app_mock()\n\n    if transport is None:\n        transport = _create_transport(sslcontext)\n\n    if protocol is None:\n        protocol = mock.Mock()\n        protocol.transport = transport\n\n    if writer is None:\n        writer = mock.Mock()\n        writer.write_headers = make_mocked_coro(None)\n        writer.write = make_mocked_coro(None)\n        writer.write_eof = make_mocked_coro(None)\n        writer.drain = make_mocked_coro(None)\n        writer.transport = transport\n\n    protocol.transport = transport\n\n    req = Request(\n        message, payload, protocol, writer, task, loop, client_max_size=client_max_size\n    )\n\n    match_info = UrlMappingMatchInfo(\n        {} if match_info is None else match_info, mock.Mock()\n    )\n    match_info.add_app(app)\n    req._match_info = match_info\n\n    return req"
    },
    {
      "chunk_id": 379,
      "source": "__internal__/data_repo/aiohttp/aiohttp/test_utils.py",
      "content": "def make_mocked_coro(\n    return_value: Any = sentinel, raise_exception: Any = sentinel\n) -> Any:\n    \"\"\"Creates a coroutine mock.\"\"\"\n\n    async def mock_coro(*args: Any, **kwargs: Any) -> Any:\n        if raise_exception is not sentinel:\n            raise raise_exception\n        if not inspect.isawaitable(return_value):\n            return return_value\n        await return_value\n\n    return mock.Mock(wraps=mock_coro)\n```"
    },
    {
      "chunk_id": 380,
      "source": "__internal__/data_repo/aiohttp/aiohttp/worker.py",
      "content": "\"\"\"Async gunicorn worker for aiohttp.web\"\"\"\n\nimport asyncio\nimport os\nimport re\nimport signal\nimport sys\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom gunicorn.config import AccessLogFormat as GunicornAccessLogFormat\nfrom gunicorn.workers import base\n\nfrom aiohttp import web\n\nfrom .helpers import set_result\nfrom .web_app import Application\nfrom .web_log import AccessLogger\n\nif TYPE_CHECKING:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nelse:\n    try:\n        import ssl\n\n        SSLContext = ssl.SSLContext\n    except ImportError:  # pragma: no cover\n        ssl = None  # type: ignore[assignment]\n        SSLContext = object  # type: ignore[misc,assignment]\n\n\n__all__ = (\"GunicornWebWorker\", \"GunicornUVLoopWebWorker\")"
    },
    {
      "chunk_id": 381,
      "source": "__internal__/data_repo/aiohttp/aiohttp/worker.py",
      "content": "class GunicornWebWorker(base.Worker):  # type: ignore[misc,no-any-unimported]\n    DEFAULT_AIOHTTP_LOG_FORMAT = AccessLogger.LOG_FORMAT\n    DEFAULT_GUNICORN_LOG_FORMAT = GunicornAccessLogFormat.default\n\n    def __init__(self, *args: Any, **kw: Any) -> None:  # pragma: no cover\n        super().__init__(*args, **kw)\n\n        self._task: Optional[asyncio.Task[None]] = None\n        self.exit_code = 0\n        self._notify_waiter: Optional[asyncio.Future[bool]] = None\n\n    def init_process(self) -> None:\n        # create new event_loop after fork\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n\n        super().init_process()\n\n    def run(self) -> None:\n        self._task = self.loop.create_task(self._run())\n\n        try:  # ignore all finalization problems\n            self.loop.run_until_complete(self._task)\n        except Exception:\n            self.log.exception(\"Exception in gunicorn worker\")\n        self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n        self.loop.close()\n\n        sys.exit(self.exit_code)\n\n    async def _run(self) -> None:\n        runner = None\n        if isinstance(self.wsgi, Application):\n            app = self.wsgi\n        elif asyncio.iscoroutinefunction(self.wsgi):\n            wsgi = await self.wsgi()\n            if isinstance(wsgi, web.AppRunner):\n                runner = wsgi\n                app = runner.app\n            else:\n                app = wsgi\n        else:\n            raise RuntimeError(\n                \"wsgi app should be either Application or \"\n                \"async function returning Application, got {}\".format(self.wsgi)\n            )\n\n        if runner is None:\n            access_log = self.log.access_log if self.cfg.accesslog else None\n            runner = web.AppRunner(\n                app,\n                logger=self.log,\n                keepalive_timeout=self.cfg.keepalive,\n                access_log=access_log,\n                access_log_format=self._get_valid_log_format(\n                    self.cfg.access_log_format\n                ),\n                shutdown_timeout=self.cfg.graceful_timeout / 100 * 95,\n            )\n        await runner.setup()\n\n        ctx = self._create_ssl_context(self.cfg) if self.cfg.is_ssl else None\n\n        assert runner is not None\n        server = runner.server\n        assert server is not None\n        for sock in self.sockets:\n            site = web.SockSite(\n                runner,\n                sock,\n                ssl_context=ctx,\n            )\n            await site.start()\n\n        # If our parent changed then we shut down.\n        pid = os.getpid()\n        try:\n            while self.alive:  # type: ignore[has-type]\n                self.notify()\n\n                cnt = server.requests_count\n                if self.max_requests and cnt > self.max_requests:\n                    self.alive = False\n                    self.log.info(\"Max requests, shutting down: %s\", self)\n\n                elif pid == os.getpid() and self.ppid != os.getppid():\n                    self.alive = False\n                    self.log.info(\"Parent changed, shutting down: %s\", self)\n                else:\n                    await self._wait_next_notify()\n        except BaseException:\n            pass\n\n        await runner.cleanup()\n\n    def _wait_next_notify(self) -> \"asyncio.Future[bool]\":\n        self._notify_waiter_done()\n\n        loop = self.loop\n        assert loop is not None\n        self._notify_waiter = waiter = loop.create_future()\n        self.loop.call_later(1.0, self._notify_waiter_done, waiter)\n\n        return waiter\n\n    def _notify_waiter_done(\n        self, waiter: Optional[\"asyncio.Future[bool]\"] = None\n    ) -> None:\n        if waiter is None:\n            waiter = self._notify_waiter\n        if waiter is not None:\n            set_result(waiter, True)\n\n        if waiter is self._notify_waiter:\n            self._notify_waiter = None\n\n    def init_signals(self) -> None:\n        # Set up signals through the event loop API.\n\n        self.loop.add_signal_handler(\n            signal.SIGQUIT, self.handle_quit, signal.SIGQUIT, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGTERM, self.handle_exit, signal.SIGTERM, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGINT, self.handle_quit, signal.SIGINT, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGWINCH, self.handle_winch, signal.SIGWINCH, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGUSR1, self.handle_usr1, signal.SIGUSR1, None\n        )\n\n        self.loop.add_signal_handler(\n            signal.SIGABRT, self.handle_abort, signal.SIGABRT, None\n        )\n\n        # Don't let SIGTERM and SIGUSR1 disturb active requests\n        # by interrupting system calls\n        signal.siginterrupt(signal.SIGTERM, False)\n        signal.siginterrupt(signal.SIGUSR1, False)\n        # Reset signals so Gunicorn doesn't swallow subprocess return codes\n        # See: https://github.com/aio-libs/aiohttp/issues/6130\n\n    def handle_quit(self, sig: int, frame: Optional[FrameType]) -> None:\n        self.alive = False\n\n        # worker_int callback\n        self.cfg.worker_int(self)\n\n        # wakeup closing process\n        self._notify_waiter_done()\n\n    def handle_abort(self, sig: int, frame: Optional[FrameType]) -> None:\n        self.alive = False\n        self.exit_code = 1\n        self.cfg.worker_abort(self)\n        sys.exit(1)\n\n    @staticmethod\n    def _create_ssl_context(cfg: Any) -> \"SSLContext\":\n        \"\"\"Creates SSLContext instance for usage in asyncio.create_server.\n\n        See ssl.SSLSocket.__init__ for more details.\n        \"\"\"\n        if ssl is None:  # pragma: no cover\n            raise RuntimeError(\"SSL is not supported.\")\n\n        ctx = ssl.SSLContext(cfg.ssl_version)\n        ctx.load_cert_chain(cfg.certfile, cfg.keyfile)\n        ctx.verify_mode = cfg.cert_reqs\n        if cfg.ca_certs:\n            ctx.load_verify_locations(cfg.ca_certs)\n        if cfg.ciphers:\n            ctx.set_ciphers(cfg.ciphers)\n        return ctx\n\n    def _get_valid_log_format(self, source_format: str) -> str:\n        if source_format == self.DEFAULT_GUNICORN_LOG_FORMAT:\n            return self.DEFAULT_AIOHTTP_LOG_FORMAT\n        elif re.search(r\"%\\([^\\)]+\\)\", source_format):\n            raise ValueError(\n                \"Gunicorn's style options in form of `%(name)s` are not \"\n                \"supported for the log formatting. Please use aiohttp's \"\n                \"format specification to configure access log formatting: \"\n                \"http://docs.aiohttp.org/en/stable/logging.html\"\n                \"#format-specification\"\n            )\n        else:\n            return source_format"
    },
    {
      "chunk_id": 382,
      "source": "__internal__/data_repo/aiohttp/aiohttp/worker.py",
      "content": "class GunicornUVLoopWebWorker(GunicornWebWorker):\n    def init_process(self) -> None:\n        import uvloop\n\n        # Setup uvloop policy, so that every\n        # asyncio.get_event_loop() will create an instance\n        # of uvloop event loop.\n        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n\n        super().init_process()"
    },
    {
      "chunk_id": 383,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "```python"
    },
    {
      "chunk_id": 384,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "import asyncio\nimport base64\nimport dataclasses\nimport hashlib\nimport json\nimport os\nimport sys\nimport traceback\nimport warnings\nfrom contextlib import suppress\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Collection,\n    Coroutine,\n    Final,\n    FrozenSet,\n    Generator,\n    Generic,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    final,\n)\n\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs, http, payload\nfrom ._websocket.reader import WebSocketDataQueue\nfrom .abc import AbstractCookieJar\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientConnectionResetError,\n    ClientConnectorCertificateError,\n    ClientConnectorDNSError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientError,\n    ClientHttpProxyError,\n    ClientOSError,\n    ClientPayloadError,\n    ClientProxyConnectionError,\n    ClientResponseError,\n    ClientSSLError,\n    ConnectionTimeoutError,\n    ContentTypeError,\n    InvalidURL,\n    InvalidUrlClientError,\n    InvalidUrlRedirectClientError,\n    NonHttpUrlClientError,\n    NonHttpUrlRedirectClientError,\n    RedirectClientError,\n    ServerConnectionError,\n    ServerDisconnectedError,\n    ServerFingerprintMismatch,\n    ServerTimeoutError,\n    SocketTimeoutError,\n    TooManyRedirects,\n    WSMessageTypeError,\n    WSServerHandshakeError,\n)\nfrom .client_reqrep import (\n    SSL_ALLOWED_TYPES,\n    ClientRequest,\n    ClientResponse,\n    Fingerprint,\n    RequestInfo,\n)\nfrom .client_ws import (\n    DEFAULT_WS_CLIENT_TIMEOUT,\n    ClientWebSocketResponse,\n    ClientWSTimeout,\n)\nfrom .connector import (\n    HTTP_AND_EMPTY_SCHEMA_SET,\n    BaseConnector,\n    NamedPipeConnector,\n    TCPConnector,\n    UnixConnector,\n)\nfrom .cookiejar import CookieJar\nfrom .helpers import (\n    _SENTINEL,\n    EMPTY_BODY_METHODS,\n    BasicAuth,\n    TimeoutHandle,\n    frozen_dataclass_decorator,\n    get_env_proxy_for_url,\n    sentinel,\n    strip_auth_from_url,\n)\nfrom .http import WS_KEY, HttpVersion, WebSocketReader, WebSocketWriter\nfrom .http_websocket import WSHandshakeError, ws_ext_gen, ws_ext_parse\nfrom .tracing import Trace, TraceConfig\nfrom .typedefs import JSONEncoder, LooseCookies, LooseHeaders, Query, StrOrURL"
    },
    {
      "chunk_id": 385,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "class _RequestOptions(TypedDict, total=False):\n    params: Query\n    data: Any\n    json: Any\n    cookies: Union[LooseCookies, None]\n    headers: Union[LooseHeaders, None]\n    skip_auto_headers: Union[Iterable[str], None]\n    auth: Union[BasicAuth, None]\n    allow_redirects: bool\n    max_redirects: int\n    compress: Union[str, bool]\n    chunked: Union[bool, None]\n    expect100: bool\n    raise_for_status: Union[None, bool, Callable[[ClientResponse], Awaitable[None]]]\n    read_until_eof: bool\n    proxy: Union[StrOrURL, None]\n    proxy_auth: Union[BasicAuth, None]\n    timeout: \"Union[ClientTimeout, _SENTINEL, None]\"\n    ssl: Union[SSLContext, bool, Fingerprint]\n    server_hostname: Union[str, None]\n    proxy_headers: Union[LooseHeaders, None]\n    trace_request_ctx: Union[Mapping[str, Any], None]\n    read_bufsize: Union[int, None]\n    auto_decompress: Union[bool, None]\n    max_line_size: Union[int, None]\n    max_field_size: Union[int, None]"
    },
    {
      "chunk_id": 386,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "@frozen_dataclass_decorator\nclass ClientTimeout:\n    total: Optional[float] = None\n    connect: Optional[float] = None\n    sock_read: Optional[float] = None\n    sock_connect: Optional[float] = None\n    ceil_threshold: float = 5\n\nDEFAULT_TIMEOUT: Final[ClientTimeout] = ClientTimeout(total=5 * 60, sock_connect=30)\n\nIDEMPOTENT_METHODS = frozenset({\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\", \"PUT\", \"DELETE\"})\n\n_RetType = TypeVar(\"_RetType\", ClientResponse, ClientWebSocketResponse)\n_CharsetResolver = Callable[[ClientResponse, bytes], str]"
    },
    {
      "chunk_id": 387,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "@final\nclass ClientSession:\n    \"\"\"First-class interface for making HTTP requests.\"\"\"\n\n    __slots__ = (\n        \"_base_url\",\n        \"_base_url_origin\",\n        \"_source_traceback\",\n        \"_connector\",\n        \"_loop\",\n        \"_cookie_jar\",\n        \"_connector_owner\",\n        \"_default_auth\",\n        \"_version\",\n        \"_json_serialize\",\n        \"_requote_redirect_url\",\n        \"_timeout\",\n        \"_raise_for_status\",\n        \"_auto_decompress\",\n        \"_trust_env\",\n        \"_default_headers\",\n        \"_skip_auto_headers\",\n        \"_request_class\",\n        \"_response_class\",\n        \"_ws_response_class\",\n        \"_trace_configs\",\n        \"_read_bufsize\",\n        \"_max_line_size\",\n        \"_max_field_size\",\n        \"_resolve_charset\",\n        \"_default_proxy\",\n        \"_default_proxy_auth\",\n        \"_retry_connection\",\n    )\n\n    def __init__(\n        self,\n        base_url: Optional[StrOrURL] = None,\n        *,\n        connector: Optional[BaseConnector] = None,\n        cookies: Optional[LooseCookies] = None,\n        headers: Optional[LooseHeaders] = None,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        skip_auto_headers: Optional[Iterable[str]] = None,\n        auth: Optional[BasicAuth] = None,\n        json_serialize: JSONEncoder = json.dumps,\n        request_class: Type[ClientRequest] = ClientRequest,\n        response_class: Type[ClientResponse] = ClientResponse,\n        ws_response_class: Type[ClientWebSocketResponse] = ClientWebSocketResponse,\n        version: HttpVersion = http.HttpVersion11,\n        cookie_jar: Optional[AbstractCookieJar] = None,\n        connector_owner: bool = True,\n        raise_for_status: Union[\n            bool, Callable[[ClientResponse], Awaitable[None]]\n        ] = False,\n        timeout: Union[_SENTINEL, ClientTimeout, None] = sentinel,\n        auto_decompress: bool = True,\n        trust_env: bool = False,\n        requote_redirect_url: bool = True,\n        trace_configs: Optional[List[TraceConfig[object]]] = None,\n        read_bufsize: int = 2**16,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n        fallback_charset_resolver: _CharsetResolver = lambda r, b: \"utf-8\",\n    ) -> None:\n        # We initialise _connector to None immediately, as it's referenced in __del__()\n        # and could cause issues if an exception occurs during initialisation.\n        self._connector: Optional[BaseConnector] = None\n        if base_url is None or isinstance(base_url, URL):\n            self._base_url: Optional[URL] = base_url\n            self._base_url_origin = None if base_url is None else base_url.origin()\n        else:\n            self._base_url = URL(base_url)\n            self._base_url_origin = self._base_url.origin()\n            assert self._base_url.absolute, \"Only absolute URLs are supported\"\n        if self._base_url is not None and not self._base_url.path.endswith(\"/\"):\n            raise ValueError(\"base_url must have a trailing '/'\")\n\n        loop = asyncio.get_running_loop()\n\n        if timeout is sentinel or timeout is None:\n            timeout = DEFAULT_TIMEOUT\n        if not isinstance(timeout, ClientTimeout):\n            raise ValueError(\n                f\"timeout parameter cannot be of {type(timeout)} type, \"\n                \"please use 'timeout=ClientTimeout(...)'\",\n            )\n        self._timeout = timeout\n\n        if connector is None:\n            connector = TCPConnector()\n        # Initialize these three attrs before raising any exception,\n        # they are used in __del__\n        self._connector = connector\n        self._loop = loop\n        if loop.get_debug():\n            self._source_traceback: Optional[traceback.StackSummary] = (\n                traceback.extract_stack(sys._getframe(1))\n            )\n        else:\n            self._source_traceback = None\n\n        if connector._loop is not loop:\n            raise RuntimeError(\"Session and connector have to use same event loop\")\n\n        if cookie_jar is None:\n            cookie_jar = CookieJar()\n        self._cookie_jar = cookie_jar\n\n        if cookies:\n            self._cookie_jar.update_cookies(cookies)\n\n        self._connector_owner = connector_owner\n        self._default_auth = auth\n        self._version = version\n        self._json_serialize = json_serialize\n        self._raise_for_status = raise_for_status\n        self._auto_decompress = auto_decompress\n        self._trust_env = trust_env\n        self._requote_redirect_url = requote_redirect_url\n        self._read_bufsize = read_bufsize\n        self._max_line_size = max_line_size\n        self._max_field_size = max_field_size\n\n        # Convert to list of tuples\n        if headers:\n            real_headers: CIMultiDict[str] = CIMultiDict(headers)\n        else:\n            real_headers = CIMultiDict()\n        self._default_headers: CIMultiDict[str] = real_headers\n        if skip_auto_headers is not None:\n            self._skip_auto_headers = frozenset(istr(i) for i in skip_auto_headers)\n        else:\n            self._skip_auto_headers = frozenset()\n\n        self._request_class = request_class\n        self._response_class = response_class\n        self._ws_response_class = ws_response_class\n\n        self._trace_configs = trace_configs or []\n        for trace_config in self._trace_configs:\n            trace_config.freeze()\n\n        self._resolve_charset = fallback_charset_resolver\n\n        self._default_proxy = proxy\n        self._default_proxy_auth = proxy_auth\n        self._retry_connection: bool = True\n\n    def __init_subclass__(cls: Type[\"ClientSession\"]) -> None:\n        raise TypeError(\n            \"Inheritance class {} from ClientSession \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if not self.closed:\n            _warnings.warn(\n                f\"Unclosed client session {self!r}\",\n                ResourceWarning,\n                source=self,\n            )\n            context = {\"client_session\": self, \"message\": \"Unclosed client session\"}\n            if self._source_traceback is not None:\n                context[\"source_traceback\"] = self._source_traceback\n            self._loop.call_exception_handler(context)\n\n    if sys.version_info >= (3, 11) and TYPE_CHECKING:\n\n        def request(\n            self,\n            method: str,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n    else:\n\n        def request(\n            self, method: str, url: StrOrURL, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP request.\"\"\"\n            return _RequestContextManager(self._request(method, url, **kwargs))\n\n    def _build_url(self, str_or_url: StrOrURL) -> URL:\n        url = URL(str_or_url)\n        if self._base_url and not url.absolute:\n            return self._base_url.join(url)\n        return url\n\n    async def _request(\n        self,\n        method: str,\n        str_or_url: StrOrURL,\n        *,\n        params: Query = None,\n        data: Any = None,\n        json: Any = None,\n        cookies: Optional[LooseCookies] = None,\n        headers: Optional[LooseHeaders] = None,\n        skip_auto_headers: Optional[Iterable[str]] = None,\n        auth: Optional[BasicAuth] = None,\n        allow_redirects: bool = True,\n        max_redirects: int = 10,\n        compress: Union[str, bool] = False,\n        chunked: Optional[bool] = None,\n        expect100: bool = False,\n        raise_for_status: Union[\n            None, bool, Callable[[ClientResponse], Awaitable[None]]\n        ] = None,\n        read_until_eof: bool = True,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        timeout: Union[ClientTimeout, _SENTINEL, None] = sentinel,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        trace_request_ctx: Optional[Mapping[str, Any]] = None,\n        read_bufsize: Optional[int] = None,\n        auto_decompress: Optional[bool] = None,\n        max_line_size: Optional[int] = None,\n        max_field_size: Optional[int] = None,\n    ) -> ClientResponse:\n        # NOTE: timeout clamps existing connect and read timeouts.  We cannot\n        # set the default to None because we need to detect if the user wants\n        # to use the existing timeouts by setting timeout to None.\n\n        if self.closed:\n            raise RuntimeError(\"Session is closed\")\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n\n        if data is not None and json is not None:\n            raise ValueError(\n                \"data and json parameters can not be used at the same time\"\n            )\n        elif json is not None:\n            data = payload.JsonPayload(json, dumps=self._json_serialize)\n\n        redirects = 0\n        history: List[ClientResponse] = []\n        version = self._version\n        params = params or {}\n\n        # Merge with default headers and transform to CIMultiDict\n        headers = self._prepare_headers(headers)\n\n        try:\n            url = self._build_url(str_or_url)\n        except ValueError as e:\n            raise InvalidUrlClientError(str_or_url) from e\n\n        assert self._connector is not None\n        if url.scheme not in self._connector.allowed_protocol_schema_set:\n            raise NonHttpUrlClientError(url)\n\n        skip_headers: Optional[Iterable[istr]]\n        if skip_auto_headers is not None:\n            skip_headers = {\n                istr(i) for i in skip_auto_headers\n            } | self._skip_auto_headers\n        elif self._skip_auto_headers:\n            skip_headers = self._skip_auto_headers\n        else:\n            skip_headers = None\n\n        if proxy is None:\n            proxy = self._default_proxy\n        if proxy_auth is None:\n            proxy_auth = self._default_proxy_auth\n\n        if proxy is None:\n            proxy_headers = None\n        else:\n            proxy_headers = self._prepare_headers(proxy_headers)\n            try:\n                proxy = URL(proxy)\n            except ValueError as e:\n                raise InvalidURL(proxy) from e\n\n        if timeout is sentinel or timeout is None:\n            real_timeout: ClientTimeout = self._timeout\n        else:\n            real_timeout = timeout\n        # timeout is cumulative for all request operations\n        # (request, redirects, responses, data consuming)\n        tm = TimeoutHandle(\n            self._loop, real_timeout.total, ceil_threshold=real_timeout.ceil_threshold\n        )\n        handle = tm.start()\n\n        if read_bufsize is None:\n            read_bufsize = self._read_bufsize\n\n        if auto_decompress is None:\n            auto_decompress = self._auto_decompress\n\n        if max_line_size is None:\n            max_line_size = self._max_line_size\n\n        if max_field_size is None:\n            max_field_size = self._max_field_size\n\n        traces = [\n            Trace(\n                self,\n                trace_config,\n                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),\n            )\n            for trace_config in self._trace_configs\n        ]\n\n        for trace in traces:\n            await trace.send_request_start(method, url.update_query(params), headers)\n\n        timer = tm.timer()\n        try:\n            with timer:\n                # https://www.rfc-editor.org/rfc/rfc9112.html#name-retrying-requests\n                retry_persistent_connection = (\n                    self._retry_connection and method in IDEMPOTENT_METHODS\n                )\n                while True:\n                    url, auth_from_url = strip_auth_from_url(url)\n                    if not url.raw_host:\n                        # NOTE: Bail early, otherwise, causes `InvalidURL` through\n                        # NOTE: `self._request_class()` below.\n                        err_exc_cls = (\n                            InvalidUrlRedirectClientError\n                            if redirects\n                            else InvalidUrlClientError\n                        )\n                        raise err_exc_cls(url)\n                    # If `auth` was passed for an already authenticated URL,\n                    # disallow only if this is the initial URL; this is to avoid issues\n                    # with sketchy redirects that are not the caller's responsibility\n                    if not history and (auth and auth_from_url):\n                        raise ValueError(\n                            \"Cannot combine AUTH argument with \"\n                            \"credentials encoded in URL\"\n                        )\n\n                    # Override the auth with the one from the URL only if we\n                    # have no auth, or if we got an auth from a redirect URL\n                    if auth is None or (history and auth_from_url is not None):\n                        auth = auth_from_url\n\n                    if (\n                        auth is None\n                        and self._default_auth\n                        and (\n                            not self._base_url or self._base_url_origin == url.origin()\n                        )\n                    ):\n                        auth = self._default_auth\n                    # It would be confusing if we support explicit\n                    # Authorization header with auth argument\n                    if auth is not None and hdrs.AUTHORIZATION in headers:\n                        raise ValueError(\n                            \"Cannot combine AUTHORIZATION header \"\n                            \"with AUTH argument or credentials \"\n                            \"encoded in URL\"\n                        )\n\n                    all_cookies = self._cookie_jar.filter_cookies(url)\n\n                    if cookies is not None:\n                        tmp_cookie_jar = CookieJar(\n                            quote_cookie=self._cookie_jar.quote_cookie\n                        )\n                        tmp_cookie_jar.update_cookies(cookies)\n                        req_cookies = tmp_cookie_jar.filter_cookies(url)\n                        if req_cookies:\n                            all_cookies.load(req_cookies)\n\n                    if proxy is not None:\n                        proxy = URL(proxy)\n                    elif self._trust_env:\n                        with suppress(LookupError):\n                            proxy, proxy_auth = get_env_proxy_for_url(url)\n\n                    req = self._request_class(\n                        method,\n                        url,\n                        params=params,\n                        headers=headers,\n                        skip_auto_headers=skip_headers,\n                        data=data,\n                        cookies=all_cookies,\n                        auth=auth,\n                        version=version,\n                        compress=compress,\n                        chunked=chunked,\n                        expect100=expect100,\n                        loop=self._loop,\n                        response_class=self._response_class,\n                        proxy=proxy,\n                        proxy_auth=proxy_auth,\n                        timer=timer,\n                        session=self,\n                        ssl=ssl,\n                        server_hostname=server_hostname,\n                        proxy_headers=proxy_headers,\n                        traces=traces,\n                        trust_env=self.trust_env,\n                    )\n\n                    # connection timeout\n                    try:\n                        conn = await self._connector.connect(\n                            req, traces=traces, timeout=real_timeout\n                        )\n                    except asyncio.TimeoutError as exc:\n                        raise ConnectionTimeoutError(\n                            f\"Connection timeout to host {url}\"\n                        ) from exc\n\n                    assert conn.transport is not None\n\n                    assert conn.protocol is not None\n                    conn.protocol.set_response_params(\n                        timer=timer,\n                        skip_payload=method in EMPTY_BODY_METHODS,\n                        read_until_eof=read_until_eof,\n                        auto_decompress=auto_decompress,\n                        read_timeout=real_timeout.sock_read,\n                        read_bufsize=read_bufsize,\n                        timeout_ceil_threshold=self._connector._timeout_ceil_threshold,\n                        max_line_size=max_line_size,\n                        max_field_size=max_field_size,\n                    )\n\n                    try:\n                        try:\n                            resp = await req.send(conn)\n                            try:\n                                await resp.start(conn)\n                            except BaseException:\n                                resp.close()\n                                raise\n                        except BaseException:\n                            conn.close()\n                            raise\n                    except (ClientOSError, ServerDisconnectedError):\n                        if retry_persistent_connection:\n                            retry_persistent_connection = False\n                            continue\n                        raise\n                    except ClientError:\n                        raise\n                    except OSError as exc:\n                        if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                            raise\n                        raise ClientOSError(*exc.args) from exc\n\n                    if cookies := resp._cookies:\n                        self._cookie_jar.update_cookies(cookies, resp.url)\n\n                    # redirects\n                    if resp.status in (301, 302, 303, 307, 308) and allow_redirects:\n                        for trace in traces:\n                            await trace.send_request_redirect(\n                                method, url.update_query(params), headers, resp\n                            )\n\n                        redirects += 1\n                        history.append(resp)\n                        if max_redirects and redirects >= max_redirects:\n                            resp.close()\n                            raise TooManyRedirects(\n                                history[0].request_info, tuple(history)\n                            )\n\n                        # For 301 and 302, mimic IE, now changed in RFC\n                        # https://github.com/kennethreitz/requests/pull/269\n                        if (resp.status == 303 and resp.method != hdrs.METH_HEAD) or (\n                            resp.status in (301, 302) and resp.method == hdrs.METH_POST\n                        ):\n                            method = hdrs.METH_GET\n                            data = None\n                            if headers.get(hdrs.CONTENT_LENGTH):\n                                headers.pop(hdrs.CONTENT_LENGTH)\n\n                        r_url = resp.headers.get(hdrs.LOCATION) or resp.headers.get(\n                            hdrs.URI\n                        )\n                        if r_url is None:\n                            # see github.com/aio-libs/aiohttp/issues/2022\n                            break\n                        else:\n                            # reading from correct redirection\n                            # response is forbidden\n                            resp.release()\n\n                        try:\n                            parsed_redirect_url = URL(\n                                r_url, encoded=not self._requote_redirect_url\n                            )\n                        except ValueError as e:\n                            raise InvalidUrlRedirectClientError(\n                                r_url,\n                                \"Server attempted redirecting to a location that does not look like a URL\",\n                            ) from e\n\n                        scheme = parsed_redirect_url.scheme\n                        if scheme not in HTTP_AND_EMPTY_SCHEMA_SET:\n                            resp.close()\n                            raise NonHttpUrlRedirectClientError(r_url)\n                        elif not scheme:\n                            parsed_redirect_url = url.join(parsed_redirect_url)\n\n                        is_same_host_https_redirect = (\n                            url.host == parsed_redirect_url.host\n                            and parsed_redirect_url.scheme == \"https\"\n                            and url.scheme == \"http\"\n                        )\n\n                        try:\n                            redirect_origin = parsed_redirect_url.origin()\n                        except ValueError as origin_val_err:\n                            raise InvalidUrlRedirectClientError(\n                                parsed_redirect_url,\n                                \"Invalid redirect URL origin\",\n                            ) from origin_val_err\n\n                        if (\n                            not is_same_host_https_redirect\n                            and url.origin() != redirect_origin\n                        ):\n                            auth = None\n                            headers.pop(hdrs.AUTHORIZATION, None)\n\n                        url = parsed_redirect_url\n                        params = {}\n                        resp.release()\n                        continue\n\n                    break\n\n            # check response status\n            if raise_for_status is None:\n                raise_for_status = self._raise_for_status\n\n            if raise_for_status is None:\n                pass\n            elif callable(raise_for_status):\n                await raise_for_status(resp)\n            elif raise_for_status:\n                resp.raise_for_status()\n\n            # register connection\n            if handle is not None:\n                if resp.connection is not None:\n                    resp.connection.add_callback(handle.cancel)\n                else:\n                    handle.cancel()\n\n            resp._history = tuple(history)\n\n            for trace in traces:\n                await trace.send_request_end(\n                    method, url.update_query(params), headers, resp\n                )\n            return resp\n\n        except BaseException as e:\n            # cleanup timer\n            tm.close()\n            if handle:\n                handle.cancel()\n                handle = None\n\n            for trace in traces:\n                await trace.send_request_exception(\n                    method, url.update_query(params), headers, e\n                )\n            raise\n\n    def ws_connect(\n        self,\n        url: StrOrURL,\n        *,\n        method: str = hdrs.METH_GET,\n        protocols: Collection[str] = (),\n        timeout: Union[ClientWSTimeout, _SENTINEL] = sentinel,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        auth: Optional[BasicAuth] = None,\n        origin: Optional[str] = None,\n        params: Query = None,\n        headers: Optional[LooseHeaders] = None,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        compress: int = 0,\n        max_msg_size: int = 4 * 1024 * 1024,\n    ) -> \"_WSRequestContextManager\":\n        \"\"\"Initiate websocket connection.\"\"\"\n        return _WSRequestContextManager(\n            self._ws_connect(\n                url,\n                method=method,\n                protocols=protocols,\n                timeout=timeout,\n                receive_timeout=receive_timeout,\n                autoclose=autoclose,\n                autoping=autoping,\n                heartbeat=heartbeat,\n                auth=auth,\n                origin=origin,\n                params=params,\n                headers=headers,\n                proxy=proxy,\n                proxy_auth=proxy_auth,\n                ssl=ssl,\n                server_hostname=server_hostname,\n                proxy_headers=proxy_headers,\n                compress=compress,\n                max_msg_size=max_msg_size,\n            )\n        )\n\n    async def _ws_connect(\n        self,\n        url: StrOrURL,\n        *,\n        method: str = hdrs.METH_GET,\n        protocols: Collection[str] = (),\n        timeout: Union[ClientWSTimeout, _SENTINEL] = sentinel,\n        receive_timeout: Optional[float] = None,\n        autoclose: bool = True,\n        autoping: bool = True,\n        heartbeat: Optional[float] = None,\n        auth: Optional[BasicAuth] = None,\n        origin: Optional[str] = None,\n        params: Query = None,\n        headers: Optional[LooseHeaders] = None,\n        proxy: Optional[StrOrURL] = None,\n        proxy_auth: Optional[BasicAuth] = None,\n        ssl: Union[SSLContext, bool, Fingerprint] = True,\n        server_hostname: Optional[str] = None,\n        proxy_headers: Optional[LooseHeaders] = None,\n        compress: int = 0,\n        max_msg_size: int = 4 * 1024 * 1024,\n    ) -> ClientWebSocketResponse:\n        if timeout is not sentinel:\n            if isinstance(timeout, ClientWSTimeout):\n                ws_timeout = timeout\n            else:\n                warnings.warn(  # type: ignore[unreachable]\n                    \"parameter 'timeout' of type 'float' \"\n                    \"is deprecated, please use \"\n                    \"'timeout=ClientWSTimeout(ws_close=...)'\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                ws_timeout = ClientWSTimeout(ws_close=timeout)\n        else:\n            ws_timeout = DEFAULT_WS_CLIENT_TIMEOUT\n        if receive_timeout is not None:\n            warnings.warn(\n                \"float parameter 'receive_timeout' \"\n                \"is deprecated, please use parameter \"\n                \"'timeout=ClientWSTimeout(ws_receive=...)'\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            ws_timeout = dataclasses.replace(ws_timeout, ws_receive=receive_timeout)\n\n        if headers is None:\n            real_headers: CIMultiDict[str] = CIMultiDict()\n        else:\n            real_headers = CIMultiDict(headers)\n\n        default_headers = {\n            hdrs.UPGRADE: \"websocket\",\n            hdrs.CONNECTION: \"Upgrade\",\n            hdrs.SEC_WEBSOCKET_VERSION: \"13\",\n        }\n\n        for key, value in default_headers.items():\n            real_headers.setdefault(key, value)\n\n        sec_key = base64.b64encode(os.urandom(16))\n        real_headers[hdrs.SEC_WEBSOCKET_KEY] = sec_key.decode()\n\n        if protocols:\n            real_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = \",\".join(protocols)\n        if origin is not None:\n            real_headers[hdrs.ORIGIN] = origin\n        if compress:\n            extstr = ws_ext_gen(compress=compress)\n            real_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = extstr\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n\n        # send request\n        resp = await self.request(\n            method,\n            url,\n            params=params,\n            headers=real_headers,\n            read_until_eof=False,\n            auth=auth,\n            proxy=proxy,\n            proxy_auth=proxy_auth,\n            ssl=ssl,\n            server_hostname=server_hostname,\n            proxy_headers=proxy_headers,\n        )\n\n        try:\n            # check handshake\n            if resp.status != 101:\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid response status\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            if resp.headers.get(hdrs.UPGRADE, \"\").lower() != \"websocket\":\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid upgrade header\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            if resp.headers.get(hdrs.CONNECTION, \"\").lower() != \"upgrade\":\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid connection header\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            # key calculation\n            r_key = resp.headers.get(hdrs.SEC_WEBSOCKET_ACCEPT, \"\")\n            match = base64.b64encode(hashlib.sha1(sec_key + WS_KEY).digest()).decode()\n            if r_key != match:\n                raise WSServerHandshakeError(\n                    resp.request_info,\n                    resp.history,\n                    message=\"Invalid challenge response\",\n                    status=resp.status,\n                    headers=resp.headers,\n                )\n\n            # websocket protocol\n            protocol = None\n            if protocols and hdrs.SEC_WEBSOCKET_PROTOCOL in resp.headers:\n                resp_protocols = [\n                    proto.strip()\n                    for proto in resp.headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(\",\")\n                ]\n\n                for proto in resp_protocols:\n                    if proto in protocols:\n                        protocol = proto\n                        break\n\n            # websocket compress\n            notakeover = False\n            if compress:\n                compress_hdrs = resp.headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\n                if compress_hdrs:\n                    try:\n                        compress, notakeover = ws_ext_parse(compress_hdrs)\n                    except WSHandshakeError as exc:\n                        raise WSServerHandshakeError(\n                            resp.request_info,\n                            resp.history,\n                            message=exc.args[0],\n                            status=resp.status,\n                            headers=resp.headers,\n                        ) from exc\n                else:\n                    compress = 0\n                    notakeover = False\n\n            conn = resp.connection\n            assert conn is not None\n            conn_proto = conn.protocol\n            assert conn_proto is not None\n\n            # For WS connection the read_timeout must be either ws_timeout.ws_receive or greater\n            # None == no timeout, i.e. infinite timeout, so None is the max timeout possible\n            if ws_timeout.ws_receive is None:\n                # Reset regardless\n                conn_proto.read_timeout = None\n            elif conn_proto.read_timeout is not None:\n                conn_proto.read_timeout = max(\n                    ws_timeout.ws_receive, conn_proto.read_timeout\n                )\n\n            transport = conn.transport\n            assert transport is not None\n            reader = WebSocketDataQueue(conn_proto, 2**16, loop=self._loop)\n            conn_proto.set_parser(WebSocketReader(reader, max_msg_size), reader)\n            writer = WebSocketWriter(\n                conn_proto,\n                transport,\n                use_mask=True,\n                compress=compress,\n                notakeover=notakeover,\n            )\n        except BaseException:\n            resp.close()\n            raise\n        else:\n            return self._ws_response_class(\n                reader,\n                writer,\n                protocol,\n                resp,\n                ws_timeout,\n                autoclose,\n                autoping,\n                self._loop,\n                heartbeat=heartbeat,\n                compress=compress,\n                client_notakeover=notakeover,\n            )\n\n    def _prepare_headers(self, headers: Optional[LooseHeaders]) -> \"CIMultiDict[str]\":\n        \"\"\"Add default headers and transform it to CIMultiDict\"\"\"\n        # Convert headers to MultiDict\n        result = CIMultiDict(self._default_headers)\n        if headers:\n            if not isinstance(headers, (MultiDictProxy, MultiDict)):\n                headers = CIMultiDict(headers)\n            added_names: Set[str] = set()\n            for key, value in headers.items():\n                if key in added_names:\n                    result.add(key, value)\n                else:\n                    result[key] = value\n                    added_names.add(key)\n        return result\n\n    if sys.version_info >= (3, 11) and TYPE_CHECKING:\n\n        def get(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def options(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def head(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def post(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def put(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def patch(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n        def delete(\n            self,\n            url: StrOrURL,\n            **kwargs: Unpack[_RequestOptions],\n        ) -> \"_RequestContextManager\": ...\n\n    else:\n\n        def get(\n            self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP GET request.\"\"\"\n            return _RequestContextManager(\n                self._request(\n                    hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs\n                )\n            )\n\n        def options(\n            self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP OPTIONS request.\"\"\"\n            return _RequestContextManager(\n                self._request(\n                    hdrs.METH_OPTIONS, url, allow_redirects=allow_redirects, **kwargs\n                )\n            )\n\n        def head(\n            self, url: StrOrURL, *, allow_redirects: bool = False, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP HEAD request.\"\"\"\n            return _RequestContextManager(\n                self._request(\n                    hdrs.METH_HEAD, url, allow_redirects=allow_redirects, **kwargs\n                )\n            )\n\n        def post(\n            self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP POST request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_POST, url, data=data, **kwargs)\n            )\n\n        def put(\n            self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP PUT request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_PUT, url, data=data, **kwargs)\n            )\n\n        def patch(\n            self, url: StrOrURL, *, data: Any = None, **kwargs: Any\n        ) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP PATCH request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_PATCH, url, data=data, **kwargs)\n            )\n\n        def delete(self, url: StrOrURL, **kwargs: Any) -> \"_RequestContextManager\":\n            \"\"\"Perform HTTP DELETE request.\"\"\"\n            return _RequestContextManager(\n                self._request(hdrs.METH_DELETE, url, **kwargs)\n            )\n\n    async def close(self) -> None:\n        \"\"\"Close underlying connector.\n\n        Release all acquired resources.\n        \"\"\"\n        if not self.closed:\n            if self._connector is not None and self._connector_owner:\n                await self._connector.close()\n            self._connector = None\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"Is client session closed.\n\n        A readonly property.\n        \"\"\"\n        return self._connector is None or self._connector.closed\n\n    @property\n    def connector(self) -> Optional[BaseConnector]:\n        \"\"\"Connector instance used for the session.\"\"\"\n        return self._connector\n\n    @property\n    def cookie_jar(self) -> AbstractCookieJar:\n        \"\"\"The session cookies.\"\"\"\n        return self._cookie_jar\n\n    @property\n    def version(self) -> Tuple[int, int]:\n        \"\"\"The session HTTP protocol version.\"\"\"\n        return self._version\n\n    @property\n    def requote_redirect_url(self) -> bool:\n        \"\"\"Do URL requoting on redirection handling.\"\"\"\n        return self._requote_redirect_url\n\n    @property\n    def timeout(self) -> ClientTimeout:\n        \"\"\"Timeout for the session.\"\"\"\n        return self._timeout\n\n    @property\n    def headers(self) -> \"CIMultiDict[str]\":\n        \"\"\"The default headers of the client session.\"\"\"\n        return self._default_headers\n\n    @property\n    def skip_auto_headers(self) -> FrozenSet[istr]:\n        \"\"\"Headers for which autogeneration should be skipped\"\"\"\n        return self._skip_auto_headers\n\n    @property\n    def auth(self) -> Optional[BasicAuth]:\n        \"\"\"An object that represents HTTP Basic Authorization\"\"\"\n        return self._default_auth\n\n    @property\n    def json_serialize(self) -> JSONEncoder:\n        \"\"\"Json serializer callable\"\"\"\n        return self._json_serialize\n\n    @property\n    def connector_owner(self) -> bool:\n        \"\"\"Should connector be closed on session closing\"\"\"\n        return self._connector_owner\n\n    @property\n    def raise_for_status(\n        self,\n    ) -> Union[bool, Callable[[ClientResponse], Awaitable[None]]]:\n        \"\"\"Should `ClientResponse.raise_for_status()` be called for each response.\"\"\"\n        return self._raise_for_status\n\n    @property\n    def auto_decompress(self) -> bool:\n        \"\"\"Should the body response be automatically decompressed.\"\"\"\n        return self._auto_decompress\n\n    @property\n    def trust_env(self) -> bool:\n        \"\"\"\n        Should proxies information from environment or netrc be trusted.\n\n        Information is from HTTP_PROXY / HTTPS_PROXY environment variables\n        or ~/.netrc file if present.\n        \"\"\"\n        return self._trust_env\n\n    @property\n    def trace_configs(self) -> List[TraceConfig[Any]]:\n        \"\"\"A list of TraceConfig instances used for client tracing\"\"\"\n        return self._trace_configs\n\n    def detach(self) -> None:\n        \"\"\"Detach connector from session without closing the former.\n\n        Session is switched to closed state anyway.\n        \"\"\"\n        self._connector = None\n\n    async def __aenter__(self) -> \"ClientSession\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        await self.close()"
    },
    {
      "chunk_id": 388,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "class _BaseRequestContextManager(Coroutine[Any, Any, _RetType], Generic[_RetType]):\n    __slots__ = (\"_coro\", \"_resp\")\n\n    def __init__(self, coro: Coroutine[\"asyncio.Future[Any]\", None, _RetType]) -> None:\n        self._coro: Coroutine[\"asyncio.Future[Any]\", None, _RetType] = coro\n\n    def send(self, arg: None) -> \"asyncio.Future[Any]\":\n        return self._coro.send(arg)\n\n    def throw(self, *args: Any, **kwargs: Any) -> \"asyncio.Future[Any]\":\n        return self._coro.throw(*args, **kwargs)\n\n    def close(self) -> None:\n        return self._coro.close()\n\n    def __await__(self) -> Generator[Any, None, _RetType]:\n        ret = self._coro.__await__()\n        return ret\n\n    def __iter__(self) -> Generator[Any, None, _RetType]:\n        return self.__await__()\n\n    async def __aenter__(self) -> _RetType:\n        self._resp: _RetType = await self._coro\n        return await self._resp.__aenter__()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        await self._resp.__aexit__(exc_type, exc, tb)"
    },
    {
      "chunk_id": 389,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "_RequestContextManager = _BaseRequestContextManager[ClientResponse]\n_WSRequestContextManager = _BaseRequestContextManager[ClientWebSocketResponse]"
    },
    {
      "chunk_id": 390,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "class _SessionRequestContextManager:\n    __slots__ = (\"_coro\", \"_resp\", \"_session\")\n\n    def __init__(\n        self,\n        coro: Coroutine[\"asyncio.Future[Any]\", None, ClientResponse],\n        session: ClientSession,\n    ) -> None:\n        self._coro = coro\n        self._resp: Optional[ClientResponse] = None\n        self._session = session\n\n    async def __aenter__(self) -> ClientResponse:\n        try:\n            self._resp = await self._coro\n        except BaseException:\n            await self._session.close()\n            raise\n        else:\n            return self._resp\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        assert self._resp is not None\n        self._resp.close()\n        await self._session.close()"
    },
    {
      "chunk_id": 391,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client.py",
      "content": "if sys.version_info >= (3, 11) and TYPE_CHECKING:\n\n    def request(\n        method: str,\n        url: StrOrURL,\n        *,\n        version: HttpVersion = http.HttpVersion11,\n        connector: Optional[BaseConnector] = None,\n        **kwargs: Unpack[_RequestOptions],\n    ) -> _SessionRequestContextManager: ...\n\nelse:\n\n    def request(\n        method: str,\n        url: StrOrURL,\n        *,\n        version: HttpVersion = http.HttpVersion11,\n        connector: Optional[BaseConnector] = None,\n        **kwargs: Any,\n    ) -> _SessionRequestContextManager:\n        \"\"\"Constructs and sends a request.\n\n        Returns response object.\n        method - HTTP method\n        url - request url\n        params - (optional) Dictionary or bytes to be sent in the query\n        string of the new request\n        data - (optional) Dictionary, bytes, or file-like object to\n        send in the body of the request\n        json - (optional) Any json compatible python object\n        headers - (optional) Dictionary of HTTP Headers to send with\n        the request\n        cookies - (optional) Dict object to send with the request\n        auth - (optional) BasicAuth named tuple represent HTTP Basic Auth\n        auth - aiohttp.helpers.BasicAuth\n        allow_redirects - (optional) If set to False, do not follow\n        redirects\n        version - Request HTTP version.\n        compress - Set to True if request has to be compressed\n        with deflate encoding.\n        chunked - Set to chunk size for chunked transfer encoding.\n        expect100 - Expect 100-continue response from server.\n        connector - BaseConnector sub-class instance to support\n        connection pooling.\n        read_until_eof - Read response until eof if response\n        does not have Content-Length header.\n        loop - Optional event loop.\n        timeout - Optional ClientTimeout settings structure, 5min\n        total timeout by default.\n        Usage::\n        >>> import aiohttp\n        >>> async with aiohttp.request('GET', 'http://python.org/') as resp:\n        ...    print(resp)\n        ...    data = await resp.read()\n        <ClientResponse(https://www.python.org/) [200 OK]>\n        \"\"\"\n        connector_owner = False\n        if connector is None:\n            connector_owner = True\n            connector = TCPConnector(force_close=True)\n\n        session = ClientSession(\n            cookies=kwargs.pop(\"cookies\", None),\n            version=version,\n            timeout=kwargs.pop(\"timeout\", sentinel),\n            connector=connector,\n            connector_owner=connector_owner,\n        )\n\n        return _SessionRequestContextManager(\n            session._request(method, url, **kwargs),\n            session,\n        )\n```"
    },
    {
      "chunk_id": 392,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "\"\"\"HTTP related errors.\"\"\"\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom multidict import MultiMapping\n\nfrom .typedefs import StrOrURL\n\nif TYPE_CHECKING:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nelse:\n    try:\n        import ssl\n\n        SSLContext = ssl.SSLContext\n    except ImportError:  # pragma: no cover\n        ssl = SSLContext = None  # type: ignore[assignment]\n\nif TYPE_CHECKING:\n    from .client_reqrep import ClientResponse, ConnectionKey, Fingerprint, RequestInfo\n    from .http_parser import RawResponseMessage\nelse:\n    RequestInfo = ClientResponse = ConnectionKey = RawResponseMessage = None\n\n__all__ = (\n    \"ClientError\",\n    \"ClientConnectionError\",\n    \"ClientConnectionResetError\",\n    \"ClientOSError\",\n    \"ClientConnectorError\",\n    \"ClientProxyConnectionError\",\n    \"ClientSSLError\",\n    \"ClientConnectorDNSError\",\n    \"ClientConnectorSSLError\",\n    \"ClientConnectorCertificateError\",\n    \"ConnectionTimeoutError\",\n    \"SocketTimeoutError\",\n    \"ServerConnectionError\",\n    \"ServerTimeoutError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ClientResponseError\",\n    \"ClientHttpProxyError\",\n    \"WSServerHandshakeError\",\n    \"ContentTypeError\",\n    \"ClientPayloadError\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"RedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"WSMessageTypeError\",\n)"
    },
    {
      "chunk_id": 393,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientError(Exception):\n    \"\"\"Base class for client connection errors.\"\"\""
    },
    {
      "chunk_id": 394,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientResponseError(ClientError):\n    \"\"\"Base class for exceptions that occur after getting a response.\n\n    request_info: An instance of RequestInfo.\n    history: A sequence of responses, if redirects occurred.\n    status: HTTP status code.\n    message: Error message.\n    headers: Response headers.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_info: RequestInfo,\n        history: Tuple[ClientResponse, ...],\n        *,\n        status: Optional[int] = None,\n        message: str = \"\",\n        headers: Optional[MultiMapping[str]] = None,\n    ) -> None:\n        self.request_info = request_info\n        if status is not None:\n            self.status = status\n        else:\n            self.status = 0\n        self.message = message\n        self.headers = headers\n        self.history = history\n        self.args = (request_info, history)\n\n    def __str__(self) -> str:\n        return \"{}, message={!r}, url={!r}\".format(\n            self.status,\n            self.message,\n            str(self.request_info.real_url),\n        )\n\n    def __repr__(self) -> str:\n        args = f\"{self.request_info!r}, {self.history!r}\"\n        if self.status != 0:\n            args += f\", status={self.status!r}\"\n        if self.message != \"\":\n            args += f\", message={self.message!r}\"\n        if self.headers is not None:\n            args += f\", headers={self.headers!r}\"\n        return f\"{type(self).__name__}({args})\""
    },
    {
      "chunk_id": 395,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ContentTypeError(ClientResponseError):\n    \"\"\"ContentType found is not valid.\"\"\""
    },
    {
      "chunk_id": 396,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class WSServerHandshakeError(ClientResponseError):\n    \"\"\"websocket server handshake error.\"\"\""
    },
    {
      "chunk_id": 397,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientHttpProxyError(ClientResponseError):\n    \"\"\"HTTP proxy error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n    proxy responds with status other than ``200 OK``\n    on ``CONNECT`` request.\n    \"\"\""
    },
    {
      "chunk_id": 398,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class TooManyRedirects(ClientResponseError):\n    \"\"\"Client was redirected too many times.\"\"\""
    },
    {
      "chunk_id": 399,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectionError(ClientError):\n    \"\"\"Base class for client socket errors.\"\"\""
    },
    {
      "chunk_id": 400,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectionResetError(ClientConnectionError, ConnectionResetError):\n    \"\"\"ConnectionResetError\"\"\""
    },
    {
      "chunk_id": 401,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientOSError(ClientConnectionError, OSError):\n    \"\"\"OSError error.\"\"\""
    },
    {
      "chunk_id": 402,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectorError(ClientOSError):\n    \"\"\"Client connector error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        a connection can not be established.\n    \"\"\"\n\n    def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:\n        self._conn_key = connection_key\n        self._os_error = os_error\n        super().__init__(os_error.errno, os_error.strerror)\n        self.args = (connection_key, os_error)\n\n    @property\n    def os_error(self) -> OSError:\n        return self._os_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> Union[SSLContext, bool, \"Fingerprint\"]:\n        return self._conn_key.ssl\n\n    def __str__(self) -> str:\n        return \"Cannot connect to host {0.host}:{0.port} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )\n\n    # OSError.__reduce__ does too much black magick\n    __reduce__ = BaseException.__reduce__"
    },
    {
      "chunk_id": 403,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectorDNSError(ClientConnectorError):\n    \"\"\"DNS resolution failed during client connection.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        DNS resolution fails.\n    \"\"\""
    },
    {
      "chunk_id": 404,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientProxyConnectionError(ClientConnectorError):\n    \"\"\"Proxy connection error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        connection to proxy can not be established.\n    \"\"\""
    },
    {
      "chunk_id": 405,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class UnixClientConnectorError(ClientConnectorError):\n    \"\"\"Unix connector error.\n\n    Raised in :py:class:`aiohttp.connector.UnixConnector`\n    if connection to unix socket can not be established.\n    \"\"\"\n\n    def __init__(\n        self, path: str, connection_key: ConnectionKey, os_error: OSError\n    ) -> None:\n        self._path = path\n        super().__init__(connection_key, os_error)\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def __str__(self) -> str:\n        return \"Cannot connect to unix socket {0.path} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )"
    },
    {
      "chunk_id": 406,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ServerConnectionError(ClientConnectionError):\n    \"\"\"Server connection errors.\"\"\""
    },
    {
      "chunk_id": 407,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ServerDisconnectedError(ServerConnectionError):\n    \"\"\"Server disconnected.\"\"\"\n\n    def __init__(self, message: Union[RawResponseMessage, str, None] = None) -> None:\n        if message is None:\n            message = \"Server disconnected\"\n\n        self.args = (message,)\n        self.message = message"
    },
    {
      "chunk_id": 408,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ServerTimeoutError(ServerConnectionError, asyncio.TimeoutError):\n    \"\"\"Server timeout error.\"\"\""
    },
    {
      "chunk_id": 409,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ConnectionTimeoutError(ServerTimeoutError):\n    \"\"\"Connection timeout error.\"\"\""
    },
    {
      "chunk_id": 410,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class SocketTimeoutError(ServerTimeoutError):\n    \"\"\"Socket timeout error.\"\"\""
    },
    {
      "chunk_id": 411,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ServerFingerprintMismatch(ServerConnectionError):\n    \"\"\"SSL certificate does not match expected fingerprint.\"\"\"\n\n    def __init__(self, expected: bytes, got: bytes, host: str, port: int) -> None:\n        self.expected = expected\n        self.got = got\n        self.host = host\n        self.port = port\n        self.args = (expected, got, host, port)\n\n    def __repr__(self) -> str:\n        return \"<{} expected={!r} got={!r} host={!r} port={!r}>\".format(\n            self.__class__.__name__, self.expected, self.got, self.host, self.port\n        )"
    },
    {
      "chunk_id": 412,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientPayloadError(ClientError):\n    \"\"\"Response payload error.\"\"\""
    },
    {
      "chunk_id": 413,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class InvalidURL(ClientError, ValueError):\n    \"\"\"Invalid URL.\n\n    URL used for fetching is malformed, e.g. it doesn't contains host\n    part.\n    \"\"\"\n\n    # Derive from ValueError for backward compatibility\n\n    def __init__(self, url: StrOrURL, description: Union[str, None] = None) -> None:\n        # The type of url is not yarl.URL because the exception can be raised\n        # on URL(url) call\n        self._url = url\n        self._description = description\n\n        if description:\n            super().__init__(url, description)\n        else:\n            super().__init__(url)\n\n    @property\n    def url(self) -> StrOrURL:\n        return self._url\n\n    @property\n    def description(self) -> \"str | None\":\n        return self._description\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__} {self}>\"\n\n    def __str__(self) -> str:\n        if self._description:\n            return f\"{self._url} - {self._description}\"\n        return str(self._url)"
    },
    {
      "chunk_id": 414,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class InvalidUrlClientError(InvalidURL):\n    \"\"\"Invalid URL client error.\"\"\""
    },
    {
      "chunk_id": 415,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class RedirectClientError(ClientError):\n    \"\"\"Client redirect error.\"\"\""
    },
    {
      "chunk_id": 416,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class NonHttpUrlClientError(ClientError):\n    \"\"\"Non http URL client error.\"\"\""
    },
    {
      "chunk_id": 417,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class InvalidUrlRedirectClientError(InvalidUrlClientError, RedirectClientError):\n    \"\"\"Invalid URL redirect client error.\"\"\""
    },
    {
      "chunk_id": 418,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class NonHttpUrlRedirectClientError(NonHttpUrlClientError, RedirectClientError):\n    \"\"\"Non http URL redirect client error.\"\"\""
    },
    {
      "chunk_id": 419,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientSSLError(ClientConnectorError):\n    \"\"\"Base error for ssl.*Errors.\"\"\""
    },
    {
      "chunk_id": 420,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "if ssl is not None:\n    cert_errors = (ssl.CertificateError,)\n    cert_errors_bases = (\n        ClientSSLError,\n        ssl.CertificateError,\n    )\n\n    ssl_errors = (ssl.SSLError,)\n    ssl_error_bases = (ClientSSLError, ssl.SSLError)\nelse:  # pragma: no cover\n    cert_errors = tuple()  # type: ignore[unreachable]\n    cert_errors_bases = (\n        ClientSSLError,\n        ValueError,\n    )\n\n    ssl_errors = tuple()\n    ssl_error_bases = (ClientSSLError,)"
    },
    {
      "chunk_id": 421,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectorSSLError(*ssl_error_bases):  # type: ignore[misc]\n    \"\"\"Response ssl error.\"\"\""
    },
    {
      "chunk_id": 422,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class ClientConnectorCertificateError(*cert_errors_bases):  # type: ignore[misc]\n    \"\"\"Response certificate error.\"\"\"\n\n    def __init__(\n        self, connection_key: ConnectionKey, certificate_error: Exception\n    ) -> None:\n        self._conn_key = connection_key\n        self._certificate_error = certificate_error\n        self.args = (connection_key, certificate_error)\n\n    @property\n    def certificate_error(self) -> Exception:\n        return self._certificate_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> bool:\n        return self._conn_key.is_ssl\n\n    def __str__(self) -> str:\n        return (\n            \"Cannot connect to host {0.host}:{0.port} ssl:{0.ssl} \"\n            \"[{0.certificate_error.__class__.__name__}: \"\n            \"{0.certificate_error.args}]\".format(self)\n        )"
    },
    {
      "chunk_id": 423,
      "source": "__internal__/data_repo/aiohttp/aiohttp/client_exceptions.py",
      "content": "class WSMessageTypeError(TypeError):\n    \"\"\"WebSocket message type is not valid.\"\"\""
    },
    {
      "chunk_id": 424,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_websocket.py",
      "content": "\"\"\"WebSocket protocol versions 13 and 8.\"\"\""
    },
    {
      "chunk_id": 425,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_websocket.py",
      "content": "from ._websocket.helpers import WS_KEY, ws_ext_gen, ws_ext_parse\nfrom ._websocket.models import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WebSocketError,\n    WSCloseCode,\n    WSHandshakeError,\n    WSMessage,\n    WSMessageBinary,\n    WSMessageClose,\n    WSMessageClosed,\n    WSMessageClosing,\n    WSMessageContinuation,\n    WSMessageError,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageText,\n    WSMsgType,\n)\nfrom ._websocket.reader import WebSocketReader\nfrom ._websocket.writer import WebSocketWriter"
    },
    {
      "chunk_id": 426,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_websocket.py",
      "content": "_INTERNAL_RECEIVE_TYPES = frozenset(\n    (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.PING, WSMsgType.PONG)\n)"
    },
    {
      "chunk_id": 427,
      "source": "__internal__/data_repo/aiohttp/aiohttp/http_websocket.py",
      "content": "__all__ = (\n    \"WS_CLOSED_MESSAGE\",\n    \"WS_CLOSING_MESSAGE\",\n    \"WS_KEY\",\n    \"WebSocketReader\",\n    \"WebSocketWriter\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n    \"ws_ext_gen\",\n    \"ws_ext_parse\",\n    \"WSMessageError\",\n    \"WSHandshakeError\",\n    \"WSMessageClose\",\n    \"WSMessageClosed\",\n    \"WSMessageClosing\",\n    \"WSMessagePong\",\n    \"WSMessageBinary\",\n    \"WSMessageText\",\n    \"WSMessagePing\",\n    \"WSMessageContinuation\",\n)"
    },
    {
      "chunk_id": 428,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "```python"
    },
    {
      "chunk_id": 429,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "import asyncio\nimport functools\nimport logging\nimport random\nimport socket\nimport sys\nimport traceback\nimport warnings\nfrom collections import OrderedDict, defaultdict, deque\nfrom contextlib import suppress\nfrom http import HTTPStatus\nfrom itertools import chain, cycle, islice\nfrom time import monotonic\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    DefaultDict,\n    Deque,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport aiohappyeyeballs\n\nfrom . import hdrs, helpers\nfrom .abc import AbstractResolver, ResolveResult\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientConnectorCertificateError,\n    ClientConnectorDNSError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientHttpProxyError,\n    ClientProxyConnectionError,\n    ServerFingerprintMismatch,\n    UnixClientConnectorError,\n    cert_errors,\n    ssl_errors,\n)\nfrom .client_proto import ResponseHandler\nfrom .client_reqrep import SSL_ALLOWED_TYPES, ClientRequest, Fingerprint\nfrom .helpers import (\n    _SENTINEL,\n    ceil_timeout,\n    is_ip_address,\n    sentinel,\n    set_exception,\n    set_result,\n)\nfrom .resolver import DefaultResolver\n\nif TYPE_CHECKING:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nelse:\n    try:\n        import ssl\n\n        SSLContext = ssl.SSLContext\n    except ImportError:  # pragma: no cover\n        ssl = None  # type: ignore[assignment]\n        SSLContext = object  # type: ignore[misc,assignment]\n\nEMPTY_SCHEMA_SET = frozenset({\"\"})\nHTTP_SCHEMA_SET = frozenset({\"http\", \"https\"})\nWS_SCHEMA_SET = frozenset({\"ws\", \"wss\"})\n\nHTTP_AND_EMPTY_SCHEMA_SET = HTTP_SCHEMA_SET | EMPTY_SCHEMA_SET\nHIGH_LEVEL_SCHEMA_SET = HTTP_AND_EMPTY_SCHEMA_SET | WS_SCHEMA_SET\n\nNEEDS_CLEANUP_CLOSED = (3, 13, 0) <= sys.version_info < (\n    3,\n    13,\n    1,\n) or sys.version_info < (3, 12, 7)\n# Cleanup closed is no longer needed after https://github.com/python/cpython/pull/118960\n# which first appeared in Python 3.12.7 and 3.13.1\n\n\n__all__ = (\"BaseConnector\", \"TCPConnector\", \"UnixConnector\", \"NamedPipeConnector\")\n\n\nif TYPE_CHECKING:\n    from .client import ClientTimeout\n    from .client_reqrep import ConnectionKey\n    from .tracing import Trace"
    },
    {
      "chunk_id": 430,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class Connection:\n    \"\"\"Represents a single connection.\"\"\"\n\n    __slots__ = (\n        \"_key\",\n        \"_connector\",\n        \"_loop\",\n        \"_protocol\",\n        \"_callbacks\",\n        \"_source_traceback\",\n    )\n\n    def __init__(\n        self,\n        connector: \"BaseConnector\",\n        key: \"ConnectionKey\",\n        protocol: ResponseHandler,\n        loop: asyncio.AbstractEventLoop,\n    ) -> None:\n        self._key = key\n        self._connector = connector\n        self._loop = loop\n        self._protocol: Optional[ResponseHandler] = protocol\n        self._callbacks: List[Callable[[], None]] = []\n        self._source_traceback = (\n            traceback.extract_stack(sys._getframe(1)) if loop.get_debug() else None\n        )\n\n    def __repr__(self) -> str:\n        return f\"Connection<{self._key}>\"\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._protocol is not None:\n            _warnings.warn(\n                f\"Unclosed connection {self!r}\", ResourceWarning, source=self\n            )\n            if self._loop.is_closed():\n                return\n\n            self._connector._release(self._key, self._protocol, should_close=True)\n\n            context = {\"client_connection\": self, \"message\": \"Unclosed connection\"}\n            if self._source_traceback is not None:\n                context[\"source_traceback\"] = self._source_traceback\n            self._loop.call_exception_handler(context)\n\n    def __bool__(self) -> Literal[True]:\n        \"\"\"Force subclasses to not be falsy, to make checks simpler.\"\"\"\n        return True\n\n    @property\n    def transport(self) -> Optional[asyncio.Transport]:\n        if self._protocol is None:\n            return None\n        return self._protocol.transport\n\n    @property\n    def protocol(self) -> Optional[ResponseHandler]:\n        return self._protocol\n\n    def add_callback(self, callback: Callable[[], None]) -> None:\n        if callback is not None:\n            self._callbacks.append(callback)\n\n    def _notify_release(self) -> None:\n        callbacks, self._callbacks = self._callbacks[:], []\n\n        for cb in callbacks:\n            with suppress(Exception):\n                cb()\n\n    def close(self) -> None:\n        self._notify_release()\n\n        if self._protocol is not None:\n            self._connector._release(self._key, self._protocol, should_close=True)\n            self._protocol = None\n\n    def release(self) -> None:\n        self._notify_release()\n\n        if self._protocol is not None:\n            self._connector._release(self._key, self._protocol)\n            self._protocol = None\n\n    @property\n    def closed(self) -> bool:\n        return self._protocol is None or not self._protocol.is_connected()"
    },
    {
      "chunk_id": 431,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class _TransportPlaceholder:\n    \"\"\"placeholder for BaseConnector.connect function\"\"\"\n\n    __slots__ = (\"closed\",)\n\n    def __init__(self, closed_future: asyncio.Future[Optional[Exception]]) -> None:\n        \"\"\"Initialize a placeholder for a transport.\"\"\"\n        self.closed = closed_future\n\n    def close(self) -> None:\n        \"\"\"Close the placeholder.\"\"\""
    },
    {
      "chunk_id": 432,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class BaseConnector:\n    \"\"\"Base connector class.\n\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\n                            Disabled by default.\n    timeout_ceil_threshold - Trigger ceiling of timeout values when\n                             it's above timeout_ceil_threshold.\n    loop - Optional event loop.\n    \"\"\"\n\n    _closed = True  # prevent AttributeError in __del__ if ctor was failed\n    _source_traceback = None\n\n    # abort transport after 2 seconds (cleanup broken connections)\n    _cleanup_closed_period = 2.0\n\n    allowed_protocol_schema_set = HIGH_LEVEL_SCHEMA_SET\n\n    def __init__(\n        self,\n        *,\n        keepalive_timeout: Union[_SENTINEL, None, float] = sentinel,\n        force_close: bool = False,\n        limit: int = 100,\n        limit_per_host: int = 0,\n        enable_cleanup_closed: bool = False,\n        timeout_ceil_threshold: float = 5,\n    ) -> None:\n        if force_close:\n            if keepalive_timeout is not None and keepalive_timeout is not sentinel:\n                raise ValueError(\n                    \"keepalive_timeout cannot be set if force_close is True\"\n                )\n        else:\n            if keepalive_timeout is sentinel:\n                keepalive_timeout = 15.0\n\n        self._timeout_ceil_threshold = timeout_ceil_threshold\n\n        loop = asyncio.get_running_loop()\n\n        self._closed = False\n        if loop.get_debug():\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\n\n        # Connection pool of reusable connections.\n        # We use a deque to store connections because it has O(1) popleft()\n        # and O(1) append() operations to implement a FIFO queue.\n        self._conns: DefaultDict[\n            ConnectionKey, Deque[Tuple[ResponseHandler, float]]\n        ] = defaultdict(deque)\n        self._limit = limit\n        self._limit_per_host = limit_per_host\n        self._acquired: Set[ResponseHandler] = set()\n        self._acquired_per_host: DefaultDict[ConnectionKey, Set[ResponseHandler]] = (\n            defaultdict(set)\n        )\n        self._keepalive_timeout = cast(float, keepalive_timeout)\n        self._force_close = force_close\n\n        # {host_key: FIFO list of waiters}\n        # The FIFO is implemented with an OrderedDict with None keys because\n        # python does not have an ordered set.\n        self._waiters: DefaultDict[\n            ConnectionKey, OrderedDict[asyncio.Future[None], None]\n        ] = defaultdict(OrderedDict)\n\n        self._loop = loop\n        self._factory = functools.partial(ResponseHandler, loop=loop)\n\n        # start keep-alive connection cleanup task\n        self._cleanup_handle: Optional[asyncio.TimerHandle] = None\n\n        # start cleanup closed transports task\n        self._cleanup_closed_handle: Optional[asyncio.TimerHandle] = None\n\n        if enable_cleanup_closed and not NEEDS_CLEANUP_CLOSED:\n            warnings.warn(\n                \"enable_cleanup_closed ignored because \"\n                \"https://github.com/python/cpython/pull/118960 is fixed \"\n                f\"in Python version {sys.version_info}\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            enable_cleanup_closed = False\n\n        self._cleanup_closed_disabled = not enable_cleanup_closed\n        self._cleanup_closed_transports: List[Optional[asyncio.Transport]] = []\n\n        self._placeholder_future: asyncio.Future[Optional[Exception]] = (\n            loop.create_future()\n        )\n        self._placeholder_future.set_result(None)\n        self._cleanup_closed()\n\n    def __del__(self, _warnings: Any = warnings) -> None:\n        if self._closed:\n            return\n        if not self._conns:\n            return\n\n        conns = [repr(c) for c in self._conns.values()]\n\n        self._close_immediately()\n\n        _warnings.warn(f\"Unclosed connector {self!r}\", ResourceWarning, source=self)\n        context = {\n            \"connector\": self,\n            \"connections\": conns,\n            \"message\": \"Unclosed connector\",\n        }\n        if self._source_traceback is not None:\n            context[\"source_traceback\"] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    async def __aenter__(self) -> \"BaseConnector\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]] = None,\n        exc_value: Optional[BaseException] = None,\n        exc_traceback: Optional[TracebackType] = None,\n    ) -> None:\n        await self.close()\n\n    @property\n    def force_close(self) -> bool:\n        \"\"\"Ultimately close connection on releasing if True.\"\"\"\n        return self._force_close\n\n    @property\n    def limit(self) -> int:\n        \"\"\"The total number for simultaneous connections.\n\n        If limit is 0 the connector has no limit.\n        The default limit size is 100.\n        \"\"\"\n        return self._limit\n\n    @property\n    def limit_per_host(self) -> int:\n        \"\"\"The limit for simultaneous connections to the same endpoint.\n\n        Endpoints are the same if they are have equal\n        (host, port, is_ssl) triple.\n        \"\"\"\n        return self._limit_per_host\n\n    def _cleanup(self) -> None:\n        \"\"\"Cleanup unused transports.\"\"\"\n        if self._cleanup_handle:\n            self._cleanup_handle.cancel()\n            # _cleanup_handle should be unset, otherwise _release() will not\n            # recreate it ever!\n            self._cleanup_handle = None\n\n        now = monotonic()\n        timeout = self._keepalive_timeout\n\n        if self._conns:\n            connections = defaultdict(deque)\n            deadline = now - timeout\n            for key, conns in self._conns.items():\n                alive: Deque[Tuple[ResponseHandler, float]] = deque()\n                for proto, use_time in conns:\n                    if proto.is_connected() and use_time - deadline >= 0:\n                        alive.append((proto, use_time))\n                        continue\n                    transport = proto.transport\n                    proto.close()\n                    if not self._cleanup_closed_disabled and key.is_ssl:\n                        self._cleanup_closed_transports.append(transport)\n\n                if alive:\n                    connections[key] = alive\n\n            self._conns = connections\n\n        if self._conns:\n            self._cleanup_handle = helpers.weakref_handle(\n                self,\n                \"_cleanup\",\n                timeout,\n                self._loop,\n                timeout_ceil_threshold=self._timeout_ceil_threshold,\n            )\n\n    def _cleanup_closed(self) -> None:\n        \"\"\"Double confirmation for transport close.\n\n        Some broken ssl servers may leave socket open without proper close.\n        \"\"\"\n        if self._cleanup_closed_handle:\n            self._cleanup_closed_handle.cancel()\n\n        for transport in self._cleanup_closed_transports:\n            if transport is not None:\n                transport.abort()\n\n        self._cleanup_closed_transports = []\n\n        if not self._cleanup_closed_disabled:\n            self._cleanup_closed_handle = helpers.weakref_handle(\n                self,\n                \"_cleanup_closed\",\n                self._cleanup_closed_period,\n                self._loop,\n                timeout_ceil_threshold=self._timeout_ceil_threshold,\n            )\n\n    async def close(self) -> None:\n        \"\"\"Close all opened transports.\"\"\"\n        waiters = self._close_immediately()\n        if waiters:\n            results = await asyncio.gather(*waiters, return_exceptions=True)\n            for res in results:\n                if isinstance(res, Exception):\n                    err_msg = \"Error while closing connector: \" + repr(res)\n                    logging.error(err_msg)\n\n    def _close_immediately(self) -> List[Awaitable[object]]:\n        waiters: List[Awaitable[object]] = []\n\n        if self._closed:\n            return waiters\n\n        self._closed = True\n\n        try:\n            if self._loop.is_closed():\n                return waiters\n\n            # cancel cleanup task\n            if self._cleanup_handle:\n                self._cleanup_handle.cancel()\n\n            # cancel cleanup close task\n            if self._cleanup_closed_handle:\n                self._cleanup_closed_handle.cancel()\n\n            for data in self._conns.values():\n                for proto, t0 in data:\n                    proto.close()\n                    waiters.append(proto.closed)\n\n            for proto in self._acquired:\n                proto.close()\n                waiters.append(proto.closed)\n\n            # TODO (A.Yushovskiy, 24-May-2019) collect transp. closing futures\n            for transport in self._cleanup_closed_transports:\n                if transport is not None:\n                    transport.abort()\n\n            return waiters\n\n        finally:\n            self._conns.clear()\n            self._acquired.clear()\n            for keyed_waiters in self._waiters.values():\n                for keyed_waiter in keyed_waiters:\n                    keyed_waiter.cancel()\n            self._waiters.clear()\n            self._cleanup_handle = None\n            self._cleanup_closed_transports.clear()\n            self._cleanup_closed_handle = None\n\n    @property\n    def closed(self) -> bool:\n        \"\"\"Is connector closed.\n\n        A readonly property.\n        \"\"\"\n        return self._closed\n\n    def _available_connections(self, key: \"ConnectionKey\") -> int:\n        \"\"\"\n        Return number of available connections.\n\n        The limit, limit_per_host and the connection key are taken into account.\n\n        If it returns less than 1 means that there are no connections\n        available.\n        \"\"\"\n        # check total available connections\n        # If there are no limits, this will always return 1\n        total_remain = 1\n\n        if self._limit and (total_remain := self._limit - len(self._acquired)) <= 0:\n            return total_remain\n\n        # check limit per host\n        if host_remain := self._limit_per_host:\n            if acquired := self._acquired_per_host.get(key):\n                host_remain -= len(acquired)\n            if total_remain > host_remain:\n                return host_remain\n\n        return total_remain\n\n    async def connect(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> Connection:\n        \"\"\"Get from pool or create new connection.\"\"\"\n        key = req.connection_key\n        if (conn := await self._get(key, traces)) is not None:\n            # If we do not have to wait and we can get a connection from the pool\n            # we can avoid the timeout ceil logic and directly return the connection\n            return conn\n\n        async with ceil_timeout(timeout.connect, timeout.ceil_threshold):\n            if self._available_connections(key) <= 0:\n                await self._wait_for_available_connection(key, traces)\n                if (conn := await self._get(key, traces)) is not None:\n                    return conn\n\n            placeholder = cast(\n                ResponseHandler, _TransportPlaceholder(self._placeholder_future)\n            )\n            self._acquired.add(placeholder)\n            if self._limit_per_host:\n                self._acquired_per_host[key].add(placeholder)\n\n            try:\n                # Traces are done inside the try block to ensure that the\n                # that the placeholder is still cleaned up if an exception\n                # is raised.\n                if traces:\n                    for trace in traces:\n                        await trace.send_connection_create_start()\n                proto = await self._create_connection(req, traces, timeout)\n                if traces:\n                    for trace in traces:\n                        await trace.send_connection_create_end()\n            except BaseException:\n                self._release_acquired(key, placeholder)\n                raise\n            else:\n                if self._closed:\n                    proto.close()\n                    raise ClientConnectionError(\"Connector is closed.\")\n\n        # The connection was successfully created, drop the placeholder\n        # and add the real connection to the acquired set. There should\n        # be no awaits after the proto is added to the acquired set\n        # to ensure that the connection is not left in the acquired set\n        # on cancellation.\n        self._acquired.remove(placeholder)\n        self._acquired.add(proto)\n        if self._limit_per_host:\n            acquired_per_host = self._acquired_per_host[key]\n            acquired_per_host.remove(placeholder)\n            acquired_per_host.add(proto)\n        return Connection(self, key, proto, self._loop)\n\n    async def _wait_for_available_connection(\n        self, key: \"ConnectionKey\", traces: List[\"Trace\"]\n    ) -> None:\n        \"\"\"Wait for an available connection slot.\"\"\"\n        # We loop here because there is a race between\n        # the connection limit check and the connection\n        # being acquired. If the connection is acquired\n        # between the check and the await statement, we\n        # need to loop again to check if the connection\n        # slot is still available.\n        attempts = 0\n        while True:\n            fut: asyncio.Future[None] = self._loop.create_future()\n            keyed_waiters = self._waiters[key]\n            keyed_waiters[fut] = None\n            if attempts:\n                # If we have waited before, we need to move the waiter\n                # to the front of the queue as otherwise we might get\n                # starved and hit the timeout.\n                keyed_waiters.move_to_end(fut, last=False)\n\n            try:\n                # Traces happen in the try block to ensure that the\n                # the waiter is still cleaned up if an exception is raised.\n                if traces:\n                    for trace in traces:\n                        await trace.send_connection_queued_start()\n                await fut\n                if traces:\n                    for trace in traces:\n                        await trace.send_connection_queued_end()\n            finally:\n                # pop the waiter from the queue if its still\n                # there and not already removed by _release_waiter\n                keyed_waiters.pop(fut, None)\n                if not self._waiters.get(key, True):\n                    del self._waiters[key]\n\n            if self._available_connections(key) > 0:\n                break\n            attempts += 1\n\n    async def _get(\n        self, key: \"ConnectionKey\", traces: List[\"Trace\"]\n    ) -> Optional[Connection]:\n        \"\"\"Get next reusable connection for the key or None.\n\n        The connection will be marked as acquired.\n        \"\"\"\n        if (conns := self._conns.get(key)) is None:\n            return None\n\n        t1 = monotonic()\n        while conns:\n            proto, t0 = conns.popleft()\n            # We will we reuse the connection if its connected and\n            # the keepalive timeout has not been exceeded\n            if proto.is_connected() and t1 - t0 <= self._keepalive_timeout:\n                if not conns:\n                    # The very last connection was reclaimed: drop the key\n                    del self._conns[key]\n                self._acquired.add(proto)\n                if self._limit_per_host:\n                    self._acquired_per_host[key].add(proto)\n                if traces:\n                    for trace in traces:\n                        try:\n                            await trace.send_connection_reuseconn()\n                        except BaseException:\n                            self._release_acquired(key, proto)\n                            raise\n                return Connection(self, key, proto, self._loop)\n\n            # Connection cannot be reused, close it\n            transport = proto.transport\n            proto.close()\n            # only for SSL transports\n            if not self._cleanup_closed_disabled and key.is_ssl:\n                self._cleanup_closed_transports.append(transport)\n\n        # No more connections: drop the key\n        del self._conns[key]\n        return None\n\n    def _release_waiter(self) -> None:\n        \"\"\"\n        Iterates over all waiters until one to be released is found.\n\n        The one to be released is not finished and\n        belongs to a host that has available connections.\n        \"\"\"\n        if not self._waiters:\n            return\n\n        # Having the dict keys ordered this avoids to iterate\n        # at the same order at each call.\n        queues = list(self._waiters)\n        random.shuffle(queues)\n\n        for key in queues:\n            if self._available_connections(key) < 1:\n                continue\n\n            waiters = self._waiters[key]\n            while waiters:\n                waiter, _ = waiters.popitem(last=False)\n                if not waiter.done():\n                    waiter.set_result(None)\n                    return\n\n    def _release_acquired(self, key: \"ConnectionKey\", proto: ResponseHandler) -> None:\n        \"\"\"Release acquired connection.\"\"\"\n        if self._closed:\n            # acquired connection is already released on connector closing\n            return\n\n        self._acquired.discard(proto)\n        if self._limit_per_host and (conns := self._acquired_per_host.get(key)):\n            conns.discard(proto)\n            if not conns:\n                del self._acquired_per_host[key]\n        self._release_waiter()\n\n    def _release(\n        self,\n        key: \"ConnectionKey\",\n        protocol: ResponseHandler,\n        *,\n        should_close: bool = False,\n    ) -> None:\n        if self._closed:\n            # acquired connection is already released on connector closing\n            return\n\n        self._release_acquired(key, protocol)\n\n        if self._force_close or should_close or protocol.should_close:\n            transport = protocol.transport\n            protocol.close()\n            if key.is_ssl and not self._cleanup_closed_disabled:\n                self._cleanup_closed_transports.append(transport)\n            return\n\n        self._conns[key].append((protocol, monotonic()))\n\n        if self._cleanup_handle is None:\n            self._cleanup_handle = helpers.weakref_handle(\n                self,\n                \"_cleanup\",\n                self._keepalive_timeout,\n                self._loop,\n                timeout_ceil_threshold=self._timeout_ceil_threshold,\n            )\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        raise NotImplementedError()"
    },
    {
      "chunk_id": 433,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class _DNSCacheTable:\n    def __init__(self, ttl: Optional[float] = None) -> None:\n        self._addrs_rr: Dict[Tuple[str, int], Tuple[Iterator[ResolveResult], int]] = {}\n        self._timestamps: Dict[Tuple[str, int], float] = {}\n        self._ttl = ttl\n\n    def __contains__(self, host: object) -> bool:\n        return host in self._addrs_rr\n\n    def add(self, key: Tuple[str, int], addrs: List[ResolveResult]) -> None:\n        self._addrs_rr[key] = (cycle(addrs), len(addrs))\n\n        if self._ttl is not None:\n            self._timestamps[key] = monotonic()\n\n    def remove(self, key: Tuple[str, int]) -> None:\n        self._addrs_rr.pop(key, None)\n\n        if self._ttl is not None:\n            self._timestamps.pop(key, None)\n\n    def clear(self) -> None:\n        self._addrs_rr.clear()\n        self._timestamps.clear()\n\n    def next_addrs(self, key: Tuple[str, int]) -> List[ResolveResult]:\n        loop, length = self._addrs_rr[key]\n        addrs = list(islice(loop, length))\n        # Consume one more element to shift internal state of `cycle`\n        next(loop)\n        return addrs\n\n    def expired(self, key: Tuple[str, int]) -> bool:\n        if self._ttl is None:\n            return False\n\n        return self._timestamps[key] + self._ttl < monotonic()"
    },
    {
      "chunk_id": 434,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "def _make_ssl_context(verified: bool) -> SSLContext:\n    \"\"\"Create SSL context.\n\n    This method is not async-friendly and should be called from a thread\n    because it will load certificates from disk and do other blocking I/O.\n    \"\"\"\n    if ssl is None:\n        # No ssl support\n        return None  # type: ignore[unreachable]\n    if verified:\n        sslcontext = ssl.create_default_context()\n    else:\n        sslcontext = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n        sslcontext.options |= ssl.OP_NO_SSLv2\n        sslcontext.options |= ssl.OP_NO_SSLv3\n        sslcontext.check_hostname = False\n        sslcontext.verify_mode = ssl.CERT_NONE\n        sslcontext.options |= ssl.OP_NO_COMPRESSION\n        sslcontext.set_default_verify_paths()\n    sslcontext.set_alpn_protocols((\"http/1.1\",))\n    return sslcontext"
    },
    {
      "chunk_id": 435,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "_SSL_CONTEXT_VERIFIED = _make_ssl_context(True)\n_SSL_CONTEXT_UNVERIFIED = _make_ssl_context(False)"
    },
    {
      "chunk_id": 436,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class TCPConnector(BaseConnector):\n    \"\"\"TCP connector.\n\n    verify_ssl - Set to True to check ssl certifications.\n    fingerprint - Pass the binary sha256\n        digest of the expected certificate in DER format to verify\n        that the certificate the server presents matches. See also\n        https://en.wikipedia.org/wiki/HTTP_Public_Key_Pinning\n    resolver - Enable DNS lookups and use this\n        resolver\n    use_dns_cache - Use memory cache for DNS lookups.\n    ttl_dns_cache - Max seconds having cached a DNS entry, None forever.\n    family - socket address family\n    local_addr - local tuple of (host, port) to bind socket to\n\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\n                            Disabled by default.\n    happy_eyeballs_delay - This is the \u201cConnection Attempt Delay\u201d\n                           as defined in RFC 8305. To disable\n                           the happy eyeballs algorithm, set to None.\n    interleave - \u201cFirst Address Family Count\u201d as defined in RFC 8305\n    loop - Optional event loop.\n    \"\"\"\n\n    allowed_protocol_schema_set = HIGH_LEVEL_SCHEMA_SET | frozenset({\"tcp\"})\n\n    def __init__(\n        self,\n        *,\n        use_dns_cache: bool = True,\n        ttl_dns_cache: Optional[int] = 10,\n        family: socket.AddressFamily = socket.AddressFamily.AF_UNSPEC,\n        ssl: Union[bool, Fingerprint, SSLContext] = True,\n        local_addr: Optional[Tuple[str, int]] = None,\n        resolver: Optional[AbstractResolver] = None,\n        keepalive_timeout: Union[None, float, _SENTINEL] = sentinel,\n        force_close: bool = False,\n        limit: int = 100,\n        limit_per_host: int = 0,\n        enable_cleanup_closed: bool = False,\n        timeout_ceil_threshold: float = 5,\n        happy_eyeballs_delay: Optional[float] = 0.25,\n        interleave: Optional[int] = None,\n    ):\n        super().__init__(\n            keepalive_timeout=keepalive_timeout,\n            force_close=force_close,\n            limit=limit,\n            limit_per_host=limit_per_host,\n            enable_cleanup_closed=enable_cleanup_closed,\n            timeout_ceil_threshold=timeout_ceil_threshold,\n        )\n\n        if not isinstance(ssl, SSL_ALLOWED_TYPES):\n            raise TypeError(\n                \"ssl should be SSLContext, Fingerprint, or bool, \"\n                \"got {!r} instead.\".format(ssl)\n            )\n        self._ssl = ssl\n        if resolver is None:\n            resolver = DefaultResolver()\n        self._resolver: AbstractResolver = resolver\n\n        self._use_dns_cache = use_dns_cache\n        self._cached_hosts = _DNSCacheTable(ttl=ttl_dns_cache)\n        self._throttle_dns_futures: Dict[Tuple[str, int], Set[asyncio.Future[None]]] = (\n            {}\n        )\n        self._family = family\n        self._local_addr_infos = aiohappyeyeballs.addr_to_addr_infos(local_addr)\n        self._happy_eyeballs_delay = happy_eyeballs_delay\n        self._interleave = interleave\n        self._resolve_host_tasks: Set[\"asyncio.Task[List[ResolveResult]]\"] = set()\n\n    def _close_immediately(self) -> List[Awaitable[object]]:\n        for fut in chain.from_iterable(self._throttle_dns_futures.values()):\n            fut.cancel()\n\n        waiters = super()._close_immediately()\n\n        for t in self._resolve_host_tasks:\n            t.cancel()\n            waiters.append(t)\n\n        return waiters\n\n    @property\n    def family(self) -> int:\n        \"\"\"Socket family like AF_INET.\"\"\"\n        return self._family\n\n    @property\n    def use_dns_cache(self) -> bool:\n        \"\"\"True if local DNS caching is enabled.\"\"\"\n        return self._use_dns_cache\n\n    def clear_dns_cache(\n        self, host: Optional[str] = None, port: Optional[int] = None\n    ) -> None:\n        \"\"\"Remove specified host/port or clear all dns local cache.\"\"\"\n        if host is not None and port is not None:\n            self._cached_hosts.remove((host, port))\n        elif host is not None or port is not None:\n            raise ValueError(\"either both host and port or none of them are allowed\")\n        else:\n            self._cached_hosts.clear()\n\n    async def _resolve_host(\n        self, host: str, port: int, traces: Optional[Sequence[\"Trace\"]] = None\n    ) -> List[ResolveResult]:\n        \"\"\"Resolve host and return list of addresses.\"\"\"\n        if is_ip_address(host):\n            return [\n                {\n                    \"hostname\": host,\n                    \"host\": host,\n                    \"port\": port,\n                    \"family\": self._family,\n                    \"proto\": 0,\n                    \"flags\": 0,\n                }\n            ]\n\n        if not self._use_dns_cache:\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_start(host)\n\n            res = await self._resolver.resolve(host, port, family=self._family)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_end(host)\n\n            return res\n\n        key = (host, port)\n        if key in self._cached_hosts and not self._cached_hosts.expired(key):\n            # get result early, before any await (#4014)\n            result = self._cached_hosts.next_addrs(key)\n\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_cache_hit(host)\n            return result\n\n        futures: Set[asyncio.Future[None]]\n        #\n        # If multiple connectors are resolving the same host, we wait\n        # for the first one to resolve and then use the result for all of them.\n        # We use a throttle to ensure that we only resolve the host once\n        # and then use the result for all the waiters.\n        #\n        if key in self._throttle_dns_futures:\n            # get futures early, before any await (#4014)\n            futures = self._throttle_dns_futures[key]\n            future: asyncio.Future[None] = self._loop.create_future()\n            futures.add(future)\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_cache_hit(host)\n            try:\n                await future\n            finally:\n                futures.discard(future)\n            return self._cached_hosts.next_addrs(key)\n\n        # update dict early, before any await (#4014)\n        self._throttle_dns_futures[key] = futures = set()\n        # In this case we need to create a task to ensure that we can shield\n        # the task from cancellation as cancelling this lookup should not cancel\n        # the underlying lookup or else the cancel event will get broadcast to\n        # all the waiters across all connections.\n        #\n        coro = self._resolve_host_with_throttle(key, host, port, futures, traces)\n        loop = asyncio.get_running_loop()\n        if sys.version_info >= (3, 12):\n            # Optimization for Python 3.12, try to send immediately\n            resolved_host_task = asyncio.Task(coro, loop=loop, eager_start=True)\n        else:\n            resolved_host_task = loop.create_task(coro)\n\n        if not resolved_host_task.done():\n            self._resolve_host_tasks.add(resolved_host_task)\n            resolved_host_task.add_done_callback(self._resolve_host_tasks.discard)\n\n        try:\n            return await asyncio.shield(resolved_host_task)\n        except asyncio.CancelledError:\n\n            def drop_exception(fut: \"asyncio.Future[List[ResolveResult]]\") -> None:\n                with suppress(Exception, asyncio.CancelledError):\n                    fut.result()\n\n            resolved_host_task.add_done_callback(drop_exception)\n            raise\n\n    async def _resolve_host_with_throttle(\n        self,\n        key: Tuple[str, int],\n        host: str,\n        port: int,\n        futures: Set[asyncio.Future[None]],\n        traces: Optional[Sequence[\"Trace\"]],\n    ) -> List[ResolveResult]:\n        \"\"\"Resolve host and set result for all waiters.\n\n        This method must be run in a task and shielded from cancellation\n        to avoid cancelling the underlying lookup.\n        \"\"\"\n        if traces:\n            for trace in traces:\n                await trace.send_dns_cache_miss(host)\n        try:\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_start(host)\n\n            addrs = await self._resolver.resolve(host, port, family=self._family)\n            if traces:\n                for trace in traces:\n                    await trace.send_dns_resolvehost_end(host)\n\n            self._cached_hosts.add(key, addrs)\n            for fut in futures:\n                set_result(fut, None)\n        except BaseException as e:\n            # any DNS exception is set for the waiters to raise the same exception.\n            # This coro is always run in task that is shielded from cancellation so\n            # we should never be propagating cancellation here.\n            for fut in futures:\n                set_exception(fut, e)\n            raise\n        finally:\n            self._throttle_dns_futures.pop(key)\n\n        return self._cached_hosts.next_addrs(key)\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        \"\"\"Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection.\n        \"\"\"\n        if req.proxy:\n            _, proto = await self._create_proxy_connection(req, traces, timeout)\n        else:\n            _, proto = await self._create_direct_connection(req, traces, timeout)\n\n        return proto\n\n    def _get_ssl_context(self, req: ClientRequest) -> Optional[SSLContext]:\n        \"\"\"Logic to get the correct SSL context\n\n        0. if req.ssl is false, return None\n\n        1. if ssl_context is specified in req, use it\n        2. if _ssl_context is specified in self, use it\n        3. otherwise:\n            1. if verify_ssl is not specified in req, use self.ssl_context\n               (will generate a default context according to self.verify_ssl)\n            2. if verify_ssl is True in req, generate a default SSL context\n            3. if verify_ssl is False in req, generate a SSL context that\n               won't verify\n        \"\"\"\n        if not req.is_ssl():\n            return None\n\n        if ssl is None:  # pragma: no cover\n            raise RuntimeError(\"SSL is not supported.\")\n        sslcontext = req.ssl\n        if isinstance(sslcontext, ssl.SSLContext):\n            return sslcontext\n        if sslcontext is not True:\n            # not verified or fingerprinted\n            return _SSL_CONTEXT_UNVERIFIED\n        sslcontext = self._ssl\n        if isinstance(sslcontext, ssl.SSLContext):\n            return sslcontext\n        if sslcontext is not True:\n            # not verified or fingerprinted\n            return _SSL_CONTEXT_UNVERIFIED\n        return _SSL_CONTEXT_VERIFIED\n\n    def _get_fingerprint(self, req: ClientRequest) -> Optional[\"Fingerprint\"]:\n        ret = req.ssl\n        if isinstance(ret, Fingerprint):\n            return ret\n        ret = self._ssl\n        if isinstance(ret, Fingerprint):\n            return ret\n        return None\n\n    async def _wrap_create_connection(\n        self,\n        *args: Any,\n        addr_infos: List[aiohappyeyeballs.AddrInfoType],\n        req: ClientRequest,\n        timeout: \"ClientTimeout\",\n        client_error: Type[Exception] = ClientConnectorError,\n        **kwargs: Any,\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                sock = await aiohappyeyeballs.start_connection(\n                    addr_infos=addr_infos,\n                    local_addr_infos=self._local_addr_infos,\n                    happy_eyeballs_delay=self._happy_eyeballs_delay,\n                    interleave=self._interleave,\n                    loop=self._loop,\n                )\n                return await self._loop.create_connection(*args, **kwargs, sock=sock)\n        except cert_errors as exc:\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n        except ssl_errors as exc:\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise client_error(req.connection_key, exc) from exc\n\n    def _warn_about_tls_in_tls(\n        self,\n        underlying_transport: asyncio.Transport,\n        req: ClientRequest,\n    ) -> None:\n        \"\"\"Issue a warning if the requested URL has HTTPS scheme.\"\"\"\n        if req.request_info.url.scheme != \"https\":\n            return\n\n        asyncio_supports_tls_in_tls = getattr(\n            underlying_transport,\n            \"_start_tls_compatible\",\n            False,\n        )\n\n        if asyncio_supports_tls_in_tls:\n            return\n\n        warnings.warn(\n            \"An HTTPS request is being sent through an HTTPS proxy. \"\n            \"This support for TLS in TLS is known to be disabled \"\n            \"in the stdlib asyncio. This is why you'll probably see \"\n            \"an error in the log below.\\n\\n\"\n            \"It is possible to enable it via monkeypatching. \"\n            \"For more details, see:\\n\"\n            \"* https://bugs.python.org/issue37179\\n\"\n            \"* https://github.com/python/cpython/pull/28073\\n\\n\"\n            \"You can temporarily patch this as follows:\\n\"\n            \"* https://docs.aiohttp.org/en/stable/client_advanced.html#proxy-support\\n\"\n            \"* https://github.com/aio-libs/aiohttp/discussions/6044\\n\",\n            RuntimeWarning,\n            source=self,\n            # Why `4`? At least 3 of the calls in the stack originate\n            # from the methods in this class.\n            stacklevel=3,\n        )\n\n    async def _start_tls_connection(\n        self,\n        underlying_transport: asyncio.Transport,\n        req: ClientRequest,\n        timeout: \"ClientTimeout\",\n        client_error: Type[Exception] = ClientConnectorError,\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\n        \"\"\"Wrap the raw TCP transport with TLS.\"\"\"\n        tls_proto = self._factory()  # Create a brand new proto for TLS\n        sslcontext = self._get_ssl_context(req)\n        if TYPE_CHECKING:\n            # _start_tls_connection is unreachable in the current code path\n            # if sslcontext is None.\n            assert sslcontext is not None\n\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                try:\n                    tls_transport = await self._loop.start_tls(\n                        underlying_transport,\n                        tls_proto,\n                        sslcontext,\n                        server_hostname=req.server_hostname or req.host,\n                        ssl_handshake_timeout=timeout.total,\n                    )\n                except BaseException:\n                    # We need to close the underlying transport since\n                    # `start_tls()` probably failed before it had a\n                    # chance to do this:\n                    underlying_transport.close()\n                    raise\n                if isinstance(tls_transport, asyncio.Transport):\n                    fingerprint = self._get_fingerprint(req)\n                    if fingerprint:\n                        try:\n                            fingerprint.check(tls_transport)\n                        except ServerFingerprintMismatch:\n                            tls_transport.close()\n                            if not self._cleanup_closed_disabled:\n                                self._cleanup_closed_transports.append(tls_transport)\n                            raise\n        except cert_errors as exc:\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n        except ssl_errors as exc:\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise client_error(req.connection_key, exc) from exc\n        except TypeError as type_err:\n            # Example cause looks like this:\n            # TypeError: transport <asyncio.sslproto._SSLProtocolTransport\n            # object at 0x7f760615e460> is not supported by start_tls()\n\n            raise ClientConnectionError(\n                \"Cannot initialize a TLS-in-TLS connection to host \"\n                f\"{req.host!s}:{req.port:d} through an underlying connection \"\n                f\"to an HTTPS proxy {req.proxy!s} ssl:{req.ssl or 'default'} \"\n                f\"[{type_err!s}]\"\n            ) from type_err\n        else:\n            if tls_transport is None:\n                msg = \"Failed to start TLS (possibly caused by closing transport)\"\n                raise client_error(req.connection_key, OSError(msg))\n            tls_proto.connection_made(\n                tls_transport\n            )  # Kick the state machine of the new TLS protocol\n\n        return tls_transport, tls_proto\n\n    def _convert_hosts_to_addr_infos(\n        self, hosts: List[ResolveResult]\n    ) -> List[aiohappyeyeballs.AddrInfoType]:\n        \"\"\"Converts the list of hosts to a list of addr_infos.\n\n        The list of hosts is the result of a DNS lookup. The list of\n        addr_infos is the result of a call to `socket.getaddrinfo()`.\n        \"\"\"\n        addr_infos: List[aiohappyeyeballs.AddrInfoType] = []\n        for hinfo in hosts:\n            host = hinfo[\"host\"]\n            is_ipv6 = \":\" in host\n            family = socket.AF_INET6 if is_ipv6 else socket.AF_INET\n            if self._family and self._family != family:\n                continue\n            addr = (host, hinfo[\"port\"], 0, 0) if is_ipv6 else (host, hinfo[\"port\"])\n            addr_infos.append(\n                (family, socket.SOCK_STREAM, socket.IPPROTO_TCP, \"\", addr)\n            )\n        return addr_infos\n\n    async def _create_direct_connection(\n        self,\n        req: ClientRequest,\n        traces: List[\"Trace\"],\n        timeout: \"ClientTimeout\",\n        *,\n        client_error: Type[Exception] = ClientConnectorError,\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\n        sslcontext = self._get_ssl_context(req)\n        fingerprint = self._get_fingerprint(req)\n\n        host = req.url.raw_host\n        assert host is not None\n        # Replace multiple trailing dots with a single one.\n        # A trailing dot is only present for fully-qualified domain names.\n        # See https://github.com/aio-libs/aiohttp/pull/7364.\n        if host.endswith(\"..\"):\n            host = host.rstrip(\".\") + \".\"\n        port = req.port\n        assert port is not None\n        try:\n            # Cancelling this lookup should not cancel the underlying lookup\n            #  or else the cancel event will get broadcast to all the waiters\n            #  across all connections.\n            hosts = await self._resolve_host(host, port, traces=traces)\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            # in case of proxy it is not ClientProxyConnectionError\n            # it is problem of resolving proxy ip itself\n            raise ClientConnectorDNSError(req.connection_key, exc) from exc\n\n        last_exc: Optional[Exception] = None\n        addr_infos = self._convert_hosts_to_addr_infos(hosts)\n        while addr_infos:\n            # Strip trailing dots, certificates contain FQDN without dots.\n            # See https://github.com/aio-libs/aiohttp/issues/3636\n            server_hostname = (\n                (req.server_hostname or host).rstrip(\".\") if sslcontext else None\n            )\n\n            try:\n                transp, proto = await self._wrap_create_connection(\n                    self._factory,\n                    timeout=timeout,\n                    ssl=sslcontext,\n                    addr_infos=addr_infos,\n                    server_hostname=server_hostname,\n                    req=req,\n                    client_error=client_error,\n                )\n            except (ClientConnectorError, asyncio.TimeoutError) as exc:\n                last_exc = exc\n                aiohappyeyeballs.pop_addr_infos_interleave(addr_infos, self._interleave)\n                continue\n\n            if req.is_ssl() and fingerprint:\n                try:\n                    fingerprint.check(transp)\n                except ServerFingerprintMismatch as exc:\n                    transp.close()\n                    if not self._cleanup_closed_disabled:\n                        self._cleanup_closed_transports.append(transp)\n                    last_exc = exc\n                    # Remove the bad peer from the list of addr_infos\n                    sock: socket.socket = transp.get_extra_info(\"socket\")\n                    bad_peer = sock.getpeername()\n                    aiohappyeyeballs.remove_addr_infos(addr_infos, bad_peer)\n                    continue\n\n            return transp, proto\n        assert last_exc is not None\n        raise last_exc\n\n    async def _create_proxy_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\n        headers: Dict[str, str] = {}\n        if req.proxy_headers is not None:\n            headers = req.proxy_headers  # type: ignore[assignment]\n        headers[hdrs.HOST] = req.headers[hdrs.HOST]\n\n        url = req.proxy\n        assert url is not None\n        proxy_req = ClientRequest(\n            hdrs.METH_GET,\n            url,\n            headers=headers,\n            auth=req.proxy_auth,\n            loop=self._loop,\n            ssl=req.ssl,\n        )\n\n        # create connection to proxy server\n        transport, proto = await self._create_direct_connection(\n            proxy_req, [], timeout, client_error=ClientProxyConnectionError\n        )\n\n        auth = proxy_req.headers.pop(hdrs.AUTHORIZATION, None)\n        if auth is not None:\n            if not req.is_ssl():\n                req.headers[hdrs.PROXY_AUTHORIZATION] = auth\n            else:\n                proxy_req.headers[hdrs.PROXY_AUTHORIZATION] = auth\n\n        if req.is_ssl():\n            self._warn_about_tls_in_tls(transport, req)\n\n            # For HTTPS requests over HTTP proxy\n            # we must notify proxy to tunnel connection\n            # so we send CONNECT command:\n            #   CONNECT www.python.org:443 HTTP/1.1\n            #   Host: www.python.org\n            #\n            # next we must do TLS handshake and so on\n            # to do this we must wrap raw socket into secure one\n            # asyncio handles this perfectly\n            proxy_req.method = hdrs.METH_CONNECT\n            proxy_req.url = req.url\n            key = req.connection_key._replace(\n                proxy=None, proxy_auth=None, proxy_headers_hash=None\n            )\n            conn = Connection(self, key, proto, self._loop)\n            proxy_resp = await proxy_req.send(conn)\n            try:\n                protocol = conn._protocol\n                assert protocol is not None\n\n                # read_until_eof=True will ensure the connection isn't closed\n                # once the response is received and processed allowing\n                # START_TLS to work on the connection below.\n                protocol.set_response_params(\n                    read_until_eof=True,\n                    timeout_ceil_threshold=self._timeout_ceil_threshold,\n                )\n                resp = await proxy_resp.start(conn)\n            except BaseException:\n                proxy_resp.close()\n                conn.close()\n                raise\n            else:\n                conn._protocol = None\n                try:\n                    if resp.status != 200:\n                        message = resp.reason\n                        if message is None:\n                            message = HTTPStatus(resp.status).phrase\n                        raise ClientHttpProxyError(\n                            proxy_resp.request_info,\n                            resp.history,\n                            status=resp.status,\n                            message=message,\n                            headers=resp.headers,\n                        )\n                except BaseException:\n                    # It shouldn't be closed in `finally` because it's fed to\n                    # `loop.start_tls()` and the docs say not to touch it after\n                    # passing there.\n                    transport.close()\n                    raise\n\n                return await self._start_tls_connection(\n                    # Access the old transport for the last time before it's\n                    # closed and forgotten forever:\n                    transport,\n                    req=req,\n                    timeout=timeout,\n                )\n            finally:\n                proxy_resp.close()\n\n        return transport, proto"
    },
    {
      "chunk_id": 437,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class UnixConnector(BaseConnector):\n    \"\"\"Unix socket connector.\n\n    path - Unix socket path.\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    loop - Optional event loop.\n    \"\"\"\n\n    allowed_protocol_schema_set = HIGH_LEVEL_SCHEMA_SET | frozenset({\"unix\"})\n\n    def __init__(\n        self,\n        path: str,\n        force_close: bool = False,\n        keepalive_timeout: Union[_SENTINEL, float, None] = sentinel,\n        limit: int = 100,\n        limit_per_host: int = 0,\n    ) -> None:\n        super().__init__(\n            force_close=force_close,\n            keepalive_timeout=keepalive_timeout,\n            limit=limit,\n            limit_per_host=limit_per_host,\n        )\n        self._path = path\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path to unix socket.\"\"\"\n        return self._path\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                _, proto = await self._loop.create_unix_connection(\n                    self._factory, self._path\n                )\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise UnixClientConnectorError(self.path, req.connection_key, exc) from exc\n\n        return proto"
    },
    {
      "chunk_id": 438,
      "source": "__internal__/data_repo/aiohttp/aiohttp/connector.py",
      "content": "class NamedPipeConnector(BaseConnector):\n    \"\"\"Named pipe connector.\n\n    Only supported by the proactor event loop.\n    See also: https://docs.python.org/3/library/asyncio-eventloop.html\n\n    path - Windows named pipe path.\n    keepalive_timeout - (optional) Keep-alive timeout.\n    force_close - Set to True to force close and do reconnect\n        after each request (and between redirects).\n    limit - The total number of simultaneous connections.\n    limit_per_host - Number of simultaneous connections to one host.\n    loop - Optional event loop.\n    \"\"\"\n\n    allowed_protocol_schema_set = HIGH_LEVEL_SCHEMA_SET | frozenset({\"npipe\"})\n\n    def __init__(\n        self,\n        path: str,\n        force_close: bool = False,\n        keepalive_timeout: Union[_SENTINEL, float, None] = sentinel,\n        limit: int = 100,\n        limit_per_host: int = 0,\n    ) -> None:\n        super().__init__(\n            force_close=force_close,\n            keepalive_timeout=keepalive_timeout,\n            limit=limit,\n            limit_per_host=limit_per_host,\n        )\n        if not isinstance(\n            self._loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\n        ):\n            raise RuntimeError(\n                \"Named Pipes only available in proactor loop under windows\"\n            )\n        self._path = path\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path to the named pipe.\"\"\"\n        return self._path\n\n    async def _create_connection(\n        self, req: ClientRequest, traces: List[\"Trace\"], timeout: \"ClientTimeout\"\n    ) -> ResponseHandler:\n        try:\n            async with ceil_timeout(\n                timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n            ):\n                _, proto = await self._loop.create_pipe_connection(  # type: ignore[attr-defined]\n                    self._factory, self._path\n                )\n                # the drain is required so that the connection_made is called\n                # and transport is set otherwise it is not set before the\n                # `assert conn.transport is not None`\n                # in client.py's _request method\n                await asyncio.sleep(0)\n                # other option is to manually set transport like\n                # `proto.transport = trans`\n        except OSError as exc:\n            if exc.errno is None and isinstance(exc, asyncio.TimeoutError):\n                raise\n            raise ClientConnectorError(req.connection_key, exc) from exc\n\n        return cast(ResponseHandler, proto)\n```"
    },
    {
      "chunk_id": 439,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "import asyncio\nimport contextlib\nimport inspect\nimport warnings\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterator,\n    Optional,\n    Protocol,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\n\nimport pytest\n\nfrom .test_utils import (\n    BaseTestServer,\n    RawTestServer,\n    TestClient,\n    TestServer,\n    loop_context,\n    setup_test_loop,\n    teardown_test_loop,\n    unused_port as _unused_port,\n)\nfrom .web import Application, BaseRequest, Request\nfrom .web_protocol import _RequestHandler\n\ntry:\n    import uvloop\nexcept ImportError:  # pragma: no cover\n    uvloop = None  # type: ignore[assignment]\n\n_Request = TypeVar(\"_Request\", bound=BaseRequest)"
    },
    {
      "chunk_id": 440,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "class AiohttpClient(Protocol):\n    @overload\n    async def __call__(\n        self,\n        __param: Application,\n        *,\n        server_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> TestClient[Request, Application]: ...\n    @overload\n    async def __call__(\n        self,\n        __param: BaseTestServer[_Request],\n        *,\n        server_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> TestClient[_Request, None]: ..."
    },
    {
      "chunk_id": 441,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "class AiohttpServer(Protocol):\n    def __call__(\n        self, app: Application, *, port: Optional[int] = None, **kwargs: Any\n    ) -> Awaitable[TestServer]: ..."
    },
    {
      "chunk_id": 442,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "class AiohttpRawServer(Protocol):\n    def __call__(\n        self,\n        handler: _RequestHandler[BaseRequest],\n        *,\n        port: Optional[int] = None,\n        **kwargs: Any,\n    ) -> Awaitable[RawTestServer]: ..."
    },
    {
      "chunk_id": 443,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "def pytest_addoption(parser):  # type: ignore[no-untyped-def]\n    parser.addoption(\n        \"--aiohttp-fast\",\n        action=\"store_true\",\n        default=False,\n        help=\"run tests faster by disabling extra checks\",\n    )\n    parser.addoption(\n        \"--aiohttp-loop\",\n        action=\"store\",\n        default=\"pyloop\",\n        help=\"run tests with specific loop: pyloop, uvloop or all\",\n    )\n    parser.addoption(\n        \"--aiohttp-enable-loop-debug\",\n        action=\"store_true\",\n        default=False,\n        help=\"enable event loop debug mode\",\n    )"
    },
    {
      "chunk_id": 444,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "def pytest_fixture_setup(fixturedef):  # type: ignore[no-untyped-def]\n    \"\"\"Set up pytest fixture.\n\n    Allow fixtures to be coroutines. Run coroutine fixtures in an event loop.\n    \"\"\"\n    func = fixturedef.func\n\n    if inspect.isasyncgenfunction(func):\n        # async generator fixture\n        is_async_gen = True\n    elif asyncio.iscoroutinefunction(func):\n        # regular async fixture\n        is_async_gen = False\n    else:\n        # not an async fixture, nothing to do\n        return\n\n    strip_request = False\n    if \"request\" not in fixturedef.argnames:\n        fixturedef.argnames += (\"request\",)\n        strip_request = True\n\n    def wrapper(*args, **kwargs):  # type: ignore[no-untyped-def]\n        request = kwargs[\"request\"]\n        if strip_request:\n            del kwargs[\"request\"]\n\n        # if neither the fixture nor the test use the 'loop' fixture,\n        # 'getfixturevalue' will fail because the test is not parameterized\n        # (this can be removed someday if 'loop' is no longer parameterized)\n        if \"loop\" not in request.fixturenames:\n            raise Exception(\n                \"Asynchronous fixtures must depend on the 'loop' fixture or \"\n                \"be used in tests depending from it.\"\n            )\n\n        _loop = request.getfixturevalue(\"loop\")\n\n        if is_async_gen:\n            # for async generators, we need to advance the generator once,\n            # then advance it again in a finalizer\n            gen = func(*args, **kwargs)\n\n            def finalizer():  # type: ignore[no-untyped-def]\n                try:\n                    return _loop.run_until_complete(gen.__anext__())\n                except StopAsyncIteration:\n                    pass\n\n            request.addfinalizer(finalizer)\n            return _loop.run_until_complete(gen.__anext__())\n        else:\n            return _loop.run_until_complete(func(*args, **kwargs))\n\n    fixturedef.func = wrapper"
    },
    {
      "chunk_id": 445,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef fast(request):  # type: ignore[no-untyped-def]\n    \"\"\"--fast config option\"\"\"\n    return request.config.getoption(\"--aiohttp-fast\")"
    },
    {
      "chunk_id": 446,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef loop_debug(request):  # type: ignore[no-untyped-def]\n    \"\"\"--enable-loop-debug config option\"\"\"\n    return request.config.getoption(\"--aiohttp-enable-loop-debug\")"
    },
    {
      "chunk_id": 447,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@contextlib.contextmanager\ndef _runtime_warning_context():  # type: ignore[no-untyped-def]\n    \"\"\"Context manager which checks for RuntimeWarnings.\n\n    This exists specifically to\n    avoid \"coroutine 'X' was never awaited\" warnings being missed.\n\n    If RuntimeWarnings occur in the context a RuntimeError is raised.\n    \"\"\"\n    with warnings.catch_warnings(record=True) as _warnings:\n        yield\n        rw = [\n            \"{w.filename}:{w.lineno}:{w.message}\".format(w=w)\n            for w in _warnings\n            if w.category == RuntimeWarning\n        ]\n        if rw:\n            raise RuntimeError(\n                \"{} Runtime Warning{},\\n{}\".format(\n                    len(rw), \"\" if len(rw) == 1 else \"s\", \"\\n\".join(rw)\n                )\n            )\n\n    # Propagate warnings to pytest\n    for msg in _warnings:\n        warnings.showwarning(\n            msg.message, msg.category, msg.filename, msg.lineno, msg.file, msg.line\n        )"
    },
    {
      "chunk_id": 448,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@contextlib.contextmanager\ndef _passthrough_loop_context(loop, fast=False):  # type: ignore[no-untyped-def]\n    \"\"\"Passthrough loop context.\n\n    Sets up and tears down a loop unless one is passed in via the loop\n    argument when it's passed straight through.\n    \"\"\"\n    if loop:\n        # loop already exists, pass it straight through\n        yield loop\n    else:\n        # this shadows loop_context's standard behavior\n        loop = setup_test_loop()\n        yield loop\n        teardown_test_loop(loop, fast=fast)"
    },
    {
      "chunk_id": 449,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "def pytest_pycollect_makeitem(collector, name, obj):  # type: ignore[no-untyped-def]\n    \"\"\"Fix pytest collecting for coroutines.\"\"\"\n    if collector.funcnamefilter(name) and asyncio.iscoroutinefunction(obj):\n        return list(collector._genfunctions(name, obj))"
    },
    {
      "chunk_id": 450,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "def pytest_pyfunc_call(pyfuncitem):  # type: ignore[no-untyped-def]\n    \"\"\"Run coroutines in an event loop instead of a normal function call.\"\"\"\n    fast = pyfuncitem.config.getoption(\"--aiohttp-fast\")\n    if asyncio.iscoroutinefunction(pyfuncitem.function):\n        existing_loop = pyfuncitem.funcargs.get(\n            \"proactor_loop\"\n        ) or pyfuncitem.funcargs.get(\"loop\", None)\n        with _runtime_warning_context():\n            with _passthrough_loop_context(existing_loop, fast=fast) as _loop:\n                testargs = {\n                    arg: pyfuncitem.funcargs[arg]\n                    for arg in pyfuncitem._fixtureinfo.argnames\n                }\n                _loop.run_until_complete(pyfuncitem.obj(**testargs))\n\n        return True"
    },
    {
      "chunk_id": 451,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "def pytest_generate_tests(metafunc):  # type: ignore[no-untyped-def]\n    if \"loop_factory\" not in metafunc.fixturenames:\n        return\n\n    loops = metafunc.config.option.aiohttp_loop\n    avail_factories: Dict[str, Type[asyncio.AbstractEventLoopPolicy]]\n    avail_factories = {\"pyloop\": asyncio.DefaultEventLoopPolicy}\n\n    if uvloop is not None:  # pragma: no cover\n        avail_factories[\"uvloop\"] = uvloop.EventLoopPolicy\n\n    if loops == \"all\":\n        loops = \"pyloop,uvloop?\"\n\n    factories = {}  # type: ignore[var-annotated]\n    for name in loops.split(\",\"):\n        required = not name.endswith(\"?\")\n        name = name.strip(\" ?\")\n        if name not in avail_factories:  # pragma: no cover\n            if required:\n                raise ValueError(\n                    \"Unknown loop '%s', available loops: %s\"\n                    % (name, list(factories.keys()))\n                )\n            else:\n                continue\n        factories[name] = avail_factories[name]\n    metafunc.parametrize(\n        \"loop_factory\", list(factories.values()), ids=list(factories.keys())\n    )"
    },
    {
      "chunk_id": 452,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef loop(loop_factory, fast, loop_debug):  # type: ignore[no-untyped-def]\n    \"\"\"Return an instance of the event loop.\"\"\"\n    policy = loop_factory()\n    asyncio.set_event_loop_policy(policy)\n    with loop_context(fast=fast) as _loop:\n        if loop_debug:\n            _loop.set_debug(True)  # pragma: no cover\n        asyncio.set_event_loop(_loop)\n        yield _loop"
    },
    {
      "chunk_id": 453,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef proactor_loop():  # type: ignore[no-untyped-def]\n    policy = asyncio.WindowsProactorEventLoopPolicy()  # type: ignore[attr-defined]\n    asyncio.set_event_loop_policy(policy)\n\n    with loop_context(policy.new_event_loop) as _loop:\n        asyncio.set_event_loop(_loop)\n        yield _loop"
    },
    {
      "chunk_id": 454,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef aiohttp_unused_port() -> Callable[[], int]:\n    \"\"\"Return a port that is unused on the current host.\"\"\"\n    return _unused_port"
    },
    {
      "chunk_id": 455,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef aiohttp_server(loop: asyncio.AbstractEventLoop) -> Iterator[AiohttpServer]:\n    \"\"\"Factory to create a TestServer instance, given an app.\n\n    aiohttp_server(app, **kwargs)\n    \"\"\"\n    servers = []\n\n    async def go(\n        app: Application,\n        *,\n        host: str = \"127.0.0.1\",\n        port: Optional[int] = None,\n        **kwargs: Any,\n    ) -> TestServer:\n        server = TestServer(app, host=host, port=port)\n        await server.start_server(**kwargs)\n        servers.append(server)\n        return server\n\n    yield go\n\n    async def finalize() -> None:\n        while servers:\n            await servers.pop().close()\n\n    loop.run_until_complete(finalize())"
    },
    {
      "chunk_id": 456,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef aiohttp_raw_server(loop: asyncio.AbstractEventLoop) -> Iterator[AiohttpRawServer]:\n    \"\"\"Factory to create a RawTestServer instance, given a web handler.\n\n    aiohttp_raw_server(handler, **kwargs)\n    \"\"\"\n    servers = []\n\n    async def go(\n        handler: _RequestHandler[BaseRequest],\n        *,\n        port: Optional[int] = None,\n        **kwargs: Any,\n    ) -> RawTestServer:\n        server = RawTestServer(handler, port=port)\n        await server.start_server(**kwargs)\n        servers.append(server)\n        return server\n\n    yield go\n\n    async def finalize() -> None:\n        while servers:\n            await servers.pop().close()\n\n    loop.run_until_complete(finalize())"
    },
    {
      "chunk_id": 457,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef aiohttp_client_cls() -> Type[TestClient[Any, Any]]:\n    \"\"\"\n    Client class to use in ``aiohttp_client`` factory.\n\n    Use it for passing custom ``TestClient`` implementations.\n\n    Example::\n\n       class MyClient(TestClient):\n           async def login(self, *, user, pw):\n               payload = {\"username\": user, \"password\": pw}\n               return await self.post(\"/login\", json=payload)\n\n       @pytest.fixture\n       def aiohttp_client_cls():\n           return MyClient\n\n       def test_login(aiohttp_client):\n           app = web.Application()\n           client = await aiohttp_client(app)\n           await client.login(user=\"admin\", pw=\"s3cr3t\")\n\n    \"\"\"\n    return TestClient"
    },
    {
      "chunk_id": 458,
      "source": "__internal__/data_repo/aiohttp/aiohttp/pytest_plugin.py",
      "content": "@pytest.fixture\ndef aiohttp_client(\n    loop: asyncio.AbstractEventLoop, aiohttp_client_cls: Type[TestClient[Any, Any]]\n) -> Iterator[AiohttpClient]:\n    \"\"\"Factory to create a TestClient instance.\n\n    aiohttp_client(app, **kwargs)\n    aiohttp_client(server, **kwargs)\n    aiohttp_client(raw_server, **kwargs)\n    \"\"\"\n    clients = []\n\n    @overload\n    async def go(\n        __param: Application,\n        *,\n        server_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> TestClient[Request, Application]: ...\n    @overload\n    async def go(\n        __param: BaseTestServer[_Request],\n        *,\n        server_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> TestClient[_Request, None]: ...\n    async def go(\n        __param: Union[Application, BaseTestServer[Any]],\n        *,\n        server_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> TestClient[Any, Any]:\n        if isinstance(__param, Application):\n            server_kwargs = server_kwargs or {}\n            server = TestServer(__param, **server_kwargs)\n            client = aiohttp_client_cls(server, **kwargs)\n        elif isinstance(__param, BaseTestServer):\n            client = aiohttp_client_cls(__param, **kwargs)\n        else:\n            raise ValueError(\"Unknown argument type: %r\" % type(__param))\n\n        await client.start_server()\n        clients.append(client)\n        return client\n\n    yield go\n\n    async def finalize() -> None:\n        while clients:\n            await clients.pop().close()\n\n    loop.run_until_complete(finalize())"
    },
    {
      "chunk_id": 459,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tcp_helpers.py",
      "content": "```python\nimport asyncio\nimport socket\nfrom contextlib import suppress\nfrom typing import Optional  # noqa\n```"
    },
    {
      "chunk_id": 460,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tcp_helpers.py",
      "content": "```python\ndef tcp_keepalive(transport: asyncio.Transport) -> None:\n    sock = transport.get_extra_info(\"socket\")\n    if sock is not None:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n```"
    },
    {
      "chunk_id": 461,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tcp_helpers.py",
      "content": "```python\ndef tcp_keepalive(transport: asyncio.Transport) -> None:  # pragma: no cover\n    pass\n```"
    },
    {
      "chunk_id": 462,
      "source": "__internal__/data_repo/aiohttp/aiohttp/tcp_helpers.py",
      "content": "```python\ndef tcp_nodelay(transport: asyncio.Transport, value: bool) -> None:\n    sock = transport.get_extra_info(\"socket\")\n\n    if sock is None:\n        return\n\n    if sock.family not in (socket.AF_INET, socket.AF_INET6):\n        return\n\n    value = bool(value)\n\n    # socket may be closed already, on windows OSError get raised\n    with suppress(OSError):\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, value)\n```"
    },
    {
      "chunk_id": 463,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_middlewares.py",
      "content": "import re\nimport warnings\nfrom typing import TYPE_CHECKING, Tuple, Type, TypeVar\n\nfrom .typedefs import Handler, Middleware\nfrom .web_exceptions import HTTPMove, HTTPPermanentRedirect\nfrom .web_request import Request\nfrom .web_response import StreamResponse\nfrom .web_urldispatcher import SystemRoute\n\n__all__ = (\n    \"middleware\",\n    \"normalize_path_middleware\",\n)"
    },
    {
      "chunk_id": 464,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_middlewares.py",
      "content": "async def _check_request_resolves(request: Request, path: str) -> Tuple[bool, Request]:\n    alt_request = request.clone(rel_url=path)\n\n    match_info = await request.app.router.resolve(alt_request)\n    alt_request._match_info = match_info\n\n    if match_info.http_exception is None:\n        return True, alt_request\n\n    return False, request"
    },
    {
      "chunk_id": 465,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_middlewares.py",
      "content": "def middleware(f: _Func) -> _Func:\n    warnings.warn(\n        \"Middleware decorator is deprecated since 4.0 \"\n        \"and its behaviour is default, \"\n        \"you can simply remove this decorator.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return f"
    },
    {
      "chunk_id": 466,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_middlewares.py",
      "content": "def normalize_path_middleware(\n    *,\n    append_slash: bool = True,\n    remove_slash: bool = False,\n    merge_slashes: bool = True,\n    redirect_class: Type[HTTPMove] = HTTPPermanentRedirect,\n) -> Middleware:\n    \"\"\"Factory for producing a middleware that normalizes the path of a request.\n\n    Normalizing means:\n        - Add or remove a trailing slash to the path.\n        - Double slashes are replaced by one.\n\n    The middleware returns as soon as it finds a path that resolves\n    correctly. The order if both merge and append/remove are enabled is\n        1) merge slashes\n        2) append/remove slash\n        3) both merge slashes and append/remove slash.\n    If the path resolves with at least one of those conditions, it will\n    redirect to the new path.\n\n    Only one of `append_slash` and `remove_slash` can be enabled. If both\n    are `True` the factory will raise an assertion error\n\n    If `append_slash` is `True` the middleware will append a slash when\n    needed. If a resource is defined with trailing slash and the request\n    comes without it, it will append it automatically.\n\n    If `remove_slash` is `True`, `append_slash` must be `False`. When enabled\n    the middleware will remove trailing slashes and redirect if the resource\n    is defined\n\n    If merge_slashes is True, merge multiple consecutive slashes in the\n    path into one.\n    \"\"\"\n    correct_configuration = not (append_slash and remove_slash)\n    assert correct_configuration, \"Cannot both remove and append slash\"\n\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        if isinstance(request.match_info.route, SystemRoute):\n            paths_to_check = []\n            if \"?\" in request.raw_path:\n                path, query = request.raw_path.split(\"?\", 1)\n                query = \"?\" + query\n            else:\n                query = \"\"\n                path = request.raw_path\n\n            if merge_slashes:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path))\n            if append_slash and not request.path.endswith(\"/\"):\n                paths_to_check.append(path + \"/\")\n            if remove_slash and request.path.endswith(\"/\"):\n                paths_to_check.append(path[:-1])\n            if merge_slashes and append_slash:\n                paths_to_check.append(re.sub(\"//+\", \"/\", path + \"/\"))\n            if merge_slashes and remove_slash and path.endswith(\"/\"):\n                merged_slashes = re.sub(\"//+\", \"/\", path)\n                paths_to_check.append(merged_slashes[:-1])\n\n            for path in paths_to_check:\n                path = re.sub(\"^//+\", \"/\", path)  # SECURITY: GHSA-v6wp-4m6f-gcjg\n                resolves, request = await _check_request_resolves(request, path)\n                if resolves:\n                    raise redirect_class(request.raw_path + query)\n\n        return await handler(request)\n\n    return impl"
    },
    {
      "chunk_id": 467,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_middlewares.py",
      "content": "def _fix_request_current_app(app: \"Application\") -> Middleware:\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        match_info = request.match_info\n        prev = match_info.current_app\n        match_info.current_app = app\n        try:\n            return await handler(request)\n        finally:\n            match_info.current_app = prev\n\n    return impl"
    },
    {
      "chunk_id": 468,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "import base64\nimport binascii\nimport json\nimport re\nimport sys\nimport uuid\nimport warnings\nimport zlib\nfrom collections import deque\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Deque,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\nfrom urllib.parse import parse_qsl, unquote, urlencode\n\nfrom multidict import CIMultiDict, CIMultiDictProxy\n\nfrom .compression_utils import ZLibCompressor, ZLibDecompressor\nfrom .hdrs import (\n    CONTENT_DISPOSITION,\n    CONTENT_ENCODING,\n    CONTENT_LENGTH,\n    CONTENT_TRANSFER_ENCODING,\n    CONTENT_TYPE,\n)\nfrom .helpers import CHAR, TOKEN, parse_mimetype, reify\nfrom .http import HeadersParser\nfrom .payload import (\n    JsonPayload,\n    LookupError,\n    Order,\n    Payload,\n    StringPayload,\n    get_payload,\n    payload_type,\n)\nfrom .streams import StreamReader\n\nif sys.version_info >= (3, 11):\n    from typing import Self\nelse:\n    from typing import TypeVar\n\n    Self = TypeVar(\"Self\", bound=\"BodyPartReader\")\n\n__all__ = (\n    \"MultipartReader\",\n    \"MultipartWriter\",\n    \"BodyPartReader\",\n    \"BadContentDispositionHeader\",\n    \"BadContentDispositionParam\",\n    \"parse_content_disposition\",\n    \"content_disposition_filename\",\n)"
    },
    {
      "chunk_id": 469,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class BadContentDispositionHeader(RuntimeWarning):\n    pass"
    },
    {
      "chunk_id": 470,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class BadContentDispositionParam(RuntimeWarning):\n    pass"
    },
    {
      "chunk_id": 471,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "def parse_content_disposition(\n    header: Optional[str],\n) -> Tuple[Optional[str], Dict[str, str]]:\n    def is_token(string: str) -> bool:\n        return bool(string) and TOKEN >= set(string)\n\n    def is_quoted(string: str) -> bool:\n        return string[0] == string[-1] == '\"'\n\n    def is_rfc5987(string: str) -> bool:\n        return is_token(string) and string.count(\"'\") == 2\n\n    def is_extended_param(string: str) -> bool:\n        return string.endswith(\"*\")\n\n    def is_continuous_param(string: str) -> bool:\n        pos = string.find(\"*\") + 1\n        if not pos:\n            return False\n        substring = string[pos:-1] if string.endswith(\"*\") else string[pos:]\n        return substring.isdigit()\n\n    def unescape(text: str, *, chars: str = \"\".join(map(re.escape, CHAR))) -> str:\n        return re.sub(f\"\\\\\\\\([{chars}])\", \"\\\\1\", text)\n\n    if not header:\n        return None, {}\n\n    disptype, *parts = header.split(\";\")\n    if not is_token(disptype):\n        warnings.warn(BadContentDispositionHeader(header))\n        return None, {}\n\n    params: Dict[str, str] = {}\n    while parts:\n        item = parts.pop(0)\n\n        if \"=\" not in item:\n            warnings.warn(BadContentDispositionHeader(header))\n            return None, {}\n\n        key, value = item.split(\"=\", 1)\n        key = key.lower().strip()\n        value = value.lstrip()\n\n        if key in params:\n            warnings.warn(BadContentDispositionHeader(header))\n            return None, {}\n\n        if not is_token(key):\n            warnings.warn(BadContentDispositionParam(item))\n            continue\n\n        elif is_continuous_param(key):\n            if is_quoted(value):\n                value = unescape(value[1:-1])\n            elif not is_token(value):\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n        elif is_extended_param(key):\n            if is_rfc5987(value):\n                encoding, _, value = value.split(\"'\", 2)\n                encoding = encoding or \"utf-8\"\n            else:\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n            try:\n                value = unquote(value, encoding, \"strict\")\n            except UnicodeDecodeError:  # pragma: nocover\n                warnings.warn(BadContentDispositionParam(item))\n                continue\n\n        else:\n            failed = True\n            if is_quoted(value):\n                failed = False\n                value = unescape(value[1:-1].lstrip(\"\\\\/\"))\n            elif is_token(value):\n                failed = False\n            elif parts:\n                _value = f\"{value};{parts[0]}\"\n                if is_quoted(_value):\n                    parts.pop(0)\n                    value = unescape(_value[1:-1].lstrip(\"\\\\/\"))\n                    failed = False\n\n            if failed:\n                warnings.warn(BadContentDispositionHeader(header))\n                return None, {}\n\n        params[key] = value\n\n    return disptype.lower(), params"
    },
    {
      "chunk_id": 472,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "def content_disposition_filename(\n    params: Mapping[str, str], name: str = \"filename\"\n) -> Optional[str]:\n    name_suf = \"%s*\" % name\n    if not params:\n        return None\n    elif name_suf in params:\n        return params[name_suf]\n    elif name in params:\n        return params[name]\n    else:\n        parts = []\n        fnparams = sorted(\n            (key, value) for key, value in params.items() if key.startswith(name_suf)\n        )\n        for num, (key, value) in enumerate(fnparams):\n            _, tail = key.split(\"*\", 1)\n            if tail.endswith(\"*\"):\n                tail = tail[:-1]\n            if tail == str(num):\n                parts.append(value)\n            else:\n                break\n        if not parts:\n            return None\n        value = \"\".join(parts)\n        if \"'\" in value:\n            encoding, _, value = value.split(\"'\", 2)\n            encoding = encoding or \"utf-8\"\n            return unquote(value, encoding, \"strict\")\n        return value"
    },
    {
      "chunk_id": 473,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class MultipartResponseWrapper:\n    \"\"\"Wrapper around the MultipartReader.\n\n    It takes care about\n    underlying connection and close it when it needs in.\n    \"\"\"\n\n    def __init__(\n        self,\n        resp: \"ClientResponse\",\n        stream: \"MultipartReader\",\n    ) -> None:\n        self.resp = resp\n        self.stream = stream\n\n    def __aiter__(self) -> \"MultipartResponseWrapper\":\n        return self\n\n    async def __anext__(\n        self,\n    ) -> Union[\"MultipartReader\", \"BodyPartReader\"]:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True when all response data had been read.\"\"\"\n        return self.resp.content.at_eof()\n\n    async def next(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", \"BodyPartReader\"]]:\n        \"\"\"Emits next multipart reader object.\"\"\"\n        item = await self.stream.next()\n        if self.stream.at_eof():\n            await self.release()\n        return item\n\n    async def release(self) -> None:\n        \"\"\"Release the connection gracefully.\n\n        All remaining content is read to the void.\n        \"\"\"\n        self.resp.release()"
    },
    {
      "chunk_id": 474,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class BodyPartReader:\n    \"\"\"Multipart reader for single body part.\"\"\"\n\n    chunk_size = 8192\n\n    def __init__(\n        self,\n        boundary: bytes,\n        headers: \"CIMultiDictProxy[str]\",\n        content: StreamReader,\n        *,\n        subtype: str = \"mixed\",\n        default_charset: Optional[str] = None,\n    ) -> None:\n        self.headers = headers\n        self._boundary = boundary\n        self._boundary_len = len(boundary) + 2  # Boundary + \\r\\n\n        self._content = content\n        self._default_charset = default_charset\n        self._at_eof = False\n        self._is_form_data = subtype == \"form-data\"\n        length = None if self._is_form_data else self.headers.get(CONTENT_LENGTH, None)\n        self._length = int(length) if length is not None else None\n        self._read_bytes = 0\n        self._unread: Deque[bytes] = deque()\n        self._prev_chunk: Optional[bytes] = None\n        self._content_eof = 0\n        self._cache: Dict[str, Any] = {}\n\n    def __aiter__(self) -> Self:\n        return self\n\n    async def __anext__(self) -> bytes:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    async def next(self) -> Optional[bytes]:\n        item = await self.read()\n        if not item:\n            return None\n        return item\n\n    async def read(self, *, decode: bool = False) -> bytes:\n        \"\"\"Reads body part data.\n\n        decode: Decodes data following by encoding\n                method from Content-Encoding header. If it missed\n                data remains untouched\n        \"\"\"\n        if self._at_eof:\n            return b\"\"\n        data = bytearray()\n        while not self._at_eof:\n            data.extend(await self.read_chunk(self.chunk_size))\n        if decode:  # type: ignore[unreachable]\n            return self.decode(data)\n        return data\n\n    async def read_chunk(self, size: int = chunk_size) -> bytes:\n        \"\"\"Reads body part content chunk of the specified size.\n\n        size: chunk size\n        \"\"\"\n        if self._at_eof:\n            return b\"\"\n        if self._length:\n            chunk = await self._read_chunk_from_length(size)\n        else:\n            chunk = await self._read_chunk_from_stream(size)\n\n        encoding = self.headers.get(CONTENT_TRANSFER_ENCODING)\n        if encoding and encoding.lower() == \"base64\":\n            stripped_chunk = b\"\".join(chunk.split())\n            remainder = len(stripped_chunk) % 4\n\n            while remainder != 0 and not self.at_eof():\n                over_chunk_size = 4 - remainder\n                over_chunk = b\"\"\n\n                if self._prev_chunk:\n                    over_chunk = self._prev_chunk[:over_chunk_size]\n                    self._prev_chunk = self._prev_chunk[len(over_chunk) :]\n\n                if len(over_chunk) != over_chunk_size:\n                    over_chunk += await self._content.read(4 - len(over_chunk))\n\n                if not over_chunk:\n                    self._at_eof = True\n\n                stripped_chunk += b\"\".join(over_chunk.split())\n                chunk += over_chunk\n                remainder = len(stripped_chunk) % 4\n\n        self._read_bytes += len(chunk)\n        if self._read_bytes == self._length:\n            self._at_eof = True\n        if self._at_eof:\n            clrf = await self._content.readline()\n            assert (\n                b\"\\r\\n\" == clrf\n            ), \"reader did not read all the data or it is malformed\"\n        return chunk\n\n    async def _read_chunk_from_length(self, size: int) -> bytes:\n        assert self._length is not None, \"Content-Length required for chunked read\"\n        chunk_size = min(size, self._length - self._read_bytes)\n        chunk = await self._content.read(chunk_size)\n        if self._content.at_eof():\n            self._at_eof = True\n        return chunk\n\n    async def _read_chunk_from_stream(self, size: int) -> bytes:\n        assert (\n            size >= self._boundary_len\n        ), \"Chunk size must be greater or equal than boundary length + 2\"\n        first_chunk = self._prev_chunk is None\n        if first_chunk:\n            self._prev_chunk = await self._content.read(size)\n\n        chunk = b\"\"\n        while len(chunk) < self._boundary_len:\n            chunk += await self._content.read(size)\n            self._content_eof += int(self._content.at_eof())\n            assert self._content_eof < 3, \"Reading after EOF\"\n            if self._content_eof:\n                break\n        if len(chunk) > size:\n            self._content.unread_data(chunk[size:])\n            chunk = chunk[:size]\n\n        assert self._prev_chunk is not None\n        window = self._prev_chunk + chunk\n        sub = b\"\\r\\n\" + self._boundary\n        if first_chunk:\n            idx = window.find(sub)\n        else:\n            idx = window.find(sub, max(0, len(self._prev_chunk) - len(sub)))\n        if idx >= 0:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n                self._content.unread_data(window[idx:])\n            if size > idx:\n                self._prev_chunk = self._prev_chunk[:idx]\n            chunk = window[len(self._prev_chunk) : idx]\n            if not chunk:\n                self._at_eof = True\n        result = self._prev_chunk\n        self._prev_chunk = chunk\n        return result\n\n    async def readline(self) -> bytes:\n        \"\"\"Reads body part by line by line.\"\"\"\n        if self._at_eof:\n            return b\"\"\n\n        if self._unread:\n            line = self._unread.popleft()\n        else:\n            line = await self._content.readline()\n\n        if line.startswith(self._boundary):\n            sline = line.rstrip(b\"\\r\\n\")\n            boundary = self._boundary\n            last_boundary = self._boundary + b\"--\"\n            if sline == boundary or sline == last_boundary:\n                self._at_eof = True\n                self._unread.append(line)\n                return b\"\"\n        else:\n            next_line = await self._content.readline()\n            if next_line.startswith(self._boundary):\n                line = line[:-2]\n            self._unread.append(next_line)\n\n        return line\n\n    async def release(self) -> None:\n        \"\"\"Like read(), but reads all the data to the void.\"\"\"\n        if self._at_eof:\n            return\n        while not self._at_eof:\n            await self.read_chunk(self.chunk_size)\n\n    async def text(self, *, encoding: Optional[str] = None) -> str:\n        \"\"\"Like read(), but assumes that body part contains text data.\"\"\"\n        data = await self.read(decode=True)\n        encoding = encoding or self.get_charset(default=\"utf-8\")\n        return data.decode(encoding)\n\n    async def json(self, *, encoding: Optional[str] = None) -> Optional[Dict[str, Any]]:\n        \"\"\"Like read(), but assumes that body parts contains JSON data.\"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return None\n        encoding = encoding or self.get_charset(default=\"utf-8\")\n        return cast(Dict[str, Any], json.loads(data.decode(encoding)))\n\n    async def form(self, *, encoding: Optional[str] = None) -> List[Tuple[str, str]]:\n        \"\"\"Like read(), but assumes that body parts contain form urlencoded data.\"\"\"\n        data = await self.read(decode=True)\n        if not data:\n            return []\n        if encoding is not None:\n            real_encoding = encoding\n        else:\n            real_encoding = self.get_charset(default=\"utf-8\")\n        try:\n            decoded_data = data.rstrip().decode(real_encoding)\n        except UnicodeDecodeError:\n            raise ValueError(\"data cannot be decoded with %s encoding\" % real_encoding)\n\n        return parse_qsl(\n            decoded_data,\n            keep_blank_values=True,\n            encoding=real_encoding,\n        )\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True if the boundary was reached or False otherwise.\"\"\"\n        return self._at_eof\n\n    def decode(self, data: bytes) -> bytes:\n        \"\"\"Decodes data.\n\n        Decoding is done according the specified Content-Encoding\n        or Content-Transfer-Encoding headers value.\n        \"\"\"\n        if CONTENT_TRANSFER_ENCODING in self.headers:\n            data = self._decode_content_transfer(data)\n        if not self._is_form_data and CONTENT_ENCODING in self.headers:\n            return self._decode_content(data)\n        return data\n\n    def _decode_content(self, data: bytes) -> bytes:\n        encoding = self.headers.get(CONTENT_ENCODING, \"\").lower()\n        if encoding == \"identity\":\n            return data\n        if encoding in {\"deflate\", \"gzip\"}:\n            return ZLibDecompressor(\n                encoding=encoding,\n                suppress_deflate_header=True,\n            ).decompress_sync(data)\n\n        raise RuntimeError(f\"unknown content encoding: {encoding}\")\n\n    def _decode_content_transfer(self, data: bytes) -> bytes:\n        encoding = self.headers.get(CONTENT_TRANSFER_ENCODING, \"\").lower()\n\n        if encoding == \"base64\":\n            return base64.b64decode(data)\n        elif encoding == \"quoted-printable\":\n            return binascii.a2b_qp(data)\n        elif encoding in (\"binary\", \"8bit\", \"7bit\"):\n            return data\n        else:\n            raise RuntimeError(f\"unknown content transfer encoding: {encoding}\")\n\n    def get_charset(self, default: str) -> str:\n        \"\"\"Returns charset parameter from Content-Type header or default.\"\"\"\n        ctype = self.headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n        return mimetype.parameters.get(\"charset\", self._default_charset or default)\n\n    @reify\n    def name(self) -> Optional[str]:\n        \"\"\"Returns name specified in Content-Disposition header.\n\n        If the header is missing or malformed, returns None.\n        \"\"\"\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, \"name\")\n\n    @reify\n    def filename(self) -> Optional[str]:\n        \"\"\"Returns filename specified in Content-Disposition header.\n\n        Returns None if the header is missing or malformed.\n        \"\"\"\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, \"filename\")"
    },
    {
      "chunk_id": 475,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "@payload_type(BodyPartReader, order=Order.try_first)\nclass BodyPartReaderPayload(Payload):\n    _value: BodyPartReader\n\n    def __init__(self, value: BodyPartReader, *args: Any, **kwargs: Any) -> None:\n        super().__init__(value, *args, **kwargs)\n\n        params: Dict[str, str] = {}\n        if value.name is not None:\n            params[\"name\"] = value.name\n        if value.filename is not None:\n            params[\"filename\"] = value.filename\n\n        if params:\n            self.set_content_disposition(\"attachment\", True, **params)\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        raise TypeError(\"Unable to decode.\")\n\n    async def write(self, writer: Any) -> None:\n        field = self._value\n        chunk = await field.read_chunk(size=2**16)\n        while chunk:\n            await writer.write(field.decode(chunk))\n            chunk = await field.read_chunk(size=2**16)"
    },
    {
      "chunk_id": 476,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class MultipartReader:\n    \"\"\"Multipart body reader.\"\"\"\n\n    response_wrapper_cls = MultipartResponseWrapper\n    multipart_reader_cls: Optional[Type[\"MultipartReader\"]] = None\n    part_reader_cls = BodyPartReader\n\n    def __init__(self, headers: Mapping[str, str], content: StreamReader) -> None:\n        self._mimetype = parse_mimetype(headers[CONTENT_TYPE])\n        assert self._mimetype.type == \"multipart\", \"multipart/* content type expected\"\n        if \"boundary\" not in self._mimetype.parameters:\n            raise ValueError(\n                \"boundary missed for Content-Type: %s\" % headers[CONTENT_TYPE]\n            )\n\n        self.headers = headers\n        self._boundary = (\"--\" + self._get_boundary()).encode()\n        self._content = content\n        self._default_charset: Optional[str] = None\n        self._last_part: Optional[Union[\"MultipartReader\", BodyPartReader]] = None\n        self._at_eof = False\n        self._at_bof = True\n        self._unread: List[bytes] = []\n\n    def __aiter__(self) -> Self:\n        return self\n\n    async def __anext__(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", BodyPartReader]]:\n        part = await self.next()\n        if part is None:\n            raise StopAsyncIteration\n        return part\n\n    @classmethod\n    def from_response(\n        cls,\n        response: \"ClientResponse\",\n    ) -> MultipartResponseWrapper:\n        \"\"\"Constructs reader instance from HTTP response.\n\n        :param response: :class:`~aiohttp.client.ClientResponse` instance\n        \"\"\"\n        obj = cls.response_wrapper_cls(\n            response, cls(response.headers, response.content)\n        )\n        return obj\n\n    def at_eof(self) -> bool:\n        \"\"\"Returns True if the final boundary was reached, false otherwise.\"\"\"\n        return self._at_eof\n\n    async def next(\n        self,\n    ) -> Optional[Union[\"MultipartReader\", BodyPartReader]]:\n        \"\"\"Emits the next multipart body part.\"\"\"\n        if self._at_eof:\n            return None\n        await self._maybe_release_last_part()\n        if self._at_bof:\n            await self._read_until_first_boundary()\n            self._at_bof = False\n        else:\n            await self._read_boundary()\n        if self._at_eof:\n            return None\n\n        part = await self.fetch_next_part()\n        if (\n            self._last_part is None\n            and self._mimetype.subtype == \"form-data\"\n            and isinstance(part, BodyPartReader)\n        ):\n            _, params = parse_content_disposition(part.headers.get(CONTENT_DISPOSITION))\n            if params.get(\"name\") == \"_charset_\":\n                charset = await part.read_chunk(32)\n                if len(charset) > 31:\n                    raise RuntimeError(\"Invalid default charset\")\n                self._default_charset = charset.strip().decode()\n                part = await self.fetch_next_part()\n        self._last_part = part\n        return self._last_part\n\n    async def release(self) -> None:\n        \"\"\"Reads all the body parts to the void till the final boundary.\"\"\"\n        while not self._at_eof:\n            item = await self.next()\n            if item is None:\n                break\n            await item.release()\n\n    async def fetch_next_part(\n        self,\n    ) -> Union[\"MultipartReader\", BodyPartReader]:\n        \"\"\"Returns the next body part reader.\"\"\"\n        headers = await self._read_headers()\n        return self._get_part_reader(headers)\n\n    def _get_part_reader(\n        self,\n        headers: \"CIMultiDictProxy[str]\",\n    ) -> Union[\"MultipartReader\", BodyPartReader]:\n        \"\"\"Dispatches the response by the `Content-Type` header.\n\n        Returns a suitable reader instance.\n\n        :param dict headers: Response headers\n        \"\"\"\n        ctype = headers.get(CONTENT_TYPE, \"\")\n        mimetype = parse_mimetype(ctype)\n\n        if mimetype.type == \"multipart\":\n            if self.multipart_reader_cls is None:\n                return type(self)(headers, self._content)\n            return self.multipart_reader_cls(headers, self._content)\n        else:\n            return self.part_reader_cls(\n                self._boundary,\n                headers,\n                self._content,\n                subtype=self._mimetype.subtype,\n                default_charset=self._default_charset,\n            )\n\n    def _get_boundary(self) -> str:\n        boundary = self._mimetype.parameters[\"boundary\"]\n        if len(boundary) > 70:\n            raise ValueError(\"boundary %r is too long (70 chars max)\" % boundary)\n\n        return boundary\n\n    async def _readline(self) -> bytes:\n        if self._unread:\n            return self._unread.pop()\n        return await self._content.readline()\n\n    async def _read_until_first_boundary(self) -> None:\n        while True:\n            chunk = await self._readline()\n            if chunk == b\"\":\n                raise ValueError(f\"Could not find starting boundary {self._boundary!r}\")\n            chunk = chunk.rstrip()\n            if chunk == self._boundary:\n                return\n            elif chunk == self._boundary + b\"--\":\n                self._at_eof = True\n                return\n\n    async def _read_boundary(self) -> None:\n        chunk = (await self._readline()).rstrip()\n        if chunk == self._boundary:\n            pass\n        elif chunk == self._boundary + b\"--\":\n            self._at_eof = True\n            epilogue = await self._readline()\n            next_line = await self._readline()\n\n            if next_line[:2] == b\"--\":\n                self._unread.append(next_line)\n            else:\n                self._unread.extend([next_line, epilogue])\n        else:\n            raise ValueError(f\"Invalid boundary {chunk!r}, expected {self._boundary!r}\")\n\n    async def _read_headers(self) -> \"CIMultiDictProxy[str]\":\n        lines = [b\"\"]\n        while True:\n            chunk = await self._content.readline()\n            chunk = chunk.strip()\n            lines.append(chunk)\n            if not chunk:\n                break\n        parser = HeadersParser()\n        headers, raw_headers = parser.parse_headers(lines)\n        return headers\n\n    async def _maybe_release_last_part(self) -> None:\n        \"\"\"Ensures that the last read body part is read completely.\"\"\"\n        if self._last_part is not None:\n            if not self._last_part.at_eof():\n                await self._last_part.release()\n            self._unread.extend(self._last_part._unread)\n            self._last_part = None"
    },
    {
      "chunk_id": 477,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "_Part = Tuple[Payload, str, str]"
    },
    {
      "chunk_id": 478,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class MultipartWriter(Payload):\n    \"\"\"Multipart body writer.\"\"\"\n\n    _value: None\n\n    def __init__(self, subtype: str = \"mixed\", boundary: Optional[str] = None) -> None:\n        boundary = boundary if boundary is not None else uuid.uuid4().hex\n        try:\n            self._boundary = boundary.encode(\"ascii\")\n        except UnicodeEncodeError:\n            raise ValueError(\"boundary should contain ASCII only chars\") from None\n\n        if len(boundary) > 70:\n            raise ValueError(\"boundary %r is too long (70 chars max)\" % boundary)\n\n        ctype = f\"multipart/{subtype}; boundary={self._boundary_value}\"\n\n        super().__init__(None, content_type=ctype)\n\n        self._parts: List[_Part] = []\n        self._is_form_data = subtype == \"form-data\"\n\n    def __enter__(self) -> \"MultipartWriter\":\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __iter__(self) -> Iterator[_Part]:\n        return iter(self._parts)\n\n    def __len__(self) -> int:\n        return len(self._parts)\n\n    def __bool__(self) -> bool:\n        return True\n\n    _valid_tchar_regex = re.compile(rb\"\\A[!#$%&'*+\\-.^_`|~\\w]+\\Z\")\n    _invalid_qdtext_char_regex = re.compile(rb\"[\\x00-\\x08\\x0A-\\x1F\\x7F]\")\n\n    @property\n    def _boundary_value(self) -> str:\n        value = self._boundary\n        if re.match(self._valid_tchar_regex, value):\n            return value.decode(\"ascii\")\n\n        if re.search(self._invalid_qdtext_char_regex, value):\n            raise ValueError(\"boundary value contains invalid characters\")\n\n        quoted_value_content = value.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted_value_content = quoted_value_content.replace(b'\"', b'\\\\\"')\n\n        return '\"' + quoted_value_content.decode(\"ascii\") + '\"'\n\n    @property\n    def boundary(self) -> str:\n        return self._boundary.decode(\"ascii\")\n\n    def append(self, obj: Any, headers: Optional[Mapping[str, str]] = None) -> Payload:\n        if headers is None:\n            headers = CIMultiDict()\n\n        if isinstance(obj, Payload):\n            obj.headers.update(headers)\n            return self.append_payload(obj)\n        else:\n            try:\n                payload = get_payload(obj, headers=headers)\n            except LookupError:\n                raise TypeError(\"Cannot create payload from %r\" % obj)\n            else:\n                return self.append_payload(payload)\n\n    def append_payload(self, payload: Payload) -> Payload:\n        \"\"\"Adds a new body part to multipart writer.\"\"\"\n        encoding: Optional[str] = None\n        te_encoding: Optional[str] = None\n        if self._is_form_data:\n            assert (\n                not {CONTENT_ENCODING, CONTENT_LENGTH, CONTENT_TRANSFER_ENCODING}\n                & payload.headers.keys()\n            )\n            if CONTENT_DISPOSITION not in payload.headers:\n                name = f\"section-{len(self._parts)}\"\n                payload.set_content_disposition(\"form-data\", name=name)\n        else:\n            encoding = payload.headers.get(CONTENT_ENCODING, \"\").lower()\n            if encoding and encoding not in (\"deflate\", \"gzip\", \"identity\"):\n                raise RuntimeError(f\"unknown content encoding: {encoding}\")\n            if encoding == \"identity\":\n                encoding = None\n\n            te_encoding = payload.headers.get(CONTENT_TRANSFER_ENCODING, \"\").lower()\n            if te_encoding not in (\"\", \"base64\", \"quoted-printable\", \"binary\"):\n                raise RuntimeError(f\"unknown content transfer encoding: {te_encoding}\")\n            if te_encoding == \"binary\":\n                te_encoding = None\n\n            size = payload.size\n            if size is not None and not (encoding or te_encoding):\n                payload.headers[CONTENT_LENGTH] = str(size)\n\n        self._parts.append((payload, encoding, te_encoding))  # type: ignore[arg-type]\n        return payload\n\n    def append_json(\n        self, obj: Any, headers: Optional[Mapping[str, str]] = None\n    ) -> Payload:\n        \"\"\"Helper to append JSON part.\"\"\"\n        if headers is None:\n            headers = CIMultiDict()\n\n        return self.append_payload(JsonPayload(obj, headers=headers))\n\n    def append_form(\n        self,\n        obj: Union[Sequence[Tuple[str, str]], Mapping[str, str]],\n        headers: Optional[Mapping[str, str]] = None,\n    ) -> Payload:\n        \"\"\"Helper to append form urlencoded part.\"\"\"\n        assert isinstance(obj, (Sequence, Mapping))\n\n        if headers is None:\n            headers = CIMultiDict()\n\n        if isinstance(obj, Mapping):\n            obj = list(obj.items())\n        data = urlencode(obj, doseq=True)\n\n        return self.append_payload(\n            StringPayload(\n                data, headers=headers, content_type=\"application/x-www-form-urlencoded\"\n            )\n        )\n\n    @property\n    def size(self) -> Optional[int]:\n        \"\"\"Size of the payload.\"\"\"\n        total = 0\n        for part, encoding, te_encoding in self._parts:\n            if encoding or te_encoding or part.size is None:\n                return None\n\n            total += int(\n                2\n                + len(self._boundary)\n                + 2\n                + part.size\n                + len(part._binary_headers)\n                + 2\n            )\n\n        total += 2 + len(self._boundary) + 4\n        return total\n\n    def decode(self, encoding: str = \"utf-8\", errors: str = \"strict\") -> str:\n        return \"\".join(\n            \"--\"\n            + self.boundary\n            + \"\\r\\n\"\n            + part._binary_headers.decode(encoding, errors)\n            + part.decode()\n            for part, _e, _te in self._parts\n        )\n\n    async def write(self, writer: Any, close_boundary: bool = True) -> None:\n        \"\"\"Write body.\"\"\"\n        for part, encoding, te_encoding in self._parts:\n            if self._is_form_data:\n                assert CONTENT_DISPOSITION in part.headers\n                assert \"name=\" in part.headers[CONTENT_DISPOSITION]\n\n            await writer.write(b\"--\" + self._boundary + b\"\\r\\n\")\n            await writer.write(part._binary_headers)\n\n            if encoding or te_encoding:\n                w = MultipartPayloadWriter(writer)\n                if encoding:\n                    w.enable_compression(encoding)\n                if te_encoding:\n                    w.enable_encoding(te_encoding)\n                await part.write(w)  # type: ignore[arg-type]\n                await w.write_eof()\n            else:\n                await part.write(writer)\n\n            await writer.write(b\"\\r\\n\")\n\n        if close_boundary:\n            await writer.write(b\"--\" + self._boundary + b\"--\\r\\n\")"
    },
    {
      "chunk_id": 479,
      "source": "__internal__/data_repo/aiohttp/aiohttp/multipart.py",
      "content": "class MultipartPayloadWriter:\n    def __init__(self, writer: Any) -> None:\n        self._writer = writer\n        self._encoding: Optional[str] = None\n        self._compress: Optional[ZLibCompressor] = None\n        self._encoding_buffer: Optional[bytearray] = None\n\n    def enable_encoding(self, encoding: str) -> None:\n        if encoding == \"base64\":\n            self._encoding = encoding\n            self._encoding_buffer = bytearray()\n        elif encoding == \"quoted-printable\":\n            self._encoding = \"quoted-printable\"\n\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: int = zlib.Z_DEFAULT_STRATEGY\n    ) -> None:\n        self._compress = ZLibCompressor(\n            encoding=encoding,\n            suppress_deflate_header=True,\n            strategy=strategy,\n        )\n\n    async def write_eof(self) -> None:\n        if self._compress is not None:\n            chunk = self._compress.flush()\n            if chunk:\n                self._compress = None\n                await self.write(chunk)\n\n        if self._encoding == \"base64\":\n            if self._encoding_buffer:\n                await self._writer.write(base64.b64encode(self._encoding_buffer))\n\n    async def write(self, chunk: bytes) -> None:\n        if self._compress is not None:\n            if chunk:\n                chunk = await self._compress.compress(chunk)\n                if not chunk:\n                    return\n\n        if self._encoding == \"base64\":\n            buf = self._encoding_buffer\n            assert buf is not None\n            buf.extend(chunk)\n\n            if buf:\n                div, mod = divmod(len(buf), 3)\n                enc_chunk, self._encoding_buffer = (buf[: div * 3], buf[div * 3 :])\n                if enc_chunk:\n                    b64chunk = base64.b64encode(enc_chunk)\n                    await self._writer.write(b64chunk)\n        elif self._encoding == \"quoted-printable\":\n            await self._writer.write(binascii.b2a_qp(chunk))\n        else:\n            await self._writer.write(chunk)"
    },
    {
      "chunk_id": 480,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "```python"
    },
    {
      "chunk_id": 481,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "import asyncio\nimport signal\nimport socket\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Any, Generic, List, Optional, Set, Type, TypeVar\n\nfrom yarl import URL\n\nfrom .abc import AbstractAccessLogger, AbstractStreamWriter\nfrom .http_parser import RawRequestMessage\nfrom .streams import StreamReader\nfrom .typedefs import PathLike\nfrom .web_app import Application\nfrom .web_log import AccessLogger\nfrom .web_protocol import RequestHandler\nfrom .web_request import BaseRequest, Request\nfrom .web_server import Server\n\nif TYPE_CHECKING:\n    from ssl import SSLContext\nelse:\n    try:\n        from ssl import SSLContext\n    except ImportError:  # pragma: no cover\n        SSLContext = object  # type: ignore[misc,assignment]\n\n__all__ = (\n    \"BaseSite\",\n    \"TCPSite\",\n    \"UnixSite\",\n    \"NamedPipeSite\",\n    \"SockSite\",\n    \"BaseRunner\",\n    \"AppRunner\",\n    \"ServerRunner\",\n    \"GracefulExit\",\n)\n\n_Request = TypeVar(\"_Request\", bound=BaseRequest)"
    },
    {
      "chunk_id": 482,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class GracefulExit(SystemExit):\n    code = 1"
    },
    {
      "chunk_id": 483,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "def _raise_graceful_exit() -> None:\n    raise GracefulExit()"
    },
    {
      "chunk_id": 484,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class BaseSite(ABC):\n    __slots__ = (\"_runner\", \"_ssl_context\", \"_backlog\", \"_server\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner[Any]\",\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        if runner.server is None:\n            raise RuntimeError(\"Call runner.setup() before making a site\")\n        self._runner = runner\n        self._ssl_context = ssl_context\n        self._backlog = backlog\n        self._server: Optional[asyncio.AbstractServer] = None\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def start(self) -> None:\n        self._runner._reg_site(self)\n\n    async def stop(self) -> None:\n        self._runner._check_site(self)\n        if self._server is not None:  # Maybe not started yet\n            self._server.close()\n\n        self._runner._unreg_site(self)"
    },
    {
      "chunk_id": 485,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class TCPSite(BaseSite):\n    __slots__ = (\"_host\", \"_port\", \"_reuse_address\", \"_reuse_port\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner[Any]\",\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n        reuse_address: Optional[bool] = None,\n        reuse_port: Optional[bool] = None,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._host = host\n        if port is None:\n            port = 8443 if self._ssl_context else 8080\n        self._port = port\n        self._reuse_address = reuse_address\n        self._reuse_port = reuse_port\n\n    @property\n    def name(self) -> str:\n        scheme = \"https\" if self._ssl_context else \"http\"\n        host = \"0.0.0.0\" if not self._host else self._host\n        return str(URL.build(scheme=scheme, host=host, port=self._port))\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_server(\n            server,\n            self._host,\n            self._port,\n            ssl=self._ssl_context,\n            backlog=self._backlog,\n            reuse_address=self._reuse_address,\n            reuse_port=self._reuse_port,\n        )"
    },
    {
      "chunk_id": 486,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class UnixSite(BaseSite):\n    __slots__ = (\"_path\",)\n\n    def __init__(\n        self,\n        runner: \"BaseRunner[Any]\",\n        path: PathLike,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._path = path\n\n    @property\n    def name(self) -> str:\n        scheme = \"https\" if self._ssl_context else \"http\"\n        return f\"{scheme}://unix:{self._path}:\"\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_unix_server(\n            server,\n            self._path,\n            ssl=self._ssl_context,\n            backlog=self._backlog,\n        )"
    },
    {
      "chunk_id": 487,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class NamedPipeSite(BaseSite):\n    __slots__ = (\"_path\",)\n\n    def __init__(self, runner: \"BaseRunner[Any]\", path: str) -> None:\n        loop = asyncio.get_event_loop()\n        if not isinstance(\n            loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\n        ):\n            raise RuntimeError(\n                \"Named Pipes only available in proactor loop under windows\"\n            )\n        super().__init__(runner)\n        self._path = path\n\n    @property\n    def name(self) -> str:\n        return self._path\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        _server = await loop.start_serving_pipe(  # type: ignore[attr-defined]\n            server, self._path\n        )\n        self._server = _server[0]"
    },
    {
      "chunk_id": 488,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class SockSite(BaseSite):\n    __slots__ = (\"_sock\", \"_name\")\n\n    def __init__(\n        self,\n        runner: \"BaseRunner[Any]\",\n        sock: socket.socket,\n        *,\n        ssl_context: Optional[SSLContext] = None,\n        backlog: int = 128,\n    ) -> None:\n        super().__init__(\n            runner,\n            ssl_context=ssl_context,\n            backlog=backlog,\n        )\n        self._sock = sock\n        scheme = \"https\" if self._ssl_context else \"http\"\n        if hasattr(socket, \"AF_UNIX\") and sock.family == socket.AF_UNIX:\n            name = f\"{scheme}://unix:{sock.getsockname()}:\"\n        else:\n            host, port = sock.getsockname()[:2]\n            name = str(URL.build(scheme=scheme, host=host, port=port))\n        self._name = name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    async def start(self) -> None:\n        await super().start()\n        loop = asyncio.get_event_loop()\n        server = self._runner.server\n        assert server is not None\n        self._server = await loop.create_server(\n            server, sock=self._sock, ssl=self._ssl_context, backlog=self._backlog\n        )"
    },
    {
      "chunk_id": 489,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class BaseRunner(ABC, Generic[_Request]):\n    __slots__ = (\"_handle_signals\", \"_kwargs\", \"_server\", \"_sites\", \"_shutdown_timeout\")\n\n    def __init__(\n        self,\n        *,\n        handle_signals: bool = False,\n        shutdown_timeout: float = 60.0,\n        **kwargs: Any,\n    ) -> None:\n        self._handle_signals = handle_signals\n        self._kwargs = kwargs\n        self._server: Optional[Server[_Request]] = None\n        self._sites: List[BaseSite] = []\n        self._shutdown_timeout = shutdown_timeout\n\n    @property\n    def server(self) -> Optional[Server[_Request]]:\n        return self._server\n\n    @property\n    def addresses(self) -> List[Any]:\n        ret: List[Any] = []\n        for site in self._sites:\n            server = site._server\n            if server is not None:\n                sockets = server.sockets  # type: ignore[attr-defined]\n                if sockets is not None:\n                    for sock in sockets:\n                        ret.append(sock.getsockname())\n        return ret\n\n    @property\n    def sites(self) -> Set[BaseSite]:\n        return set(self._sites)\n\n    async def setup(self) -> None:\n        loop = asyncio.get_event_loop()\n\n        if self._handle_signals:\n            try:\n                loop.add_signal_handler(signal.SIGINT, _raise_graceful_exit)\n                loop.add_signal_handler(signal.SIGTERM, _raise_graceful_exit)\n            except NotImplementedError:  # pragma: no cover\n                # add_signal_handler is not implemented on Windows\n                pass\n\n        self._server = await self._make_server()\n\n    @abstractmethod\n    async def shutdown(self) -> None:\n        \"\"\"Call any shutdown hooks to help server close gracefully.\"\"\"\n\n    async def cleanup(self) -> None:\n        # The loop over sites is intentional, an exception on gather()\n        # leaves self._sites in unpredictable state.\n        # The loop guarantees that a site is either deleted on success or\n        # still present on failure\n        for site in list(self._sites):\n            await site.stop()\n\n        if self._server:  # If setup succeeded\n            # Yield to event loop to ensure incoming requests prior to stopping the sites\n            # have all started to be handled before we proceed to close idle connections.\n            await asyncio.sleep(0)\n            self._server.pre_shutdown()\n            await self.shutdown()\n            await self._server.shutdown(self._shutdown_timeout)\n        await self._cleanup_server()\n\n        self._server = None\n        if self._handle_signals:\n            loop = asyncio.get_running_loop()\n            try:\n                loop.remove_signal_handler(signal.SIGINT)\n                loop.remove_signal_handler(signal.SIGTERM)\n            except NotImplementedError:  # pragma: no cover\n                # remove_signal_handler is not implemented on Windows\n                pass\n\n    @abstractmethod\n    async def _make_server(self) -> Server[_Request]:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def _cleanup_server(self) -> None:\n        pass  # pragma: no cover\n\n    def _reg_site(self, site: BaseSite) -> None:\n        if site in self._sites:\n            raise RuntimeError(f\"Site {site} is already registered in runner {self}\")\n        self._sites.append(site)\n\n    def _check_site(self, site: BaseSite) -> None:\n        if site not in self._sites:\n            raise RuntimeError(f\"Site {site} is not registered in runner {self}\")\n\n    def _unreg_site(self, site: BaseSite) -> None:\n        if site not in self._sites:\n            raise RuntimeError(f\"Site {site} is not registered in runner {self}\")\n        self._sites.remove(site)"
    },
    {
      "chunk_id": 490,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class ServerRunner(BaseRunner[BaseRequest]):\n    \"\"\"Low-level web server runner\"\"\"\n\n    __slots__ = (\"_web_server\",)\n\n    def __init__(\n        self,\n        web_server: Server[BaseRequest],\n        *,\n        handle_signals: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(handle_signals=handle_signals, **kwargs)\n        self._web_server = web_server\n\n    async def shutdown(self) -> None:\n        pass\n\n    async def _make_server(self) -> Server[BaseRequest]:\n        return self._web_server\n\n    async def _cleanup_server(self) -> None:\n        pass"
    },
    {
      "chunk_id": 491,
      "source": "__internal__/data_repo/aiohttp/aiohttp/web_runner.py",
      "content": "class AppRunner(BaseRunner[Request]):\n    \"\"\"Web Application runner\"\"\"\n\n    __slots__ = (\"_app\",)\n\n    def __init__(\n        self,\n        app: Application,\n        *,\n        handle_signals: bool = False,\n        access_log_class: Type[AbstractAccessLogger] = AccessLogger,\n        **kwargs: Any,\n    ) -> None:\n        if not isinstance(app, Application):\n            raise TypeError(\n                \"The first argument should be web.Application \"\n                \"instance, got {!r}\".format(app)\n            )\n        kwargs[\"access_log_class\"] = access_log_class\n\n        if app._handler_args:\n            for k, v in app._handler_args.items():\n                kwargs[k] = v\n\n        if not issubclass(kwargs[\"access_log_class\"], AbstractAccessLogger):\n            raise TypeError(\n                \"access_log_class must be subclass of \"\n                \"aiohttp.abc.AbstractAccessLogger, got {}\".format(\n                    kwargs[\"access_log_class\"]\n                )\n            )\n\n        super().__init__(handle_signals=handle_signals, **kwargs)\n        self._app = app\n\n    @property\n    def app(self) -> Application:\n        return self._app\n\n    async def shutdown(self) -> None:\n        await self._app.shutdown()\n\n    async def _make_server(self) -> Server[Request]:\n        self._app.on_startup.freeze()\n        await self._app.startup()\n        self._app.freeze()\n\n        return Server(\n            self._app._handle,\n            request_factory=self._make_request,\n            **self._kwargs,\n        )\n\n    def _make_request(\n        self,\n        message: RawRequestMessage,\n        payload: StreamReader,\n        protocol: RequestHandler[Request],\n        writer: AbstractStreamWriter,\n        task: \"asyncio.Task[None]\",\n        _cls: Type[Request] = Request,\n    ) -> Request:\n        loop = asyncio.get_running_loop()\n        return _cls(\n            message,\n            payload,\n            protocol,\n            writer,\n            task,\n            loop,\n            client_max_size=self.app._client_max_size,\n        )\n\n    async def _cleanup_server(self) -> None:\n        await self._app.cleanup()\n```"
    },
    {
      "chunk_id": 492,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/__init__.py",
      "content": "\"\"\"WebSocket protocol versions 13 and 8.\"\"\""
    },
    {
      "chunk_id": 493,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/writer.py",
      "content": "\"\"\"WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport random\nimport zlib\nfrom functools import partial\nfrom typing import Any, Final, Optional, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..client_exceptions import ClientConnectionResetError\nfrom ..compression_utils import ZLibCompressor\nfrom .helpers import (\n    MASK_LEN,\n    MSG_SIZE,\n    PACK_CLOSE_CODE,\n    PACK_LEN1,\n    PACK_LEN2,\n    PACK_LEN3,\n    PACK_RANDBITS,\n    websocket_mask,\n)\nfrom .models import WS_DEFLATE_TRAILING, WSMsgType\n\nDEFAULT_LIMIT: Final[int] = 2**16\n\nWEBSOCKET_MAX_SYNC_CHUNK_SIZE = 5 * 1024"
    },
    {
      "chunk_id": 494,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/writer.py",
      "content": "class WebSocketWriter:\n    \"\"\"WebSocket writer.\n\n    The writer is responsible for sending messages to the client. It is\n    created by the protocol when a connection is established. The writer\n    should avoid implementing any application logic and should only be\n    concerned with the low-level details of the WebSocket protocol.\n    \"\"\"\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        transport: asyncio.Transport,\n        *,\n        use_mask: bool = False,\n        limit: int = DEFAULT_LIMIT,\n        random: random.Random = random.Random(),\n        compress: int = 0,\n        notakeover: bool = False,\n    ) -> None:\n        \"\"\"Initialize a WebSocket writer.\"\"\"\n        self.protocol = protocol\n        self.transport = transport\n        self.use_mask = use_mask\n        self.get_random_bits = partial(random.getrandbits, 32)\n        self.compress = compress\n        self.notakeover = notakeover\n        self._closing = False\n        self._limit = limit\n        self._output_size = 0\n        self._compressobj: Any = None  # actually compressobj\n\n    async def send_frame(\n        self, message: bytes, opcode: int, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if self._closing and not (opcode & WSMsgType.CLOSE):\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n\n        rsv = 0\n        if (compress or self.compress) and opcode < 8:\n            rsv = 0x40\n\n            if compress:\n                compressobj = self._make_compress_obj(compress)\n            else:\n                if not self._compressobj:\n                    self._compressobj = self._make_compress_obj(self.compress)\n                compressobj = self._compressobj\n\n            message = (\n                await compressobj.compress(message)\n                + compressobj.flush(\n                    zlib.Z_FULL_FLUSH if self.notakeover else zlib.Z_SYNC_FLUSH\n                )\n            ).removesuffix(WS_DEFLATE_TRAILING)\n\n        msg_length = len(message)\n\n        use_mask = self.use_mask\n        mask_bit = 0x80 if use_mask else 0\n\n        first_byte = 0x80 | rsv | opcode\n        if msg_length < 126:\n            header = PACK_LEN1(first_byte, msg_length | mask_bit)\n            header_len = 2\n        elif msg_length < 65536:\n            header = PACK_LEN2(first_byte, 126 | mask_bit, msg_length)\n            header_len = 4\n        else:\n            header = PACK_LEN3(first_byte, 127 | mask_bit, msg_length)\n            header_len = 10\n\n        if self.transport.is_closing():\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n\n        if use_mask:\n            mask = PACK_RANDBITS(self.get_random_bits())\n            message = bytearray(message)\n            websocket_mask(mask, message)\n            self.transport.write(header + mask + message)\n            self._output_size += MASK_LEN\n        elif msg_length > MSG_SIZE:\n            self.transport.write(header)\n            self.transport.write(message)\n        else:\n            self.transport.write(header + message)\n\n        self._output_size += header_len + msg_length\n\n        if self._output_size > self._limit:\n            self._output_size = 0\n            if self.protocol._paused:\n                await self.protocol._drain_helper()\n\n    def _make_compress_obj(self, compress: int) -> ZLibCompressor:\n        return ZLibCompressor(\n            level=zlib.Z_BEST_SPEED,\n            wbits=-compress,\n            max_sync_chunk_size=WEBSOCKET_MAX_SYNC_CHUNK_SIZE,\n        )\n\n    async def close(self, code: int = 1000, message: Union[bytes, str] = b\"\") -> None:\n        \"\"\"Close the websocket, sending the specified code and message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        try:\n            await self.send_frame(\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE\n            )\n        finally:\n            self._closing = True"
    },
    {
      "chunk_id": 495,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "\"\"\"Models for WebSocket protocol versions 13 and 8.\"\"\""
    },
    {
      "chunk_id": 496,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "import json\nfrom enum import IntEnum\nfrom typing import Any, Callable, Final, Literal, NamedTuple, Optional, Union, cast"
    },
    {
      "chunk_id": 497,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "WS_DEFLATE_TRAILING: Final[bytes] = bytes([0x00, 0x00, 0xFF, 0xFF])"
    },
    {
      "chunk_id": 498,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSCloseCode(IntEnum):\n    OK = 1000\n    GOING_AWAY = 1001\n    PROTOCOL_ERROR = 1002\n    UNSUPPORTED_DATA = 1003\n    ABNORMAL_CLOSURE = 1006\n    INVALID_TEXT = 1007\n    POLICY_VIOLATION = 1008\n    MESSAGE_TOO_BIG = 1009\n    MANDATORY_EXTENSION = 1010\n    INTERNAL_ERROR = 1011\n    SERVICE_RESTART = 1012\n    TRY_AGAIN_LATER = 1013\n    BAD_GATEWAY = 1014"
    },
    {
      "chunk_id": 499,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMsgType(IntEnum):\n    # websocket spec types\n    CONTINUATION = 0x0\n    TEXT = 0x1\n    BINARY = 0x2\n    PING = 0x9\n    PONG = 0xA\n    CLOSE = 0x8\n\n    # aiohttp specific types\n    CLOSING = 0x100\n    CLOSED = 0x101\n    ERROR = 0x102"
    },
    {
      "chunk_id": 500,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageContinuation(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CONTINUATION] = WSMsgType.CONTINUATION"
    },
    {
      "chunk_id": 501,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageText(NamedTuple):\n    data: str\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.TEXT] = WSMsgType.TEXT\n\n    def json(\n        self, *, loads: Callable[[Union[str, bytes, bytearray]], Any] = json.loads\n    ) -> Any:\n        \"\"\"Return parsed JSON data.\"\"\"\n        return loads(self.data)"
    },
    {
      "chunk_id": 502,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageBinary(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.BINARY] = WSMsgType.BINARY\n\n    def json(\n        self, *, loads: Callable[[Union[str, bytes, bytearray]], Any] = json.loads\n    ) -> Any:\n        \"\"\"Return parsed JSON data.\"\"\"\n        return loads(self.data)"
    },
    {
      "chunk_id": 503,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessagePing(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.PING] = WSMsgType.PING"
    },
    {
      "chunk_id": 504,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessagePong(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.PONG] = WSMsgType.PONG"
    },
    {
      "chunk_id": 505,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageClose(NamedTuple):\n    data: int\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSE] = WSMsgType.CLOSE"
    },
    {
      "chunk_id": 506,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageClosing(NamedTuple):\n    data: None = None\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSING] = WSMsgType.CLOSING"
    },
    {
      "chunk_id": 507,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageClosed(NamedTuple):\n    data: None = None\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSED] = WSMsgType.CLOSED"
    },
    {
      "chunk_id": 508,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSMessageError(NamedTuple):\n    data: BaseException\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.ERROR] = WSMsgType.ERROR"
    },
    {
      "chunk_id": 509,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "WSMessage = Union[\n    WSMessageContinuation,\n    WSMessageText,\n    WSMessageBinary,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageClose,\n    WSMessageClosing,\n    WSMessageClosed,\n    WSMessageError,\n]"
    },
    {
      "chunk_id": 510,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "WS_CLOSED_MESSAGE = WSMessageClosed()\nWS_CLOSING_MESSAGE = WSMessageClosing()"
    },
    {
      "chunk_id": 511,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WebSocketError(Exception):\n    \"\"\"WebSocket protocol parser error.\"\"\"\n\n    def __init__(self, code: int, message: str) -> None:\n        self.code = code\n        super().__init__(code, message)\n\n    def __str__(self) -> str:\n        return cast(str, self.args[1])"
    },
    {
      "chunk_id": 512,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/models.py",
      "content": "class WSHandshakeError(Exception):\n    \"\"\"WebSocket protocol handshake error.\"\"\""
    },
    {
      "chunk_id": 513,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader.py",
      "content": "\"\"\"Reader for WebSocket protocol versions 13 and 8.\"\"\""
    },
    {
      "chunk_id": 514,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader.py",
      "content": "from typing import TYPE_CHECKING"
    },
    {
      "chunk_id": 515,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader.py",
      "content": "from ..helpers import NO_EXTENSIONS"
    },
    {
      "chunk_id": 516,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader.py",
      "content": "if TYPE_CHECKING or NO_EXTENSIONS:  # pragma: no cover\n    from .reader_py import (\n        WebSocketDataQueue as WebSocketDataQueuePython,\n        WebSocketReader as WebSocketReaderPython,\n    )\n\n    WebSocketReader = WebSocketReaderPython\n    WebSocketDataQueue = WebSocketDataQueuePython\nelse:\n    try:\n        from .reader_c import (  # type: ignore[import-not-found]\n            WebSocketDataQueue as WebSocketDataQueueCython,\n            WebSocketReader as WebSocketReaderCython,\n        )\n\n        WebSocketReader = WebSocketReaderCython\n        WebSocketDataQueue = WebSocketDataQueueCython\n    except ImportError:  # pragma: no cover\n        from .reader_py import (\n            WebSocketDataQueue as WebSocketDataQueuePython,\n            WebSocketReader as WebSocketReaderPython,\n        )\n\n        WebSocketReader = WebSocketReaderPython\n        WebSocketDataQueue = WebSocketDataQueuePython"
    },
    {
      "chunk_id": 517,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "\"\"\"Helpers for WebSocket protocol versions 13 and 8.\"\"\""
    },
    {
      "chunk_id": 518,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "import functools\nimport re\nfrom struct import Struct\nfrom typing import TYPE_CHECKING, Final, List, Optional, Pattern, Tuple\n\nfrom ..helpers import NO_EXTENSIONS\nfrom .models import WSHandshakeError"
    },
    {
      "chunk_id": 519,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "UNPACK_LEN3 = Struct(\"!Q\").unpack_from\nUNPACK_CLOSE_CODE = Struct(\"!H\").unpack\nPACK_LEN1 = Struct(\"!BB\").pack\nPACK_LEN2 = Struct(\"!BBH\").pack\nPACK_LEN3 = Struct(\"!BBQ\").pack\nPACK_CLOSE_CODE = Struct(\"!H\").pack\nPACK_RANDBITS = Struct(\"!L\").pack\nMSG_SIZE: Final[int] = 2**14\nMASK_LEN: Final[int] = 4\n\nWS_KEY: Final[bytes] = b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\""
    },
    {
      "chunk_id": 520,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "@functools.lru_cache\ndef _xor_table() -> List[bytes]:\n    return [bytes(a ^ b for a in range(256)) for b in range(256)]"
    },
    {
      "chunk_id": 521,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "def _websocket_mask_python(mask: bytes, data: bytearray) -> None:\n    \"\"\"Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytearray`\n    object of any length. The contents of `data` are masked with `mask`,\n    as specified in section 5.3 of RFC 6455.\n\n    Note that this function mutates the `data` argument.\n\n    This pure-python implementation may be replaced by an optimized\n    version when available.\n\n    \"\"\"\n    assert isinstance(data, bytearray), data\n    assert len(mask) == 4, mask\n\n    if data:\n        _XOR_TABLE = _xor_table()\n        a, b, c, d = (_XOR_TABLE[n] for n in mask)\n        data[::4] = data[::4].translate(a)\n        data[1::4] = data[1::4].translate(b)\n        data[2::4] = data[2::4].translate(c)\n        data[3::4] = data[3::4].translate(d)"
    },
    {
      "chunk_id": 522,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "if TYPE_CHECKING or NO_EXTENSIONS:  # pragma: no cover\n    websocket_mask = _websocket_mask_python\nelse:\n    try:\n        from .mask import _websocket_mask_cython  # type: ignore[import-not-found]\n\n        websocket_mask = _websocket_mask_cython\n    except ImportError:  # pragma: no cover\n        websocket_mask = _websocket_mask_python"
    },
    {
      "chunk_id": 523,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "_WS_EXT_RE: Final[Pattern[str]] = re.compile(\n    r\"^(?:;\\s*(?:\"\n    r\"(server_no_context_takeover)|\"\n    r\"(client_no_context_takeover)|\"\n    r\"(server_max_window_bits(?:=(\\d+))?)|\"\n    r\"(client_max_window_bits(?:=(\\d+))?)))*$\"\n)"
    },
    {
      "chunk_id": 524,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "_WS_EXT_RE_SPLIT: Final[Pattern[str]] = re.compile(r\"permessage-deflate([^,]+)?\")"
    },
    {
      "chunk_id": 525,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "def ws_ext_parse(extstr: Optional[str], isserver: bool = False) -> Tuple[int, bool]:\n    if not extstr:\n        return 0, False\n\n    compress = 0\n    notakeover = False\n    for ext in _WS_EXT_RE_SPLIT.finditer(extstr):\n        defext = ext.group(1)\n        # Return compress = 15 when get `permessage-deflate`\n        if not defext:\n            compress = 15\n            break\n        match = _WS_EXT_RE.match(defext)\n        if match:\n            compress = 15\n            if isserver:\n                # Server never fail to detect compress handshake.\n                # Server does not need to send max wbit to client\n                if match.group(4):\n                    compress = int(match.group(4))\n                    # Group3 must match if group4 matches\n                    # Compress wbit 8 does not support in zlib\n                    # If compress level not support,\n                    # CONTINUE to next extension\n                    if compress > 15 or compress < 9:\n                        compress = 0\n                        continue\n                if match.group(1):\n                    notakeover = True\n                # Ignore regex group 5 & 6 for client_max_window_bits\n                break\n            else:\n                if match.group(6):\n                    compress = int(match.group(6))\n                    # Group5 must match if group6 matches\n                    # Compress wbit 8 does not support in zlib\n                    # If compress level not support,\n                    # FAIL the parse progress\n                    if compress > 15 or compress < 9:\n                        raise WSHandshakeError(\"Invalid window size\")\n                if match.group(2):\n                    notakeover = True\n                # Ignore regex group 5 & 6 for client_max_window_bits\n                break\n        # Return Fail if client side and not match\n        elif not isserver:\n            raise WSHandshakeError(\"Extension for deflate not supported\" + ext.group(1))\n\n    return compress, notakeover"
    },
    {
      "chunk_id": 526,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/helpers.py",
      "content": "def ws_ext_gen(\n    compress: int = 15, isserver: bool = False, server_notakeover: bool = False\n) -> str:\n    # client_notakeover=False not used for server\n    # compress wbit 8 does not support in zlib\n    if compress < 9 or compress > 15:\n        raise ValueError(\n            \"Compress wbits must between 9 and 15, zlib does not support wbits=8\"\n        )\n    enabledext = [\"permessage-deflate\"]\n    if not isserver:\n        enabledext.append(\"client_max_window_bits\")\n\n    if compress < 15:\n        enabledext.append(\"server_max_window_bits=\" + str(compress))\n    if server_notakeover:\n        enabledext.append(\"server_no_context_takeover\")\n    # if client_notakeover:\n    #     enabledext.append('client_no_context_takeover')\n    return \"; \".join(enabledext)"
    },
    {
      "chunk_id": 527,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_c.py",
      "content": "```python"
    },
    {
      "chunk_id": 528,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_c.py",
      "content": "\"\"\"Reader for WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport builtins\nfrom collections import deque\nfrom typing import Deque, Final, List, Optional, Set, Tuple, Type, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..compression_utils import ZLibDecompressor\nfrom ..helpers import _EXC_SENTINEL, set_exception\nfrom ..streams import EofStream\nfrom .helpers import UNPACK_CLOSE_CODE, UNPACK_LEN3, websocket_mask\nfrom .models import (\n    WS_DEFLATE_TRAILING,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMessageBinary,\n    WSMessageClose,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageText,\n    WSMsgType,\n)\n\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\n\n# States for the reader, used to parse the WebSocket frame\n# integer values are used so they can be cythonized\nREAD_HEADER = 1\nREAD_PAYLOAD_LENGTH = 2\nREAD_PAYLOAD_MASK = 3\nREAD_PAYLOAD = 4\n\nWS_MSG_TYPE_BINARY = WSMsgType.BINARY\nWS_MSG_TYPE_TEXT = WSMsgType.TEXT\n\n# WSMsgType values unpacked so they can by cythonized to ints\nOP_CODE_CONTINUATION = WSMsgType.CONTINUATION.value\nOP_CODE_TEXT = WSMsgType.TEXT.value\nOP_CODE_BINARY = WSMsgType.BINARY.value\nOP_CODE_CLOSE = WSMsgType.CLOSE.value\nOP_CODE_PING = WSMsgType.PING.value\nOP_CODE_PONG = WSMsgType.PONG.value\n\nEMPTY_FRAME_ERROR = (True, b\"\")\nEMPTY_FRAME = (False, b\"\")\n\nTUPLE_NEW = tuple.__new__"
    },
    {
      "chunk_id": 529,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_c.py",
      "content": "class WebSocketDataQueue:\n    \"\"\"WebSocketDataQueue resumes and pauses an underlying stream.\n\n    It is a destination for WebSocket data.\n    \"\"\"\n\n    def __init__(\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self._size = 0\n        self._protocol = protocol\n        self._limit = limit * 2\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Union[Type[BaseException], BaseException, None] = None\n        self._buffer: Deque[WSMessage] = deque()\n        self._get_buffer = self._buffer.popleft\n        self._put_buffer = self._buffer.append\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: builtins.BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def _release_waiter(self) -> None:\n        if (waiter := self._waiter) is None:\n            return\n        self._waiter = None\n        if not waiter.done():\n            waiter.set_result(None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n        self._release_waiter()\n\n    def feed_data(self, data: \"WSMessage\") -> None:\n        size = data.size\n        self._size += size\n        self._put_buffer(data)\n        self._release_waiter()\n        if self._size > self._limit and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    async def read(self) -> WSMessage:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n        return self._read_from_buffer()\n\n    def _read_from_buffer(self) -> WSMessage:\n        if self._buffer:\n            data = self._get_buffer()\n            size = data.size\n            self._size -= size\n            if self._size < self._limit and self._protocol._reading_paused:\n                self._protocol.resume_reading()\n            return data\n        if self._exception is not None:\n            raise self._exception\n        raise EofStream"
    },
    {
      "chunk_id": 530,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_c.py",
      "content": "class WebSocketReader:\n    def __init__(\n        self, queue: WebSocketDataQueue, max_msg_size: int, compress: bool = True\n    ) -> None:\n        self.queue = queue\n        self._max_msg_size = max_msg_size\n\n        self._exc: Optional[Exception] = None\n        self._partial = bytearray()\n        self._state = READ_HEADER\n\n        self._opcode: Optional[int] = None\n        self._frame_fin = False\n        self._frame_opcode: Optional[int] = None\n        self._frame_payload: Union[bytes, bytearray] = b\"\"\n        self._frame_payload_len = 0\n\n        self._tail: bytes = b\"\"\n        self._has_mask = False\n        self._frame_mask: Optional[bytes] = None\n        self._payload_length = 0\n        self._payload_length_flag = 0\n        self._compressed: Optional[bool] = None\n        self._decompressobj: Optional[ZLibDecompressor] = None\n        self._compress = compress\n\n    def feed_eof(self) -> None:\n        self.queue.feed_eof()\n\n    # data can be bytearray on Windows because proactor event loop uses bytearray\n    # and asyncio types this to Union[bytes, bytearray, memoryview] so we need\n    # coerce data to bytes if it is not\n    def feed_data(\n        self, data: Union[bytes, bytearray, memoryview]\n    ) -> Tuple[bool, bytes]:\n        if type(data) is not bytes:\n            data = bytes(data)\n\n        if self._exc is not None:\n            return True, data\n\n        try:\n            self._feed_data(data)\n        except Exception as exc:\n            self._exc = exc\n            set_exception(self.queue, exc)\n            return EMPTY_FRAME_ERROR\n\n        return EMPTY_FRAME\n\n    def _feed_data(self, data: bytes) -> None:\n        msg: WSMessage\n        for frame in self.parse_frame(data):\n            fin = frame[0]\n            opcode = frame[1]\n            payload = frame[2]\n            compressed = frame[3]\n\n            is_continuation = opcode == OP_CODE_CONTINUATION\n            if opcode == OP_CODE_TEXT or opcode == OP_CODE_BINARY or is_continuation:\n                # load text/binary\n                if not fin:\n                    # got partial frame payload\n                    if not is_continuation:\n                        self._opcode = opcode\n                    self._partial += payload\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Message size {} exceeds limit {}\".format(\n                                len(self._partial), self._max_msg_size\n                            ),\n                        )\n                    continue\n\n                has_partial = bool(self._partial)\n                if is_continuation:\n                    if self._opcode is None:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Continuation frame for non started message\",\n                        )\n                    opcode = self._opcode\n                    self._opcode = None\n                # previous frame was non finished\n                # we should get continuation opcode\n                elif has_partial:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"The opcode in non-fin frame is expected \"\n                        \"to be zero, got {!r}\".format(opcode),\n                    )\n\n                assembled_payload: Union[bytes, bytearray]\n                if has_partial:\n                    assembled_payload = self._partial + payload\n                    self._partial.clear()\n                else:\n                    assembled_payload = payload\n\n                if self._max_msg_size and len(assembled_payload) >= self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        \"Message size {} exceeds limit {}\".format(\n                            len(assembled_payload), self._max_msg_size\n                        ),\n                    )\n\n                # Decompress process must to be done after all packets\n                # received.\n                if compressed:\n                    if not self._decompressobj:\n                        self._decompressobj = ZLibDecompressor(\n                            suppress_deflate_header=True\n                        )\n                    payload_merged = self._decompressobj.decompress_sync(\n                        assembled_payload + WS_DEFLATE_TRAILING, self._max_msg_size\n                    )\n                    if self._decompressobj.unconsumed_tail:\n                        left = len(self._decompressobj.unconsumed_tail)\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Decompressed message size {} exceeds limit {}\".format(\n                                self._max_msg_size + left, self._max_msg_size\n                            ),\n                        )\n                elif type(assembled_payload) is bytes:\n                    payload_merged = assembled_payload\n                else:\n                    payload_merged = bytes(assembled_payload)\n\n                size = len(payload_merged)\n                if opcode == OP_CODE_TEXT:\n                    try:\n                        text = payload_merged.decode(\"utf-8\")\n                    except UnicodeDecodeError as exc:\n                        raise WebSocketError(\n                            WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                        ) from exc\n\n                    # XXX: The Text and Binary messages here can be a performance\n                    # bottleneck, so we use tuple.__new__ to improve performance.\n                    # This is not type safe, but many tests should fail in\n                    # test_client_ws_functional.py if this is wrong.\n                    msg = TUPLE_NEW(WSMessageText, (text, size, \"\", WS_MSG_TYPE_TEXT))\n                else:\n                    msg = TUPLE_NEW(\n                        WSMessageBinary, (payload_merged, size, \"\", WS_MSG_TYPE_BINARY)\n                    )\n\n                self.queue.feed_data(msg)\n            elif opcode == OP_CODE_CLOSE:\n                payload_len = len(payload)\n                if payload_len >= 2:\n                    close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\n                    if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            f\"Invalid close code: {close_code}\",\n                        )\n                    try:\n                        close_message = payload[2:].decode(\"utf-8\")\n                    except UnicodeDecodeError as exc:\n                        raise WebSocketError(\n                            WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                        ) from exc\n                    msg = WSMessageClose(\n                        data=close_code, size=payload_len, extra=close_message\n                    )\n                elif payload:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        f\"Invalid close frame: {fin} {opcode} {payload!r}\",\n                    )\n                else:\n                    msg = WSMessageClose(data=0, size=payload_len, extra=\"\")\n\n                self.queue.feed_data(msg)\n            elif opcode == OP_CODE_PING:\n                self.queue.feed_data(\n                    WSMessagePing(data=payload, size=len(payload), extra=\"\")\n                )\n            elif opcode == OP_CODE_PONG:\n                self.queue.feed_data(\n                    WSMessagePong(data=payload, size=len(payload), extra=\"\")\n                )\n            else:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR, f\"Unexpected opcode={opcode!r}\"\n                )\n\n    def parse_frame(\n        self, buf: bytes\n    ) -> List[Tuple[bool, Optional[int], Union[bytes, bytearray], Optional[bool]]]:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        frames: List[\n            Tuple[bool, Optional[int], Union[bytes, bytearray], Optional[bool]]\n        ] = []\n        if self._tail:\n            buf, self._tail = self._tail + buf, b\"\"\n\n        start_pos: int = 0\n        buf_length = len(buf)\n\n        while True:\n            # read header\n            if self._state == READ_HEADER:\n                if buf_length - start_pos < 2:\n                    break\n                first_byte = buf[start_pos]\n                second_byte = buf[start_pos + 1]\n                start_pos += 2\n\n                fin = (first_byte >> 7) & 1\n                rsv1 = (first_byte >> 6) & 1\n                rsv2 = (first_byte >> 5) & 1\n                rsv3 = (first_byte >> 4) & 1\n                opcode = first_byte & 0xF\n\n                # frame-fin = %x0 ; more frames of this message follow\n                #           / %x1 ; final frame of this message\n                # frame-rsv1 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv2 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv3 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                #\n                # Remove rsv1 from this test for deflate development\n                if rsv2 or rsv3 or (rsv1 and not self._compress):\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                if opcode > 0x7 and fin == 0:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received fragmented control frame\",\n                    )\n\n                has_mask = (second_byte >> 7) & 1\n                length = second_byte & 0x7F\n\n                # Control frames MUST have a payload\n                # length of 125 bytes or less\n                if opcode > 0x7 and length > 125:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Control frame payload cannot be larger than 125 bytes\",\n                    )\n\n                # Set compress status if last package is FIN\n                # OR set compress status if this is first fragment\n                # Raise error if not first fragment with rsv1 = 0x1\n                if self._frame_fin or self._compressed is None:\n                    self._compressed = True if rsv1 else False\n                elif rsv1:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                self._frame_fin = bool(fin)\n                self._frame_opcode = opcode\n                self._has_mask = bool(has_mask)\n                self._payload_length_flag = length\n                self._state = READ_PAYLOAD_LENGTH\n\n            # read payload length\n            if self._state == READ_PAYLOAD_LENGTH:\n                length_flag = self._payload_length_flag\n                if length_flag == 126:\n                    if buf_length - start_pos < 2:\n                        break\n                    first_byte = buf[start_pos]\n                    second_byte = buf[start_pos + 1]\n                    start_pos += 2\n                    self._payload_length = first_byte << 8 | second_byte\n                elif length_flag > 126:\n                    if buf_length - start_pos < 8:\n                        break\n                    data = buf[start_pos : start_pos + 8]\n                    start_pos += 8\n                    self._payload_length = UNPACK_LEN3(data)[0]\n                else:\n                    self._payload_length = length_flag\n\n                self._state = READ_PAYLOAD_MASK if self._has_mask else READ_PAYLOAD\n\n            # read payload mask\n            if self._state == READ_PAYLOAD_MASK:\n                if buf_length - start_pos < 4:\n                    break\n                self._frame_mask = buf[start_pos : start_pos + 4]\n                start_pos += 4\n                self._state = READ_PAYLOAD\n\n            if self._state == READ_PAYLOAD:\n                chunk_len = buf_length - start_pos\n                if self._payload_length >= chunk_len:\n                    end_pos = buf_length\n                    self._payload_length -= chunk_len\n                else:\n                    end_pos = start_pos + self._payload_length\n                    self._payload_length = 0\n\n                if self._frame_payload_len:\n                    if type(self._frame_payload) is not bytearray:\n                        self._frame_payload = bytearray(self._frame_payload)\n                    self._frame_payload += buf[start_pos:end_pos]\n                else:\n                    # Fast path for the first frame\n                    self._frame_payload = buf[start_pos:end_pos]\n\n                self._frame_payload_len += end_pos - start_pos\n                start_pos = end_pos\n\n                if self._payload_length != 0:\n                    break\n\n                if self._has_mask:\n                    assert self._frame_mask is not None\n                    if type(self._frame_payload) is not bytearray:\n                        self._frame_payload = bytearray(self._frame_payload)\n                    websocket_mask(self._frame_mask, self._frame_payload)\n\n                frames.append(\n                    (\n                        self._frame_fin,\n                        self._frame_opcode,\n                        self._frame_payload,\n                        self._compressed,\n                    )\n                )\n                self._frame_payload = b\"\"\n                self._frame_payload_len = 0\n                self._state = READ_HEADER\n\n        self._tail = buf[start_pos:] if start_pos < buf_length else b\"\"\n\n        return frames\n```"
    },
    {
      "chunk_id": 531,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_py.py",
      "content": "```python"
    },
    {
      "chunk_id": 532,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_py.py",
      "content": "\"\"\"Reader for WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport builtins\nfrom collections import deque\nfrom typing import Deque, Final, List, Optional, Set, Tuple, Type, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..compression_utils import ZLibDecompressor\nfrom ..helpers import _EXC_SENTINEL, set_exception\nfrom ..streams import EofStream\nfrom .helpers import UNPACK_CLOSE_CODE, UNPACK_LEN3, websocket_mask\nfrom .models import (\n    WS_DEFLATE_TRAILING,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMessageBinary,\n    WSMessageClose,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageText,\n    WSMsgType,\n)\n\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\n\n# States for the reader, used to parse the WebSocket frame\n# integer values are used so they can be cythonized\nREAD_HEADER = 1\nREAD_PAYLOAD_LENGTH = 2\nREAD_PAYLOAD_MASK = 3\nREAD_PAYLOAD = 4\n\nWS_MSG_TYPE_BINARY = WSMsgType.BINARY\nWS_MSG_TYPE_TEXT = WSMsgType.TEXT\n\n# WSMsgType values unpacked so they can by cythonized to ints\nOP_CODE_CONTINUATION = WSMsgType.CONTINUATION.value\nOP_CODE_TEXT = WSMsgType.TEXT.value\nOP_CODE_BINARY = WSMsgType.BINARY.value\nOP_CODE_CLOSE = WSMsgType.CLOSE.value\nOP_CODE_PING = WSMsgType.PING.value\nOP_CODE_PONG = WSMsgType.PONG.value\n\nEMPTY_FRAME_ERROR = (True, b\"\")\nEMPTY_FRAME = (False, b\"\")\n\nTUPLE_NEW = tuple.__new__"
    },
    {
      "chunk_id": 533,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_py.py",
      "content": "class WebSocketDataQueue:\n    \"\"\"WebSocketDataQueue resumes and pauses an underlying stream.\n\n    It is a destination for WebSocket data.\n    \"\"\"\n\n    def __init__(\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self._size = 0\n        self._protocol = protocol\n        self._limit = limit * 2\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Union[Type[BaseException], BaseException, None] = None\n        self._buffer: Deque[WSMessage] = deque()\n        self._get_buffer = self._buffer.popleft\n        self._put_buffer = self._buffer.append\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: builtins.BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def _release_waiter(self) -> None:\n        if (waiter := self._waiter) is None:\n            return\n        self._waiter = None\n        if not waiter.done():\n            waiter.set_result(None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n        self._release_waiter()\n\n    def feed_data(self, data: \"WSMessage\") -> None:\n        size = data.size\n        self._size += size\n        self._put_buffer(data)\n        self._release_waiter()\n        if self._size > self._limit and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    async def read(self) -> WSMessage:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n        return self._read_from_buffer()\n\n    def _read_from_buffer(self) -> WSMessage:\n        if self._buffer:\n            data = self._get_buffer()\n            size = data.size\n            self._size -= size\n            if self._size < self._limit and self._protocol._reading_paused:\n                self._protocol.resume_reading()\n            return data\n        if self._exception is not None:\n            raise self._exception\n        raise EofStream"
    },
    {
      "chunk_id": 534,
      "source": "__internal__/data_repo/aiohttp/aiohttp/_websocket/reader_py.py",
      "content": "class WebSocketReader:\n    def __init__(\n        self, queue: WebSocketDataQueue, max_msg_size: int, compress: bool = True\n    ) -> None:\n        self.queue = queue\n        self._max_msg_size = max_msg_size\n\n        self._exc: Optional[Exception] = None\n        self._partial = bytearray()\n        self._state = READ_HEADER\n\n        self._opcode: Optional[int] = None\n        self._frame_fin = False\n        self._frame_opcode: Optional[int] = None\n        self._frame_payload: Union[bytes, bytearray] = b\"\"\n        self._frame_payload_len = 0\n\n        self._tail: bytes = b\"\"\n        self._has_mask = False\n        self._frame_mask: Optional[bytes] = None\n        self._payload_length = 0\n        self._payload_length_flag = 0\n        self._compressed: Optional[bool] = None\n        self._decompressobj: Optional[ZLibDecompressor] = None\n        self._compress = compress\n\n    def feed_eof(self) -> None:\n        self.queue.feed_eof()\n\n    # data can be bytearray on Windows because proactor event loop uses bytearray\n    # and asyncio types this to Union[bytes, bytearray, memoryview] so we need\n    # coerce data to bytes if it is not\n    def feed_data(\n        self, data: Union[bytes, bytearray, memoryview]\n    ) -> Tuple[bool, bytes]:\n        if type(data) is not bytes:\n            data = bytes(data)\n\n        if self._exc is not None:\n            return True, data\n\n        try:\n            self._feed_data(data)\n        except Exception as exc:\n            self._exc = exc\n            set_exception(self.queue, exc)\n            return EMPTY_FRAME_ERROR\n\n        return EMPTY_FRAME\n\n    def _feed_data(self, data: bytes) -> None:\n        msg: WSMessage\n        for frame in self.parse_frame(data):\n            fin = frame[0]\n            opcode = frame[1]\n            payload = frame[2]\n            compressed = frame[3]\n\n            is_continuation = opcode == OP_CODE_CONTINUATION\n            if opcode == OP_CODE_TEXT or opcode == OP_CODE_BINARY or is_continuation:\n                # load text/binary\n                if not fin:\n                    # got partial frame payload\n                    if not is_continuation:\n                        self._opcode = opcode\n                    self._partial += payload\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Message size {} exceeds limit {}\".format(\n                                len(self._partial), self._max_msg_size\n                            ),\n                        )\n                    continue\n\n                has_partial = bool(self._partial)\n                if is_continuation:\n                    if self._opcode is None:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            \"Continuation frame for non started message\",\n                        )\n                    opcode = self._opcode\n                    self._opcode = None\n                # previous frame was non finished\n                # we should get continuation opcode\n                elif has_partial:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"The opcode in non-fin frame is expected \"\n                        \"to be zero, got {!r}\".format(opcode),\n                    )\n\n                assembled_payload: Union[bytes, bytearray]\n                if has_partial:\n                    assembled_payload = self._partial + payload\n                    self._partial.clear()\n                else:\n                    assembled_payload = payload\n\n                if self._max_msg_size and len(assembled_payload) >= self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        \"Message size {} exceeds limit {}\".format(\n                            len(assembled_payload), self._max_msg_size\n                        ),\n                    )\n\n                # Decompress process must to be done after all packets\n                # received.\n                if compressed:\n                    if not self._decompressobj:\n                        self._decompressobj = ZLibDecompressor(\n                            suppress_deflate_header=True\n                        )\n                    payload_merged = self._decompressobj.decompress_sync(\n                        assembled_payload + WS_DEFLATE_TRAILING, self._max_msg_size\n                    )\n                    if self._decompressobj.unconsumed_tail:\n                        left = len(self._decompressobj.unconsumed_tail)\n                        raise WebSocketError(\n                            WSCloseCode.MESSAGE_TOO_BIG,\n                            \"Decompressed message size {} exceeds limit {}\".format(\n                                self._max_msg_size + left, self._max_msg_size\n                            ),\n                        )\n                elif type(assembled_payload) is bytes:\n                    payload_merged = assembled_payload\n                else:\n                    payload_merged = bytes(assembled_payload)\n\n                size = len(payload_merged)\n                if opcode == OP_CODE_TEXT:\n                    try:\n                        text = payload_merged.decode(\"utf-8\")\n                    except UnicodeDecodeError as exc:\n                        raise WebSocketError(\n                            WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                        ) from exc\n\n                    # XXX: The Text and Binary messages here can be a performance\n                    # bottleneck, so we use tuple.__new__ to improve performance.\n                    # This is not type safe, but many tests should fail in\n                    # test_client_ws_functional.py if this is wrong.\n                    msg = TUPLE_NEW(WSMessageText, (text, size, \"\", WS_MSG_TYPE_TEXT))\n                else:\n                    msg = TUPLE_NEW(\n                        WSMessageBinary, (payload_merged, size, \"\", WS_MSG_TYPE_BINARY)\n                    )\n\n                self.queue.feed_data(msg)\n            elif opcode == OP_CODE_CLOSE:\n                payload_len = len(payload)\n                if payload_len >= 2:\n                    close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\n                    if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\n                        raise WebSocketError(\n                            WSCloseCode.PROTOCOL_ERROR,\n                            f\"Invalid close code: {close_code}\",\n                        )\n                    try:\n                        close_message = payload[2:].decode(\"utf-8\")\n                    except UnicodeDecodeError as exc:\n                        raise WebSocketError(\n                            WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                        ) from exc\n                    msg = WSMessageClose(\n                        data=close_code, size=payload_len, extra=close_message\n                    )\n                elif payload:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        f\"Invalid close frame: {fin} {opcode} {payload!r}\",\n                    )\n                else:\n                    msg = WSMessageClose(data=0, size=payload_len, extra=\"\")\n\n                self.queue.feed_data(msg)\n            elif opcode == OP_CODE_PING:\n                self.queue.feed_data(\n                    WSMessagePing(data=payload, size=len(payload), extra=\"\")\n                )\n            elif opcode == OP_CODE_PONG:\n                self.queue.feed_data(\n                    WSMessagePong(data=payload, size=len(payload), extra=\"\")\n                )\n            else:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR, f\"Unexpected opcode={opcode!r}\"\n                )\n\n    def parse_frame(\n        self, buf: bytes\n    ) -> List[Tuple[bool, Optional[int], Union[bytes, bytearray], Optional[bool]]]:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        frames: List[\n            Tuple[bool, Optional[int], Union[bytes, bytearray], Optional[bool]]\n        ] = []\n        if self._tail:\n            buf, self._tail = self._tail + buf, b\"\"\n\n        start_pos: int = 0\n        buf_length = len(buf)\n\n        while True:\n            # read header\n            if self._state == READ_HEADER:\n                if buf_length - start_pos < 2:\n                    break\n                first_byte = buf[start_pos]\n                second_byte = buf[start_pos + 1]\n                start_pos += 2\n\n                fin = (first_byte >> 7) & 1\n                rsv1 = (first_byte >> 6) & 1\n                rsv2 = (first_byte >> 5) & 1\n                rsv3 = (first_byte >> 4) & 1\n                opcode = first_byte & 0xF\n\n                # frame-fin = %x0 ; more frames of this message follow\n                #           / %x1 ; final frame of this message\n                # frame-rsv1 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv2 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv3 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                #\n                # Remove rsv1 from this test for deflate development\n                if rsv2 or rsv3 or (rsv1 and not self._compress):\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                if opcode > 0x7 and fin == 0:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received fragmented control frame\",\n                    )\n\n                has_mask = (second_byte >> 7) & 1\n                length = second_byte & 0x7F\n\n                # Control frames MUST have a payload\n                # length of 125 bytes or less\n                if opcode > 0x7 and length > 125:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Control frame payload cannot be larger than 125 bytes\",\n                    )\n\n                # Set compress status if last package is FIN\n                # OR set compress status if this is first fragment\n                # Raise error if not first fragment with rsv1 = 0x1\n                if self._frame_fin or self._compressed is None:\n                    self._compressed = True if rsv1 else False\n                elif rsv1:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                self._frame_fin = bool(fin)\n                self._frame_opcode = opcode\n                self._has_mask = bool(has_mask)\n                self._payload_length_flag = length\n                self._state = READ_PAYLOAD_LENGTH\n\n            # read payload length\n            if self._state == READ_PAYLOAD_LENGTH:\n                length_flag = self._payload_length_flag\n                if length_flag == 126:\n                    if buf_length - start_pos < 2:\n                        break\n                    first_byte = buf[start_pos]\n                    second_byte = buf[start_pos + 1]\n                    start_pos += 2\n                    self._payload_length = first_byte << 8 | second_byte\n                elif length_flag > 126:\n                    if buf_length - start_pos < 8:\n                        break\n                    data = buf[start_pos : start_pos + 8]\n                    start_pos += 8\n                    self._payload_length = UNPACK_LEN3(data)[0]\n                else:\n                    self._payload_length = length_flag\n\n                self._state = READ_PAYLOAD_MASK if self._has_mask else READ_PAYLOAD\n\n            # read payload mask\n            if self._state == READ_PAYLOAD_MASK:\n                if buf_length - start_pos < 4:\n                    break\n                self._frame_mask = buf[start_pos : start_pos + 4]\n                start_pos += 4\n                self._state = READ_PAYLOAD\n\n            if self._state == READ_PAYLOAD:\n                chunk_len = buf_length - start_pos\n                if self._payload_length >= chunk_len:\n                    end_pos = buf_length\n                    self._payload_length -= chunk_len\n                else:\n                    end_pos = start_pos + self._payload_length\n                    self._payload_length = 0\n\n                if self._frame_payload_len:\n                    if type(self._frame_payload) is not bytearray:\n                        self._frame_payload = bytearray(self._frame_payload)\n                    self._frame_payload += buf[start_pos:end_pos]\n                else:\n                    # Fast path for the first frame\n                    self._frame_payload = buf[start_pos:end_pos]\n\n                self._frame_payload_len += end_pos - start_pos\n                start_pos = end_pos\n\n                if self._payload_length != 0:\n                    break\n\n                if self._has_mask:\n                    assert self._frame_mask is not None\n                    if type(self._frame_payload) is not bytearray:\n                        self._frame_payload = bytearray(self._frame_payload)\n                    websocket_mask(self._frame_mask, self._frame_payload)\n\n                frames.append(\n                    (\n                        self._frame_fin,\n                        self._frame_opcode,\n                        self._frame_payload,\n                        self._compressed,\n                    )\n                )\n                self._frame_payload = b\"\"\n                self._frame_payload_len = 0\n                self._state = READ_HEADER\n\n        self._tail = buf[start_pos:] if start_pos < buf_length else b\"\"\n\n        return frames\n```"
    },
    {
      "chunk_id": 535,
      "source": "__internal__/data_repo/aiohttp/examples/web_ws.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web websocket server.\"\"\"\n\n# The extra strict mypy settings are here to help test that `Application[AppKey()]`\n# syntax is working correctly. A regression will cause mypy to raise an error.\n# mypy: disallow-any-expr, disallow-any-unimported, disallow-subclassing-any\n\nimport os\nfrom typing import List, Union\n\nfrom aiohttp import web\n\nWS_FILE = os.path.join(os.path.dirname(__file__), \"websocket.html\")\nsockets = web.AppKey(\"sockets\", List[web.WebSocketResponse])"
    },
    {
      "chunk_id": 536,
      "source": "__internal__/data_repo/aiohttp/examples/web_ws.py",
      "content": "async def wshandler(request: web.Request) -> Union[web.WebSocketResponse, web.Response]:\n    resp = web.WebSocketResponse()\n    available = resp.can_prepare(request)\n    if not available:\n        with open(WS_FILE, \"rb\") as fp:\n            return web.Response(body=fp.read(), content_type=\"text/html\")\n\n    await resp.prepare(request)\n\n    await resp.send_str(\"Welcome!!!\")\n\n    try:\n        print(\"Someone joined.\")\n        for ws in request.app[sockets]:\n            await ws.send_str(\"Someone joined\")\n        request.app[sockets].append(resp)\n\n        async for msg in resp:\n            if msg.type is web.WSMsgType.TEXT:\n                for ws in request.app[sockets]:\n                    if ws is not resp:\n                        await ws.send_str(msg.data)\n            else:\n                return resp\n        return resp\n\n    finally:\n        request.app[sockets].remove(resp)\n        print(\"Someone disconnected.\")\n        for ws in request.app[sockets]:\n            await ws.send_str(\"Someone disconnected.\")"
    },
    {
      "chunk_id": 537,
      "source": "__internal__/data_repo/aiohttp/examples/web_ws.py",
      "content": "async def on_shutdown(app: web.Application) -> None:\n    for ws in app[sockets]:\n        await ws.close()"
    },
    {
      "chunk_id": 538,
      "source": "__internal__/data_repo/aiohttp/examples/web_ws.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    l: List[web.WebSocketResponse] = []\n    app[sockets] = l\n    app.router.add_get(\"/\", wshandler)\n    app.on_shutdown.append(on_shutdown)\n    return app"
    },
    {
      "chunk_id": 539,
      "source": "__internal__/data_repo/aiohttp/examples/web_ws.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 540,
      "source": "__internal__/data_repo/aiohttp/examples/server_simple.py",
      "content": "from aiohttp import web"
    },
    {
      "chunk_id": 541,
      "source": "__internal__/data_repo/aiohttp/examples/server_simple.py",
      "content": "async def handle(request: web.Request) -> web.StreamResponse:\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    text = \"Hello, \" + name\n    return web.Response(text=text)"
    },
    {
      "chunk_id": 542,
      "source": "__internal__/data_repo/aiohttp/examples/server_simple.py",
      "content": "async def wshandle(request: web.Request) -> web.StreamResponse:\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n\n    async for msg in ws:\n        if msg.type is web.WSMsgType.TEXT:\n            await ws.send_str(f\"Hello, {msg.data}\")\n        elif msg.type is web.WSMsgType.BINARY:\n            await ws.send_bytes(msg.data)\n        elif msg.type is web.WSMsgType.CLOSE:\n            break\n\n    return ws"
    },
    {
      "chunk_id": 543,
      "source": "__internal__/data_repo/aiohttp/examples/server_simple.py",
      "content": "app = web.Application()\napp.add_routes(\n    [web.get(\"/\", handle), web.get(\"/echo\", wshandle), web.get(\"/{name}\", handle)]\n)"
    },
    {
      "chunk_id": 544,
      "source": "__internal__/data_repo/aiohttp/examples/server_simple.py",
      "content": "web.run_app(app)"
    },
    {
      "chunk_id": 545,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with decorator definition for routes.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 546,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "@routes.get(\"/\")\nasync def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp"
    },
    {
      "chunk_id": 547,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "@routes.get(\"/simple\")\nasync def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")"
    },
    {
      "chunk_id": 548,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "@routes.get(\"/change_body\")\nasync def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp"
    },
    {
      "chunk_id": 549,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "@routes.get(\"/hello\")\nasync def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp"
    },
    {
      "chunk_id": 550,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    app.router.add_routes(routes)\n    return app"
    },
    {
      "chunk_id": 551,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_deco.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 552,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 553,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "async def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp"
    },
    {
      "chunk_id": 554,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "async def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")"
    },
    {
      "chunk_id": 555,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "async def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp"
    },
    {
      "chunk_id": 556,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "async def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp"
    },
    {
      "chunk_id": 557,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", intro)\n    app.router.add_get(\"/simple\", simple)\n    app.router.add_get(\"/change_body\", change_body)\n    app.router.add_get(\"/hello/{name}\", hello)\n    app.router.add_get(\"/hello\", hello)\n    return app"
    },
    {
      "chunk_id": 558,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 559,
      "source": "__internal__/data_repo/aiohttp/examples/web_rewrite_headers_middleware.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for rewriting response headers by middleware.\"\"\"\n\nfrom aiohttp import web\nfrom aiohttp.typedefs import Handler"
    },
    {
      "chunk_id": 560,
      "source": "__internal__/data_repo/aiohttp/examples/web_rewrite_headers_middleware.py",
      "content": "async def handler(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Everything is fine\")"
    },
    {
      "chunk_id": 561,
      "source": "__internal__/data_repo/aiohttp/examples/web_rewrite_headers_middleware.py",
      "content": "async def middleware(request: web.Request, handler: Handler) -> web.StreamResponse:\n    try:\n        response = await handler(request)\n    except web.HTTPException as exc:\n        raise exc\n    if not response.prepared:\n        response.headers[\"SERVER\"] = \"Secured Server Software\"\n    return response"
    },
    {
      "chunk_id": 562,
      "source": "__internal__/data_repo/aiohttp/examples/web_rewrite_headers_middleware.py",
      "content": "def init() -> web.Application:\n    app = web.Application(middlewares=[middleware])\n    app.router.add_get(\"/\", handler)\n    return app"
    },
    {
      "chunk_id": 563,
      "source": "__internal__/data_repo/aiohttp/examples/web_rewrite_headers_middleware.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 564,
      "source": "__internal__/data_repo/aiohttp/examples/web_classview.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web class based views.\"\"\"\n\nimport functools\nimport json\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 565,
      "source": "__internal__/data_repo/aiohttp/examples/web_classview.py",
      "content": "class MyView(web.View):\n    async def get(self) -> web.StreamResponse:\n        return web.json_response(\n            {\n                \"method\": self.request.method,\n                \"args\": dict(self.request.rel_url.query),\n                \"headers\": dict(self.request.headers),\n            },\n            dumps=functools.partial(json.dumps, indent=4),\n        )\n\n    async def post(self) -> web.StreamResponse:\n        data = await self.request.post()\n        return web.json_response(\n            {\n                \"method\": self.request.method,\n                \"data\": dict(data),\n                \"headers\": dict(self.request.headers),\n            },\n            dumps=functools.partial(json.dumps, indent=4),\n        )"
    },
    {
      "chunk_id": 566,
      "source": "__internal__/data_repo/aiohttp/examples/web_classview.py",
      "content": "async def index(request: web.Request) -> web.StreamResponse:\n    txt = \"\"\"\n      <html>\n        <head>\n          <title>Class based view example</title>\n        </head>\n        <body>\n          <h1>Class based view example</h1>\n          <ul>\n            <li><a href=\"/\">/</a> This page\n            <li><a href=\"/get\">/get</a> Returns GET data.\n            <li><a href=\"/post\">/post</a> Returns POST data.\n          </ul>\n        </body>\n      </html>\n    \"\"\"\n    return web.Response(text=txt, content_type=\"text/html\")"
    },
    {
      "chunk_id": 567,
      "source": "__internal__/data_repo/aiohttp/examples/web_classview.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", index)\n    app.router.add_get(\"/get\", MyView)\n    app.router.add_post(\"/post\", MyView)\n    return app"
    },
    {
      "chunk_id": 568,
      "source": "__internal__/data_repo/aiohttp/examples/web_classview.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 569,
      "source": "__internal__/data_repo/aiohttp/examples/client_ws.py",
      "content": "#!/usr/bin/env python3\n\"\"\"websocket cmd client for web_ws.py example.\"\"\"\n\nimport argparse\nimport asyncio\nimport sys\nfrom contextlib import suppress\n\nimport aiohttp"
    },
    {
      "chunk_id": 570,
      "source": "__internal__/data_repo/aiohttp/examples/client_ws.py",
      "content": "async def start_client(url: str) -> None:\n    name = input(\"Please enter your name: \")\n\n    async def dispatch(ws: aiohttp.ClientWebSocketResponse) -> None:\n        while True:\n            msg = await ws.receive()\n\n            if msg.type is aiohttp.WSMsgType.TEXT:\n                print(\"Text: \", msg.data.strip())\n            elif msg.type is aiohttp.WSMsgType.BINARY:\n                print(\"Binary: \", msg.data)\n            elif msg.type is aiohttp.WSMsgType.PING:\n                await ws.pong()\n            elif msg.type is aiohttp.WSMsgType.PONG:\n                print(\"Pong received\")\n            else:\n                if msg.type is aiohttp.WSMsgType.CLOSE:\n                    await ws.close()\n                elif msg.type is aiohttp.WSMsgType.ERROR:\n                    print(\"Error during receive %s\" % ws.exception())\n                elif msg.type is aiohttp.WSMsgType.CLOSED:\n                    pass\n\n                break\n\n    async with aiohttp.ClientSession() as session:\n        async with session.ws_connect(url, autoclose=False, autoping=False) as ws:\n            # send request\n            dispatch_task = asyncio.create_task(dispatch(ws))\n\n            # Exit with Ctrl+D\n            while line := await asyncio.to_thread(sys.stdin.readline):\n                await ws.send_str(name + \": \" + line)\n\n            dispatch_task.cancel()\n            with suppress(asyncio.CancelledError):\n                await dispatch_task"
    },
    {
      "chunk_id": 571,
      "source": "__internal__/data_repo/aiohttp/examples/client_ws.py",
      "content": "ARGS = argparse.ArgumentParser(\n    description=\"websocket console client for wssrv.py example.\"\n)\nARGS.add_argument(\n    \"--host\", action=\"store\", dest=\"host\", default=\"127.0.0.1\", help=\"Host name\"\n)\nARGS.add_argument(\n    \"--port\", action=\"store\", dest=\"port\", default=8080, type=int, help=\"Port number\"\n)"
    },
    {
      "chunk_id": 572,
      "source": "__internal__/data_repo/aiohttp/examples/client_ws.py",
      "content": "if __name__ == \"__main__\":\n    args = ARGS.parse_args()\n    if \":\" in args.host:\n        args.host, port = args.host.split(\":\", 1)\n        args.port = int(port)\n\n    url = f\"http://{args.host}:{args.port}\"\n\n    asyncio.run(start_client(url))"
    },
    {
      "chunk_id": 573,
      "source": "__internal__/data_repo/aiohttp/examples/static_files.py",
      "content": "#!/usr/bin/env python3\nimport pathlib\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 574,
      "source": "__internal__/data_repo/aiohttp/examples/static_files.py",
      "content": "app = web.Application()\napp.router.add_static(\"/\", pathlib.Path(__file__).parent, show_index=True)"
    },
    {
      "chunk_id": 575,
      "source": "__internal__/data_repo/aiohttp/examples/static_files.py",
      "content": "web.run_app(app)"
    },
    {
      "chunk_id": 576,
      "source": "__internal__/data_repo/aiohttp/examples/client_json.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\n\nimport aiohttp"
    },
    {
      "chunk_id": 577,
      "source": "__internal__/data_repo/aiohttp/examples/client_json.py",
      "content": "async def fetch(session: aiohttp.ClientSession) -> None:\n    print(\"Query http://httpbin.org/get\")\n    async with session.get(\"http://httpbin.org/get\") as resp:\n        print(resp.status)\n        data = await resp.json()\n        print(data)"
    },
    {
      "chunk_id": 578,
      "source": "__internal__/data_repo/aiohttp/examples/client_json.py",
      "content": "async def go() -> None:\n    async with aiohttp.ClientSession() as session:\n        await fetch(session)"
    },
    {
      "chunk_id": 579,
      "source": "__internal__/data_repo/aiohttp/examples/client_json.py",
      "content": "loop = asyncio.get_event_loop()\nloop.run_until_complete(go())\nloop.close()"
    },
    {
      "chunk_id": 580,
      "source": "__internal__/data_repo/aiohttp/examples/curl.py",
      "content": "#!/usr/bin/env python3"
    },
    {
      "chunk_id": 581,
      "source": "__internal__/data_repo/aiohttp/examples/curl.py",
      "content": "import argparse\nimport asyncio\nimport sys\n\nimport aiohttp"
    },
    {
      "chunk_id": 582,
      "source": "__internal__/data_repo/aiohttp/examples/curl.py",
      "content": "async def curl(url: str) -> None:\n    async with aiohttp.ClientSession() as session:\n        async with session.request(\"GET\", url) as response:\n            print(repr(response))\n            chunk = await response.content.read()\n            print(\"Downloaded: %s\" % len(chunk))"
    },
    {
      "chunk_id": 583,
      "source": "__internal__/data_repo/aiohttp/examples/curl.py",
      "content": "if __name__ == \"__main__\":\n    ARGS = argparse.ArgumentParser(description=\"GET url example\")\n    ARGS.add_argument(\"url\", nargs=1, metavar=\"URL\", help=\"URL to download\")\n    ARGS.add_argument(\n        \"--iocp\",\n        default=False,\n        action=\"store_true\",\n        help=\"Use ProactorEventLoop on Windows\",\n    )\n    options = ARGS.parse_args()\n\n    if options.iocp and sys.platform == \"win32\":\n        from asyncio import events, windows_events\n\n        # https://github.com/python/mypy/issues/12286\n        el = windows_events.ProactorEventLoop()  # type: ignore[attr-defined]\n        events.set_event_loop(el)\n\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(curl(options.url[0]))"
    },
    {
      "chunk_id": 584,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example of aiohttp.web.Application.on_startup signal handler\"\"\"\nimport asyncio\nfrom contextlib import suppress\nfrom typing import AsyncIterator, List\n\nimport valkey.asyncio as valkey\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 585,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "async def websocket_handler(request: web.Request) -> web.StreamResponse:\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n    request.app[websockets].append(ws)\n    try:\n        async for msg in ws:\n            print(msg)\n            await asyncio.sleep(1)\n    finally:\n        request.app[websockets].remove(ws)\n    return ws"
    },
    {
      "chunk_id": 586,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "async def on_shutdown(app: web.Application) -> None:\n    for ws in app[websockets]:\n        await ws.close(code=999, message=b\"Server shutdown\")"
    },
    {
      "chunk_id": 587,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "async def listen_to_valkey(app: web.Application) -> None:\n    r = valkey.Valkey(host=\"localhost\", port=6379, decode_responses=True)\n    channel = \"news\"\n    async with r.pubsub() as sub:\n        await sub.subscribe(channel)\n        async for msg in sub.listen():\n            if msg[\"type\"] != \"message\":\n                continue\n            # Forward message to all connected websockets:\n            for ws in app[websockets]:\n                await ws.send_str(f\"{channel}: {msg}\")\n            print(f\"message in {channel}: {msg}\")"
    },
    {
      "chunk_id": 588,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "async def background_tasks(app: web.Application) -> AsyncIterator[None]:\n    app[valkey_listener] = asyncio.create_task(listen_to_valkey(app))\n\n    yield\n\n    print(\"cleanup background tasks...\")\n    app[valkey_listener].cancel()\n    with suppress(asyncio.CancelledError):\n        await app[valkey_listener]"
    },
    {
      "chunk_id": 589,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    l: List[web.WebSocketResponse] = []\n    app[websockets] = l\n    app.router.add_get(\"/news\", websocket_handler)\n    app.cleanup_ctx.append(background_tasks)\n    app.on_shutdown.append(on_shutdown)\n    return app"
    },
    {
      "chunk_id": 590,
      "source": "__internal__/data_repo/aiohttp/examples/background_tasks.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 591,
      "source": "__internal__/data_repo/aiohttp/examples/client_auth.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\n\nimport aiohttp"
    },
    {
      "chunk_id": 592,
      "source": "__internal__/data_repo/aiohttp/examples/client_auth.py",
      "content": "async def fetch(session: aiohttp.ClientSession) -> None:\n    print(\"Query http://httpbin.org/basic-auth/andrew/password\")\n    async with session.get(\"http://httpbin.org/basic-auth/andrew/password\") as resp:\n        print(resp.status)\n        body = await resp.text()\n        print(body)"
    },
    {
      "chunk_id": 593,
      "source": "__internal__/data_repo/aiohttp/examples/client_auth.py",
      "content": "async def go() -> None:\n    async with aiohttp.ClientSession(\n        auth=aiohttp.BasicAuth(\"andrew\", \"password\")\n    ) as session:\n        await fetch(session)"
    },
    {
      "chunk_id": 594,
      "source": "__internal__/data_repo/aiohttp/examples/client_auth.py",
      "content": "loop = asyncio.get_event_loop()\nloop.run_until_complete(go())"
    },
    {
      "chunk_id": 595,
      "source": "__internal__/data_repo/aiohttp/examples/cli_app.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nExample of serving an Application using the `aiohttp.web` CLI.\n\nServe this app using::\n\n    $ python -m aiohttp.web -H localhost -P 8080 --repeat 10 cli_app:init \\\n    > \"Hello World\"\n\nHere ``--repeat`` & ``\"Hello World\"`` are application specific command-line\narguments. `aiohttp.web` only parses & consumes the command-line arguments it\nneeds (i.e. ``-H``, ``-P`` & ``entry-func``) and passes on any additional\narguments to the `cli_app:init` function for processing.\n\"\"\""
    },
    {
      "chunk_id": 596,
      "source": "__internal__/data_repo/aiohttp/examples/cli_app.py",
      "content": "from argparse import ArgumentParser, Namespace\nfrom typing import Optional, Sequence\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 597,
      "source": "__internal__/data_repo/aiohttp/examples/cli_app.py",
      "content": "args_key = web.AppKey(\"args_key\", Namespace)"
    },
    {
      "chunk_id": 598,
      "source": "__internal__/data_repo/aiohttp/examples/cli_app.py",
      "content": "async def display_message(req: web.Request) -> web.StreamResponse:\n    args = req.app[args_key]\n    text = \"\\n\".join([args.message] * args.repeat)\n    return web.Response(text=text)"
    },
    {
      "chunk_id": 599,
      "source": "__internal__/data_repo/aiohttp/examples/cli_app.py",
      "content": "def init(argv: Optional[Sequence[str]]) -> web.Application:\n    arg_parser = ArgumentParser(\n        prog=\"aiohttp.web ...\", description=\"Application CLI\", add_help=False\n    )\n\n    # Positional argument\n    arg_parser.add_argument(\"message\", help=\"message to print\")\n\n    # Optional argument\n    arg_parser.add_argument(\n        \"--repeat\", help=\"number of times to repeat message\", type=int, default=\"1\"\n    )\n\n    # Avoid conflict with -h from `aiohttp.web` CLI parser\n    arg_parser.add_argument(\n        \"--app-help\", help=\"show this message and exit\", action=\"help\"\n    )\n\n    args = arg_parser.parse_args(argv)\n\n    app = web.Application()\n    app[args_key] = args\n    app.router.add_get(\"/\", display_message)\n\n    return app"
    },
    {
      "chunk_id": 600,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with table definition for routes.\"\"\"\n\nimport textwrap\n\nfrom aiohttp import web"
    },
    {
      "chunk_id": 601,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "async def intro(request: web.Request) -> web.StreamResponse:\n    txt = textwrap.dedent(\n        \"\"\"\\\n        Type {url}/hello/John  {url}/simple or {url}/change_body\n        in browser url bar\n    \"\"\"\n    ).format(url=\"127.0.0.1:8080\")\n    binary = txt.encode(\"utf8\")\n    resp = web.StreamResponse()\n    resp.content_length = len(binary)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(binary)\n    return resp"
    },
    {
      "chunk_id": 602,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "async def simple(request: web.Request) -> web.StreamResponse:\n    return web.Response(text=\"Simple answer\")"
    },
    {
      "chunk_id": 603,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "async def change_body(request: web.Request) -> web.StreamResponse:\n    resp = web.Response()\n    resp.body = b\"Body changed\"\n    resp.content_type = \"text/plain\"\n    return resp"
    },
    {
      "chunk_id": 604,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "async def hello(request: web.Request) -> web.StreamResponse:\n    resp = web.StreamResponse()\n    name = request.match_info.get(\"name\", \"Anonymous\")\n    answer = (\"Hello, \" + name).encode(\"utf8\")\n    resp.content_length = len(answer)\n    resp.content_type = \"text/plain\"\n    await resp.prepare(request)\n    await resp.write(answer)\n    await resp.write_eof()\n    return resp"
    },
    {
      "chunk_id": 605,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    app.router.add_routes(\n        [\n            web.get(\"/\", intro),\n            web.get(\"/simple\", simple),\n            web.get(\"/change_body\", change_body),\n            web.get(\"/hello/{name}\", hello),\n            web.get(\"/hello\", hello),\n        ]\n    )\n    return app"
    },
    {
      "chunk_id": 606,
      "source": "__internal__/data_repo/aiohttp/examples/web_srv_route_table.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 607,
      "source": "__internal__/data_repo/aiohttp/examples/fake_server.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nimport pathlib\nimport socket\nimport ssl\nfrom typing import Dict, List\n\nfrom aiohttp import ClientSession, TCPConnector, test_utils, web\nfrom aiohttp.abc import AbstractResolver, ResolveResult\nfrom aiohttp.resolver import DefaultResolver"
    },
    {
      "chunk_id": 608,
      "source": "__internal__/data_repo/aiohttp/examples/fake_server.py",
      "content": "class FakeResolver(AbstractResolver):\n    _LOCAL_HOST = {0: \"127.0.0.1\", socket.AF_INET: \"127.0.0.1\", socket.AF_INET6: \"::1\"}\n\n    def __init__(self, fakes: Dict[str, int]) -> None:\n        \"\"\"fakes -- dns -> port dict\"\"\"\n        self._fakes = fakes\n        self._resolver = DefaultResolver()\n\n    async def resolve(\n        self,\n        host: str,\n        port: int = 0,\n        family: socket.AddressFamily = socket.AF_INET,\n    ) -> List[ResolveResult]:\n        fake_port = self._fakes.get(host)\n        if fake_port is not None:\n            return [\n                {\n                    \"hostname\": host,\n                    \"host\": self._LOCAL_HOST[family],\n                    \"port\": fake_port,\n                    \"family\": family,\n                    \"proto\": 0,\n                    \"flags\": socket.AI_NUMERICHOST,\n                }\n            ]\n        else:\n            return await self._resolver.resolve(host, port, family)\n\n    async def close(self) -> None:\n        await self._resolver.close()"
    },
    {
      "chunk_id": 609,
      "source": "__internal__/data_repo/aiohttp/examples/fake_server.py",
      "content": "class FakeFacebook:\n    def __init__(self) -> None:\n        self.app = web.Application()\n        self.app.router.add_routes(\n            [\n                web.get(\"/v2.7/me\", self.on_me),\n                web.get(\"/v2.7/me/friends\", self.on_my_friends),\n            ]\n        )\n        self.runner = web.AppRunner(self.app)\n        here = pathlib.Path(__file__)\n        ssl_cert = here.parent / \"server.crt\"\n        ssl_key = here.parent / \"server.key\"\n        self.ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        self.ssl_context.load_cert_chain(str(ssl_cert), str(ssl_key))\n\n    async def start(self) -> Dict[str, int]:\n        port = test_utils.unused_port()\n        await self.runner.setup()\n        site = web.TCPSite(self.runner, \"127.0.0.1\", port, ssl_context=self.ssl_context)\n        await site.start()\n        return {\"graph.facebook.com\": port}\n\n    async def stop(self) -> None:\n        await self.runner.cleanup()\n\n    async def on_me(self, request: web.Request) -> web.StreamResponse:\n        return web.json_response({\"name\": \"John Doe\", \"id\": \"12345678901234567\"})\n\n    async def on_my_friends(self, request: web.Request) -> web.StreamResponse:\n        return web.json_response(\n            {\n                \"data\": [\n                    {\"name\": \"Bill Doe\", \"id\": \"233242342342\"},\n                    {\"name\": \"Mary Doe\", \"id\": \"2342342343222\"},\n                    {\"name\": \"Alex Smith\", \"id\": \"234234234344\"},\n                ],\n                \"paging\": {\n                    \"cursors\": {\n                        \"before\": \"QVFIUjRtc2c5NEl0ajN\",\n                        \"after\": \"QVFIUlpFQWM0TmVuaDRad0dt\",\n                    },\n                    \"next\": (\n                        \"https://graph.facebook.com/v2.7/12345678901234567/\"\n                        \"friends?access_token=EAACEdEose0cB\"\n                    ),\n                },\n                \"summary\": {\"total_count\": 3},\n            }\n        )"
    },
    {
      "chunk_id": 610,
      "source": "__internal__/data_repo/aiohttp/examples/fake_server.py",
      "content": "async def main() -> None:\n    token = \"ER34gsSGGS34XCBKd7u\"\n\n    fake_facebook = FakeFacebook()\n    info = await fake_facebook.start()\n    resolver = FakeResolver(info)\n    connector = TCPConnector(resolver=resolver, ssl=False)\n\n    async with ClientSession(connector=connector) as session:\n        async with session.get(\n            \"https://graph.facebook.com/v2.7/me\", params={\"access_token\": token}\n        ) as resp:\n            print(await resp.json())\n\n        async with session.get(\n            \"https://graph.facebook.com/v2.7/me/friends\", params={\"access_token\": token}\n        ) as resp:\n            print(await resp.json())\n\n    await fake_facebook.stop()"
    },
    {
      "chunk_id": 611,
      "source": "__internal__/data_repo/aiohttp/examples/fake_server.py",
      "content": "loop = asyncio.get_event_loop()\nloop.run_until_complete(main())"
    },
    {
      "chunk_id": 612,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Example for aiohttp.web basic server with cookies.\"\"\"\n\nfrom pprint import pformat\nfrom typing import NoReturn\n\nfrom aiohttp import web\n\ntmpl = \"\"\"\\\n<html>\n    <body>\n        <a href=\"/login\">Login</a><br/>\n        <a href=\"/logout\">Logout</a><br/>\n        <pre>{}</pre>\n    </body>\n</html>\"\"\""
    },
    {
      "chunk_id": 613,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "async def root(request: web.Request) -> web.StreamResponse:\n    resp = web.Response(content_type=\"text/html\")\n    resp.text = tmpl.format(pformat(request.cookies))\n    return resp"
    },
    {
      "chunk_id": 614,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "async def login(request: web.Request) -> NoReturn:\n    exc = web.HTTPFound(location=\"/\")\n    exc.set_cookie(\"AUTH\", \"secret\")\n    raise exc"
    },
    {
      "chunk_id": 615,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "async def logout(request: web.Request) -> NoReturn:\n    exc = web.HTTPFound(location=\"/\")\n    exc.del_cookie(\"AUTH\")\n    raise exc"
    },
    {
      "chunk_id": 616,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "def init() -> web.Application:\n    app = web.Application()\n    app.router.add_get(\"/\", root)\n    app.router.add_get(\"/login\", login)\n    app.router.add_get(\"/logout\", logout)\n    return app"
    },
    {
      "chunk_id": 617,
      "source": "__internal__/data_repo/aiohttp/examples/web_cookies.py",
      "content": "web.run_app(init())"
    },
    {
      "chunk_id": 618,
      "source": "__internal__/data_repo/aiohttp/examples/lowlevel_srv.py",
      "content": "import asyncio\n\nfrom aiohttp import web, web_request"
    },
    {
      "chunk_id": 619,
      "source": "__internal__/data_repo/aiohttp/examples/lowlevel_srv.py",
      "content": "async def handler(request: web_request.BaseRequest) -> web.StreamResponse:\n    return web.Response(text=\"OK\")"
    },
    {
      "chunk_id": 620,
      "source": "__internal__/data_repo/aiohttp/examples/lowlevel_srv.py",
      "content": "async def main(loop: asyncio.AbstractEventLoop) -> None:\n    server = web.Server(handler)\n    await loop.create_server(server, \"127.0.0.1\", 8080)\n    print(\"======= Serving on http://127.0.0.1:8080/ ======\")\n\n    # pause here for very long time by serving HTTP requests and\n    # waiting for keyboard interruption\n    await asyncio.sleep(100 * 3600)"
    },
    {
      "chunk_id": 621,
      "source": "__internal__/data_repo/aiohttp/examples/lowlevel_srv.py",
      "content": "loop = asyncio.get_event_loop()"
    },
    {
      "chunk_id": 622,
      "source": "__internal__/data_repo/aiohttp/examples/lowlevel_srv.py",
      "content": "try:\n    loop.run_until_complete(main(loop))\nexcept KeyboardInterrupt:\n    pass\nloop.close()"
    },
    {
      "chunk_id": 623,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "#!/usr/bin/env python\n\nimport io\nimport pathlib\nfrom collections import defaultdict\n\nimport multidict\n\nROOT = pathlib.Path.cwd()\nwhile ROOT.parent != ROOT and not (ROOT / \".git\").exists():\n    ROOT = ROOT.parent"
    },
    {
      "chunk_id": 624,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def calc_headers(root):\n    hdrs_file = root / \"aiohttp/hdrs.py\"\n    code = compile(hdrs_file.read_text(), str(hdrs_file), \"exec\")\n    globs = {}\n    exec(code, globs)\n    headers = [val for val in globs.values() if isinstance(val, multidict.istr)]\n    return sorted(headers)"
    },
    {
      "chunk_id": 625,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "headers = calc_headers(ROOT)"
    },
    {
      "chunk_id": 626,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def factory():\n    return defaultdict(factory)"
    },
    {
      "chunk_id": 627,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "TERMINAL = object()"
    },
    {
      "chunk_id": 628,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def build(headers):\n    dct = defaultdict(factory)\n    for hdr in headers:\n        d = dct\n        for ch in hdr:\n            d = d[ch]\n        d[TERMINAL] = hdr\n    return dct"
    },
    {
      "chunk_id": 629,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "dct = build(headers)"
    },
    {
      "chunk_id": 630,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "HEADER = \"\"\"\\\n/*  The file is autogenerated from aiohttp/hdrs.py\nRun ./tools/gen.py to update it after the origin changing. */\n\n#include \"_find_header.h\"\n\n#define NEXT_CHAR() \\\\\n{ \\\\\n    count++; \\\\\n    if (count == size) { \\\\\n        /* end of search */ \\\\\n        return -1; \\\\\n    } \\\\\n    pchar++; \\\\\n    ch = *pchar; \\\\\n    last = (count == size -1); \\\\\n} while(0);\n\nint\nfind_header(const char *str, int size)\n{\n    char *pchar = str;\n    int last;\n    char ch;\n    int count = -1;\n    pchar--;\n\"\"\""
    },
    {
      "chunk_id": 631,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "BLOCK = \"\"\"\n{label}\n    NEXT_CHAR();\n    switch (ch) {{\n{cases}\n        default:\n            return -1;\n    }}\n\"\"\""
    },
    {
      "chunk_id": 632,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "CASE = \"\"\"\\\n        case '{char}':\n            if (last) {{\n                return {index};\n            }}\n            goto {next};\"\"\""
    },
    {
      "chunk_id": 633,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "FOOTER = \"\"\"\n{missing}\nmissing:\n    /* nothing found */\n    return -1;\n}}\n\"\"\""
    },
    {
      "chunk_id": 634,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def gen_prefix(prefix, k):\n    if k == \"-\":\n        return prefix + \"_\"\n    else:\n        return prefix + k.upper()"
    },
    {
      "chunk_id": 635,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def gen_block(dct, prefix, used_blocks, missing, out):\n    cases = {}\n    for k, v in dct.items():\n        if k is TERMINAL:\n            continue\n        next_prefix = gen_prefix(prefix, k)\n        term = v.get(TERMINAL)\n        if term is not None:\n            index = headers.index(term)\n        else:\n            index = -1\n        hi = k.upper()\n        lo = k.lower()\n        case = CASE.format(char=hi, index=index, next=next_prefix)\n        cases[hi] = case\n        if lo != hi:\n            case = CASE.format(char=lo, index=index, next=next_prefix)\n            cases[lo] = case\n    label = prefix + \":\" if prefix else \"\"\n    if cases:\n        block = BLOCK.format(label=label, cases=\"\\n\".join(cases.values()))\n        out.write(block)\n    else:\n        missing.add(label)\n    for k, v in dct.items():\n        if not isinstance(v, defaultdict):\n            continue\n        block_name = gen_prefix(prefix, k)\n        if block_name in used_blocks:\n            continue\n        used_blocks.add(block_name)\n        gen_block(v, block_name, used_blocks, missing, out)"
    },
    {
      "chunk_id": 636,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def gen(dct):\n    out = io.StringIO()\n    out.write(HEADER)\n    missing = set()\n    gen_block(dct, \"\", set(), missing, out)\n    missing_labels = \"\\n\".join(sorted(missing))\n    out.write(FOOTER.format(missing=missing_labels))\n    return out"
    },
    {
      "chunk_id": 637,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "def gen_headers(headers):\n    out = io.StringIO()\n    out.write(\"# The file is autogenerated from aiohttp/hdrs.py\\n\")\n    out.write(\"# Run ./tools/gen.py to update it after the origin changing.\")\n    out.write(\"\\n\\n\")\n    out.write(\"from . import hdrs\\n\")\n    out.write(\"cdef tuple headers = (\\n\")\n    for hdr in headers:\n        out.write(\"    hdrs.{},\\n\".format(hdr.upper().replace(\"-\", \"_\")))\n    out.write(\")\\n\")\n    return out"
    },
    {
      "chunk_id": 638,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "# print(gen(dct).getvalue())\n# print(gen_headers(headers).getvalue())"
    },
    {
      "chunk_id": 639,
      "source": "__internal__/data_repo/aiohttp/tools/gen.py",
      "content": "folder = ROOT / \"aiohttp\"\n\nwith (folder / \"_find_header.c\").open(\"w\") as f:\n    f.write(gen(dct).getvalue())\n\nwith (folder / \"_headers.pxi\").open(\"w\") as f:\n    f.write(gen_headers(headers).getvalue())"
    },
    {
      "chunk_id": 640,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "import asyncio\nimport atexit\nimport math\nimport os\nimport signal\nfrom typing import List, Tuple\n\nPORT = 8888\n\nserver = os.fork()\nif server == 0:\n    loop = asyncio.get_event_loop()\n    coro = asyncio.start_server(lambda *_: None, port=PORT)\n    loop.run_until_complete(coro)\n    loop.run_forever()\nelse:\n    atexit.register(os.kill, server, signal.SIGTERM)"
    },
    {
      "chunk_id": 641,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "async def write_joined_bytearray(writer, chunks):\n    body = bytearray(chunks[0])\n    for c in chunks[1:]:\n        body += c\n    writer.write(body)"
    },
    {
      "chunk_id": 642,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "async def write_joined_list(writer, chunks):\n    body = b\"\".join(chunks)\n    writer.write(body)"
    },
    {
      "chunk_id": 643,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "async def write_separately(writer, chunks):\n    for c in chunks:\n        writer.write(c)"
    },
    {
      "chunk_id": 644,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "def fm_size(s, _fms=(\"\", \"K\", \"M\", \"G\")):\n    i = 0\n    while s >= 1024:\n        s /= 1024\n        i += 1\n    return f\"{s:.0f}{_fms[i]}B\""
    },
    {
      "chunk_id": 645,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "def fm_time(s, _fms=(\"\", \"m\", \"\u00b5\", \"n\")):\n    if s == 0:\n        return \"0\"\n    i = 0\n    while s < 1:\n        s *= 1000\n        i += 1\n    return f\"{s:.2f}{_fms[i]}s\""
    },
    {
      "chunk_id": 646,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "def _job(j: List[int]) -> Tuple[str, List[bytes]]:\n    # Always start with a 256B headers chunk\n    body = [b\"0\" * s for s in [256] + list(j)]\n    job_title = f\"{fm_size(sum(j))} / {len(j)}\"\n    return (job_title, body)"
    },
    {
      "chunk_id": 647,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "writes = [\n    (\"b''.join\", write_joined_list),\n    (\"bytearray\", write_joined_bytearray),\n    (\"multiple writes\", write_separately),\n]"
    },
    {
      "chunk_id": 648,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "bodies = (\n    [],\n    [10 * 2**0],\n    [10 * 2**7],\n    [10 * 2**17],\n    [10 * 2**27],\n    [50 * 2**27],\n    [1 * 2**0 for _ in range(10)],\n    [1 * 2**7 for _ in range(10)],\n    [1 * 2**17 for _ in range(10)],\n    [1 * 2**27 for _ in range(10)],\n    [10 * 2**27 for _ in range(5)],\n)"
    },
    {
      "chunk_id": 649,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "jobs = [_job(j) for j in bodies]"
    },
    {
      "chunk_id": 650,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "async def time(loop, fn, *args):\n    spent = []\n    while not spent or sum(spent) < 0.2:\n        s = loop.time()\n        await fn(*args)\n        e = loop.time()\n        spent.append(e - s)\n    mean = sum(spent) / len(spent)\n    sd = sum((x - mean) ** 2 for x in spent) / len(spent)\n    return len(spent), mean, math.sqrt(sd)"
    },
    {
      "chunk_id": 651,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "async def main(loop):\n    _, writer = await asyncio.open_connection(port=PORT)\n    print(\"Loop:\", loop)\n    print(\"Transport:\", writer._transport)\n    res = [\n        (\"size/chunks\", \"Write option\", \"Mean\", \"Std dev\", \"loops\", \"Variation\"),\n    ]\n    res.append([\":---\", \":---\", \"---:\", \"---:\", \"---:\", \"---:\"])\n\n    async def bench(job_title, w, body, base=None):\n        it, mean, sd = await time(loop, w[1], writer, c)\n        res.append(\n            (\n                job_title,\n                w[0],\n                fm_time(mean),\n                fm_time(sd),\n                str(it),\n                f\"{mean / base - 1:.2%}\" if base is not None else \"\",\n            )\n        )\n        return mean\n\n    for t, c in jobs:\n        print(\"Doing\", t)\n        base = await bench(t, writes[0], c)\n        for w in writes[1:]:\n            await bench(\"\", w, c, base)\n    return res"
    },
    {
      "chunk_id": 652,
      "source": "__internal__/data_repo/aiohttp/tools/bench-asyncio-write.py",
      "content": "loop = asyncio.get_event_loop()\nresults = loop.run_until_complete(main(loop))\nwith open(\"bench.md\", \"w\") as f:\n    for line in results:\n        f.write(\"| {} |\\n\".format(\" | \".join(line)))"
    },
    {
      "chunk_id": 653,
      "source": "__internal__/data_repo/aiohttp/tools/check_changes.py",
      "content": "#!/usr/bin/env python3\n\nimport re\nimport sys\nfrom pathlib import Path\n\nALLOWED_SUFFIXES = (\n    \"bugfix\",\n    \"feature\",\n    \"deprecation\",\n    \"breaking\",\n    \"doc\",\n    \"packaging\",\n    \"contrib\",\n    \"misc\",\n)\nPATTERN = re.compile(\n    r\"(\\d+|[0-9a-f]{8}|[0-9a-f]{7}|[0-9a-f]{40})\\.(\"\n    + \"|\".join(ALLOWED_SUFFIXES)\n    + r\")(\\.\\d+)?(\\.rst)?\",\n)"
    },
    {
      "chunk_id": 654,
      "source": "__internal__/data_repo/aiohttp/tools/check_changes.py",
      "content": "def get_root(script_path):\n    folder = script_path.resolve().parent\n    while not (folder / \".git\").exists():\n        folder = folder.parent\n        if folder == folder.anchor:\n            raise RuntimeError(\"git repo not found\")\n    return folder"
    },
    {
      "chunk_id": 655,
      "source": "__internal__/data_repo/aiohttp/tools/check_changes.py",
      "content": "def main(argv):\n    print('Check \"CHANGES\" folder... ', end=\"\", flush=True)\n    here = Path(argv[0])\n    root = get_root(here)\n    changes = root / \"CHANGES\"\n    failed = False\n    for fname in changes.iterdir():\n        if fname.name in (\".gitignore\", \".TEMPLATE.rst\", \"README.rst\"):\n            continue\n        if not PATTERN.match(fname.name):\n            if not failed:\n                print(\"\")\n            print(\"Illegal CHANGES record\", fname, file=sys.stderr)\n            failed = True\n\n    if failed:\n        print(\"\", file=sys.stderr)\n        print(\"See ./CHANGES/README.rst for the naming instructions\", file=sys.stderr)\n        print(\"\", file=sys.stderr)\n    else:\n        print(\"OK\")\n\n    return int(failed)"
    },
    {
      "chunk_id": 656,
      "source": "__internal__/data_repo/aiohttp/tools/check_changes.py",
      "content": "if __name__ == \"__main__\":\n    sys.exit(main(sys.argv))"
    },
    {
      "chunk_id": 657,
      "source": "__internal__/data_repo/aiohttp/tools/check_sum.py",
      "content": "#!/usr/bin/env python"
    },
    {
      "chunk_id": 658,
      "source": "__internal__/data_repo/aiohttp/tools/check_sum.py",
      "content": "import argparse\nimport hashlib\nimport pathlib\nimport sys"
    },
    {
      "chunk_id": 659,
      "source": "__internal__/data_repo/aiohttp/tools/check_sum.py",
      "content": "def main(argv):\n    args = PARSER.parse_args(argv)\n    dst = args.dst\n    assert dst.suffix == \".hash\"\n    dirname = dst.parent\n    if dirname.name != \".hash\":\n        if args.debug:\n            print(f\"Invalid name {dst} -> dirname {dirname}\", file=sys.stderr)\n        return 0\n    dirname.mkdir(exist_ok=True)\n    src_dir = dirname.parent\n    src_name = dst.stem  # drop .hash\n    full_src = src_dir / src_name\n    hasher = hashlib.sha256()\n    try:\n        hasher.update(full_src.read_bytes())\n    except OSError:\n        if args.debug:\n            print(f\"Cannot open {full_src}\", file=sys.stderr)\n        return 0\n    src_hash = hasher.hexdigest()\n    if dst.exists():\n        dst_hash = dst.read_text()\n    else:\n        dst_hash = \"\"\n    if src_hash != dst_hash:\n        dst.write_text(src_hash)\n        print(f\"re-hash {src_hash}\")\n    else:\n        if args.debug:\n            print(f\"Skip {src_hash} checksum, up-to-date\")\n    return 0"
    },
    {
      "chunk_id": 660,
      "source": "__internal__/data_repo/aiohttp/tools/check_sum.py",
      "content": "if __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))"
    },
    {
      "chunk_id": 661,
      "source": "__internal__/data_repo/aiohttp/tools/cleanup_changes.py",
      "content": "#!/usr/bin/env python"
    },
    {
      "chunk_id": 662,
      "source": "__internal__/data_repo/aiohttp/tools/cleanup_changes.py",
      "content": "import re\nimport subprocess\nfrom pathlib import Path"
    },
    {
      "chunk_id": 663,
      "source": "__internal__/data_repo/aiohttp/tools/cleanup_changes.py",
      "content": "ALLOWED_SUFFIXES = (\n    \"bugfix\",\n    \"feature\",\n    \"deprecation\",\n    \"breaking\",\n    \"doc\",\n    \"packaging\",\n    \"contrib\",\n    \"misc\",\n)\nPATTERN = re.compile(\n    r\"(\\d+|[0-9a-f]{8}|[0-9a-f]{7}|[0-9a-f]{40})\\.(\"\n    + \"|\".join(ALLOWED_SUFFIXES)\n    + r\")(\\.\\d+)?(\\.rst)?\",\n)"
    },
    {
      "chunk_id": 664,
      "source": "__internal__/data_repo/aiohttp/tools/cleanup_changes.py",
      "content": "def main():\n    root = Path(__file__).parent.parent\n    delete = []\n    changes = (root / \"CHANGES.rst\").read_text()\n    for fname in (root / \"CHANGES\").iterdir():\n        match = PATTERN.match(fname.name)\n        if match is not None:\n            commit_issue_or_pr = match.group(1)\n            tst_issue_or_pr = f\":issue:`{commit_issue_or_pr}`\"\n            tst_commit = f\":commit:`{commit_issue_or_pr}`\"\n            if tst_issue_or_pr in changes or tst_commit in changes:\n                subprocess.run([\"git\", \"rm\", fname])\n                delete.append(fname.name)\n    print(\"Deleted CHANGES records:\", \" \".join(delete))\n    print(\"Please verify and commit\")"
    },
    {
      "chunk_id": 665,
      "source": "__internal__/data_repo/aiohttp/tools/cleanup_changes.py",
      "content": "if __name__ == \"__main__\":\n    main()"
    },
    {
      "chunk_id": 666,
      "source": "__internal__/data_repo/aiohttp/requirements/sync-direct-runtime-deps.py",
      "content": "#!/usr/bin/env python\n\"\"\"Sync direct runtime dependencies from setup.cfg to runtime-deps.in.\"\"\""
    },
    {
      "chunk_id": 667,
      "source": "__internal__/data_repo/aiohttp/requirements/sync-direct-runtime-deps.py",
      "content": "from configparser import ConfigParser\nfrom pathlib import Path"
    },
    {
      "chunk_id": 668,
      "source": "__internal__/data_repo/aiohttp/requirements/sync-direct-runtime-deps.py",
      "content": "cfg = ConfigParser()\ncfg.read(Path(\"setup.cfg\"))\nreqs = cfg[\"options\"][\"install_requires\"] + cfg.items(\"options.extras_require\")[0][1]\nreqs = sorted(reqs.split(\"\\n\"), key=str.casefold)\nreqs.remove(\"\")\n\nwith open(Path(\"requirements\", \"runtime-deps.in\"), \"w\") as outfile:\n    header = \"# Extracted from `setup.cfg` via `make sync-direct-runtime-deps`\\n\\n\"\n    outfile.write(header)\n    outfile.write(\"\\n\".join(reqs) + \"\\n\")"
    },
    {
      "chunk_id": 669,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "#!/usr/bin/env python3\n#\n# aiohttp documentation build configuration file, created by\n# sphinx-quickstart on Wed Mar  5 12:35:35 2014.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport re\nfrom pathlib import Path"
    },
    {
      "chunk_id": 670,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "PROJECT_ROOT_DIR = Path(__file__).parents[1].resolve()\nIS_RELEASE_ON_RTD = (\n    os.getenv(\"READTHEDOCS\", \"False\") == \"True\"\n    and os.environ[\"READTHEDOCS_VERSION_TYPE\"] == \"tag\"\n)\nif IS_RELEASE_ON_RTD:\n    tags.add(\"is_release\")\n\n_docs_path = os.path.dirname(__file__)\n_version_path = os.path.abspath(\n    os.path.join(_docs_path, \"..\", \"aiohttp\", \"__init__.py\")\n)\nwith open(_version_path, encoding=\"latin1\") as fp:\n    try:\n        _version_info = re.search(\n            r'^__version__ = \"'\n            r\"(?P<major>\\d+)\"\n            r\"\\.(?P<minor>\\d+)\"\n            r\"\\.(?P<patch>\\d+)\"\n            r'(?P<tag>.*)?\"$',\n            fp.read(),\n            re.M,\n        ).groupdict()\n    except IndexError:\n        raise RuntimeError(\"Unable to determine version.\")"
    },
    {
      "chunk_id": 671,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    # stdlib-party extensions:\n    \"sphinx.ext.extlinks\",\n    \"sphinx.ext.graphviz\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.viewcode\",\n    # Third-party extensions:\n    \"sphinxcontrib.towncrier.ext\",  # provides `towncrier-draft-entries` directive\n]"
    },
    {
      "chunk_id": 672,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "try:\n    import sphinxcontrib.spelling  # noqa\n\n    extensions.append(\"sphinxcontrib.spelling\")\nexcept ImportError:\n    pass"
    },
    {
      "chunk_id": 673,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "intersphinx_mapping = {\n    \"pytest\": (\"http://docs.pytest.org/en/latest/\", None),\n    \"python\": (\"http://docs.python.org/3\", None),\n    \"multidict\": (\"https://multidict.readthedocs.io/en/stable/\", None),\n    \"propcache\": (\"https://propcache.aio-libs.org/en/stable\", None),\n    \"yarl\": (\"https://yarl.readthedocs.io/en/stable/\", None),\n    \"aiosignal\": (\"https://aiosignal.readthedocs.io/en/stable/\", None),\n    \"aiohttpjinja2\": (\"https://aiohttp-jinja2.readthedocs.io/en/stable/\", None),\n    \"aiohttpremotes\": (\"https://aiohttp-remotes.readthedocs.io/en/stable/\", None),\n    \"aiohttpsession\": (\"https://aiohttp-session.readthedocs.io/en/stable/\", None),\n    \"aiohttpdemos\": (\"https://aiohttp-demos.readthedocs.io/en/latest/\", None),\n    \"aiojobs\": (\"https://aiojobs.readthedocs.io/en/stable/\", None),\n}"
    },
    {
      "chunk_id": 674,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = \"index\""
    },
    {
      "chunk_id": 675,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Project information -----------------------------------------------------\n\ngithub_url = \"https://github.com\"\ngithub_repo_org = \"aio-libs\"\ngithub_repo_name = \"aiohttp\"\ngithub_repo_slug = f\"{github_repo_org}/{github_repo_name}\"\ngithub_repo_url = f\"{github_url}/{github_repo_slug}\"\ngithub_sponsors_url = f\"{github_url}/sponsors\"\n\nproject = github_repo_name\ncopyright = f\"{project} contributors\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \"{major}.{minor}\".format(**_version_info)\n# The full version, including alpha/beta/rc tags.\nrelease = \"{major}.{minor}.{patch}{tag}\".format(**_version_info)"
    },
    {
      "chunk_id": 676,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\"_build\"]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\n# pygments_style = 'sphinx'\n\n# The default language to highlight source code in.\nhighlight_language = \"python3\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False"
    },
    {
      "chunk_id": 677,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Extension configuration -------------------------------------------------\n\n# -- Options for extlinks extension ---------------------------------------\nextlinks = {\n    \"issue\": (f\"{github_repo_url}/issues/%s\", \"#%s\"),\n    \"pr\": (f\"{github_repo_url}/pull/%s\", \"PR #%s\"),\n    \"commit\": (f\"{github_repo_url}/commit/%s\", \"%s\"),\n    \"gh\": (f\"{github_url}/%s\", \"GitHub: %s\"),\n    \"user\": (f\"{github_sponsors_url}/%s\", \"@%s\"),\n}"
    },
    {
      "chunk_id": 678,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"aiohttp_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \"description\": \"Async HTTP client/server for asyncio and Python\",\n    \"canonical_url\": \"http://docs.aiohttp.org/en/stable/\",\n    \"github_user\": github_repo_org,\n    \"github_repo\": github_repo_name,\n    \"github_button\": True,\n    \"github_type\": \"star\",\n    \"github_banner\": True,\n    \"badges\": [\n        {\n            \"image\": f\"{github_repo_url}/workflows/CI/badge.svg\",\n            \"target\": f\"{github_repo_url}/actions?query=workflow%3ACI\",\n            \"height\": \"20\",\n            \"alt\": \"Azure Pipelines CI status\",\n        },\n        {\n            \"image\": f\"https://codecov.io/github/{github_repo_slug}/coverage.svg?branch=master\",\n            \"target\": f\"https://codecov.io/github/{github_repo_slug}\",\n            \"height\": \"20\",\n            \"alt\": \"Code coverage status\",\n        },\n        {\n            \"image\": f\"https://badge.fury.io/py/{project}.svg\",\n            \"target\": f\"https://badge.fury.io/py/{project}\",\n            \"height\": \"20\",\n            \"alt\": \"Latest PyPI package version\",\n        },\n        {\n            \"image\": \"https://badges.gitter.im/Join%20Chat.svg\",\n            \"target\": f\"https://gitter.im/{github_repo_org}/Lobby\",\n            \"height\": \"20\",\n            \"alt\": \"Chat on Gitter\",\n        },\n    ],\n}"
    },
    {
      "chunk_id": 679,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "html_css_files = [\n    \"css/logo-adjustments.css\",\n]\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = [alabaster.get_path()]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \"aiohttp-plain.svg\"\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = \"favicon.ico\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\nhtml_sidebars = {\n    \"**\": [\n        \"about.html\",\n        \"navigation.html\",\n        \"searchbox.html\",\n    ]\n}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = f\"{project}doc\""
    },
    {
      "chunk_id": 680,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    # 'preamble': '',\n}"
    },
    {
      "chunk_id": 681,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        \"index\",\n        f\"{project}.tex\",\n        f\"{project} Documentation\",\n        f\"{project} contributors\",\n        \"manual\",\n    ),\n]"
    },
    {
      "chunk_id": 682,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True"
    },
    {
      "chunk_id": 683,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(\"index\", project, f\"{project} Documentation\", [project], 1)]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False"
    },
    {
      "chunk_id": 684,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        \"index\",\n        project,\n        f\"{project} Documentation\",\n        \"Aiohttp contributors\",\n        project,\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    ),\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n# texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n# texinfo_no_detailmenu = False"
    },
    {
      "chunk_id": 685,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -------------------------------------------------------------------------\nnitpicky = True\nnitpick_ignore = [\n    (\"py:mod\", \"aiohttp\"),  # undocumented, no `.. currentmodule:: aiohttp` in docs\n    (\"py:class\", \"aiohttp.SimpleCookie\"),  # undocumented\n    (\"py:class\", \"aiohttp.web.RequestHandler\"),  # undocumented\n    (\"py:class\", \"aiohttp.NamedPipeConnector\"),  # undocumented\n    (\"py:class\", \"aiohttp.protocol.HttpVersion\"),  # undocumented\n    (\"py:class\", \"aiohttp.ClientRequest\"),  # undocumented\n    (\"py:class\", \"aiohttp.payload.Payload\"),  # undocumented\n    (\"py:class\", \"aiohttp.resolver.AsyncResolver\"),  # undocumented\n    (\"py:class\", \"aiohttp.resolver.ThreadedResolver\"),  # undocumented\n    (\"py:func\", \"aiohttp.ws_connect\"),  # undocumented\n    (\"py:meth\", \"start\"),  # undocumented\n    (\"py:exc\", \"aiohttp.ClientHttpProxyError\"),  # undocumented\n    (\"py:class\", \"asyncio.AbstractServer\"),  # undocumented\n    (\"py:mod\", \"aiohttp.test_tools\"),  # undocumented\n    (\"py:class\", \"list of pairs\"),  # undocumented\n    (\"py:class\", \"aiohttp.protocol.HttpVersion\"),  # undocumented\n    (\"py:meth\", \"aiohttp.ClientSession.request\"),  # undocumented\n    (\"py:class\", \"aiohttp.StreamWriter\"),  # undocumented\n    (\"py:attr\", \"aiohttp.StreamResponse.body\"),  # undocumented\n    (\"py:class\", \"aiohttp.payload.StringPayload\"),  # undocumented\n    (\"py:meth\", \"aiohttp.web.Application.copy\"),  # undocumented\n    (\"py:meth\", \"asyncio.AbstractEventLoop.create_server\"),  # undocumented\n    (\"py:data\", \"aiohttp.log.server_logger\"),  # undocumented\n    (\"py:data\", \"aiohttp.log.access_logger\"),  # undocumented\n    (\"py:data\", \"aiohttp.helpers.AccessLogger\"),  # undocumented\n    (\"py:attr\", \"helpers.AccessLogger.LOG_FORMAT\"),  # undocumented\n    (\"py:meth\", \"aiohttp.web.AbstractRoute.url\"),  # undocumented\n    (\"py:class\", \"aiohttp.web.MatchedSubAppResource\"),  # undocumented\n    (\"py:attr\", \"body\"),  # undocumented\n    (\"py:class\", \"socket.socket\"),  # undocumented\n    (\"py:class\", \"socket.AddressFamily\"),  # undocumented\n    (\"py:obj\", \"logging.DEBUG\"),  # undocumented\n    (\"py:class\", \"aiohttp.abc.AbstractAsyncAccessLogger\"),  # undocumented\n    (\"py:meth\", \"aiohttp.web.Response.write_eof\"),  # undocumented\n    (\"py:meth\", \"aiohttp.payload.Payload.set_content_disposition\"),  # undocumented\n    (\"py:class\", \"cgi.FieldStorage\"),  # undocumented\n    (\"py:meth\", \"aiohttp.web.UrlDispatcher.register_resource\"),  # undocumented\n    (\"py:func\", \"aiohttp_debugtoolbar.setup\"),  # undocumented\n]"
    },
    {
      "chunk_id": 686,
      "source": "__internal__/data_repo/aiohttp/docs/conf.py",
      "content": "# -- Options for towncrier_draft extension -----------------------------------\n\ntowncrier_draft_autoversion_mode = \"draft\"  # or: 'sphinx-version', 'sphinx-release'\ntowncrier_draft_include_empty = True\ntowncrier_draft_working_directory = PROJECT_ROOT_DIR\n# Not yet supported: towncrier_draft_config_path = 'pyproject.toml'  # relative to cwd"
    }
  ]
}